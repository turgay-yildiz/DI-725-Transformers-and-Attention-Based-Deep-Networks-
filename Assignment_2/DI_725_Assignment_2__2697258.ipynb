{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a24157c-3ff2-4a66-8dcf-37d2085a776d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; text-align:center; padding:40px;\">\n",
    "<h1  style=\"color:red;\" > DI-725 : Transformers and Attention-Based Deep Networks </h1>   \n",
    "<h2  style=\"color:red;\" > Assignment - 2 </h2>\n",
    "<br>\n",
    "<br>\n",
    "<h4  style=\"color:red;\" >Turgay Yıldız</h4>\n",
    "<br>\n",
    "<h4  style=\"color:red;\" >Graduate School of Informatics,  Middle East Technical University (METU)</h4>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf8e0a-6828-4a7e-8dd0-dc184062329e",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  First Import the Relevant Packages  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c533b0-e398-4508-bba6-c8271e76a9f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "from torch.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f343f0ef-ba51-4b1b-bfa9-db464e6eafdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c67c80-67be-49e0-aec9-2255f0367d42",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  WANDB </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a06207e-5a84-42b3-b97d-3e46c5e66b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/turgay/Turgay/Academic/2024-2025/Spring/Transformers/Assignments/Assignment_2\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2765b4fc-84b7-40cd-be07-cc9c4f2acb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"DI_725_Assignment_2__2697258.ipynb\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365eb0cb-0624-426b-9195-a74fc334fda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mturgay-yildiz-phi-e-pi\u001b[0m (\u001b[33mDI_725_Assignment_2___2697258\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/turgay/Turgay/Academic/2024-2025/Spring/Transformers/Assignments/Assignment_2/wandb/run-20250413_184621-1oan5i0i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DI_725_Assignment_2___2697258/DI_725_Assignment_2___2697258/runs/1oan5i0i' target=\"_blank\">vague-lion-41</a></strong> to <a href='https://wandb.ai/DI_725_Assignment_2___2697258/DI_725_Assignment_2___2697258' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DI_725_Assignment_2___2697258/DI_725_Assignment_2___2697258' target=\"_blank\">https://wandb.ai/DI_725_Assignment_2___2697258/DI_725_Assignment_2___2697258</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DI_725_Assignment_2___2697258/DI_725_Assignment_2___2697258/runs/1oan5i0i' target=\"_blank\">https://wandb.ai/DI_725_Assignment_2___2697258/DI_725_Assignment_2___2697258/runs/1oan5i0i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run  =  wandb.init(project=\"DI_725_Assignment_2___2697258\", entity=\"DI_725_Assignment_2___2697258\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b502c6b5-281a-4100-8e07-6fa7e5b8bf1e",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Import Dataset  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adee8bb2-6f68-4a74-827a-5e5997ce3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "path   =   \"/home/turgay/Turgay/Academic/2024-2025/Spring/Transformers/Assignments/Assignment_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bb1cd83-b128-4c70-b53a-6a3bb2819968",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_p        =    os.listdir(path + \"auair2019/images\")\n",
    "annotations_p   =   path + \"auair2019/annotations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e2c534-4aa6-4100-bfc5-f26b9d9517bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frame_20190905103112_xx_0000365.jpg',\n",
       " 'frame_20190906150731_xx_0002188.jpg',\n",
       " 'frame_20190906150731_x_0001674.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_p[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77b4a17b-6fd7-4f7a-8b0a-0b082254fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotations_p, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e17e7c6f-1ae2-430a-902b-0e62b8ab69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_info    =   annotations[\"info\"]\n",
    "annotations_ann     =   annotations[\"annotations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cd2c79d-4bf1-4e5c-8836-7af9d3b2273f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'AUAIR 2019 Dataset',\n",
       " 'url': 'https://bozcani.github.io/AU-AIR-dataset.html',\n",
       " 'version': '1.0',\n",
       " 'year': '2019',\n",
       " 'contributor': 'Ilker Bozcan',\n",
       " 'date_created': '2019/10/01'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00864bc6-f3d2-44d3-a814-b0c317472273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32823"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6241b7-7958-49b0-8b07-2c91a7d45eb4",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  BBoxes </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6581b1db-aa9a-4291-aa62-e8c8ef7d8ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'top': 857, 'left': 976, 'height': 86, 'width': 139, 'class': 0},\n",
       " {'top': 11, 'left': 624, 'height': 210, 'width': 328, 'class': 1}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_ann[1][\"bbox\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888b6aa-ac6d-4528-9dd1-01532f4030ff",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Average Length of Classes for an image </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4869347-e937-429a-94ca-ab04f73de2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = 0\n",
    "\n",
    "for i in range(len(annotations_ann)):\n",
    "\n",
    "    summ   +=    len(annotations_ann[i][\"bbox\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba5a74f0-6d7f-495a-9266-f2b10d4ff048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132031"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b9950aa-0d26-4e8f-9e78-ad9d62975bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0225147000578865"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ /  len(annotations_ann) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80cfbc6-ba31-41c7-8a7d-e271328c112f",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Max Class Indices in an image</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "592b87cb-749c-4655-b561-d7b45c72b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "summs   =  [] \n",
    "\n",
    "for i in range(len(annotations_ann)):\n",
    "\n",
    "    summs.append(len(annotations_ann[i][\"bbox\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54fafa7e-1066-42e3-8446-fc70a75fcc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(summs).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70137c1e-0056-4e8e-9a66-345f873a523b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25251"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(summs).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7ea0b42-b722-4215-936c-b2e5a2598ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'top': 396, 'left': 439, 'height': 35, 'width': 101, 'class': 1},\n",
       " {'top': 375, 'left': 453, 'height': 30, 'width': 105, 'class': 1},\n",
       " {'top': 347, 'left': 464, 'height': 31, 'width': 83, 'class': 1},\n",
       " {'top': 328, 'left': 473, 'height': 27, 'width': 68, 'class': 1},\n",
       " {'top': 303, 'left': 468, 'height': 24, 'width': 85, 'class': 1},\n",
       " {'top': 293, 'left': 470, 'height': 18, 'width': 82, 'class': 1},\n",
       " {'top': 273, 'left': 477, 'height': 21, 'width': 68, 'class': 1},\n",
       " {'top': 260, 'left': 477, 'height': 18, 'width': 73, 'class': 1},\n",
       " {'top': 241, 'left': 481, 'height': 24, 'width': 63, 'class': 1},\n",
       " {'top': 236, 'left': 399, 'height': 27, 'width': 62, 'class': 1},\n",
       " {'top': 260, 'left': 400, 'height': 23, 'width': 63, 'class': 1},\n",
       " {'top': 276, 'left': 400, 'height': 20, 'width': 57, 'class': 1},\n",
       " {'top': 295, 'left': 404, 'height': 21, 'width': 49, 'class': 1},\n",
       " {'top': 312, 'left': 411, 'height': 27, 'width': 40, 'class': 1},\n",
       " {'top': 339, 'left': 418, 'height': 20, 'width': 33, 'class': 1},\n",
       " {'top': 355, 'left': 412, 'height': 31, 'width': 39, 'class': 1},\n",
       " {'top': 154, 'left': 594, 'height': 26, 'width': 32, 'class': 1},\n",
       " {'top': 145, 'left': 653, 'height': 22, 'width': 22, 'class': 1},\n",
       " {'top': 145, 'left': 680, 'height': 27, 'width': 23, 'class': 1},\n",
       " {'top': 144, 'left': 724, 'height': 26, 'width': 32, 'class': 1},\n",
       " {'top': 133, 'left': 748, 'height': 33, 'width': 38, 'class': 1},\n",
       " {'top': 143, 'left': 783, 'height': 16, 'width': 26, 'class': 1},\n",
       " {'top': 139, 'left': 810, 'height': 24, 'width': 25, 'class': 1},\n",
       " {'top': 136, 'left': 861, 'height': 28, 'width': 36, 'class': 1},\n",
       " {'top': 133, 'left': 898, 'height': 24, 'width': 24, 'class': 1},\n",
       " {'top': 135, 'left': 964, 'height': 22, 'width': 29, 'class': 1},\n",
       " {'top': 133, 'left': 993, 'height': 21, 'width': 23, 'class': 1},\n",
       " {'top': 133, 'left': 1020, 'height': 22, 'width': 18, 'class': 1},\n",
       " {'top': 143, 'left': 1040, 'height': 20, 'width': 28, 'class': 1},\n",
       " {'top': 131, 'left': 1056, 'height': 24, 'width': 27, 'class': 1},\n",
       " {'top': 123, 'left': 1094, 'height': 33, 'width': 35, 'class': 1},\n",
       " {'top': 128, 'left': 1107, 'height': 7, 'width': 30, 'class': 1},\n",
       " {'top': 166, 'left': 1263, 'height': 27, 'width': 46, 'class': 1},\n",
       " {'top': 160, 'left': 1314, 'height': 26, 'width': 46, 'class': 1},\n",
       " {'top': 167, 'left': 1071, 'height': 15, 'width': 34, 'class': 1},\n",
       " {'top': 174, 'left': 968, 'height': 18, 'width': 37, 'class': 1},\n",
       " {'top': 173, 'left': 921, 'height': 19, 'width': 24, 'class': 1},\n",
       " {'top': 158, 'left': 1225, 'height': 34, 'width': 35, 'class': 1},\n",
       " {'top': 190, 'left': 809, 'height': 13, 'width': 30, 'class': 1},\n",
       " {'top': 184, 'left': 786, 'height': 28, 'width': 22, 'class': 1},\n",
       " {'top': 173, 'left': 891, 'height': 36, 'width': 37, 'class': 1},\n",
       " {'top': 182, 'left': 726, 'height': 17, 'width': 28, 'class': 1},\n",
       " {'top': 186, 'left': 692, 'height': 34, 'width': 38, 'class': 1},\n",
       " {'top': 188, 'left': 630, 'height': 36, 'width': 41, 'class': 1},\n",
       " {'top': 190, 'left': 595, 'height': 47, 'width': 33, 'class': 1},\n",
       " {'top': 156, 'left': 75, 'height': 25, 'width': 15, 'class': 1},\n",
       " {'top': 130, 'left': 87, 'height': 23, 'width': 26, 'class': 1},\n",
       " {'top': 117, 'left': 135, 'height': 17, 'width': 17, 'class': 1},\n",
       " {'top': 164, 'left': 0, 'height': 30, 'width': 30, 'class': 1},\n",
       " {'top': 115, 'left': 0, 'height': 23, 'width': 21, 'class': 1},\n",
       " {'top': 106, 'left': 25, 'height': 26, 'width': 26, 'class': 1},\n",
       " {'top': 102, 'left': 57, 'height': 28, 'width': 28, 'class': 1},\n",
       " {'top': 98, 'left': 85, 'height': 28, 'width': 39, 'class': 1},\n",
       " {'top': 98, 'left': 128, 'height': 17, 'width': 32, 'class': 1},\n",
       " {'top': 25, 'left': 55, 'height': 11, 'width': 35, 'class': 1},\n",
       " {'top': 10, 'left': 81, 'height': 17, 'width': 30, 'class': 1}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_ann[25251][\"bbox\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3fa98-9d26-4572-b90b-95ef2af8ce53",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Unique Classes</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2ffffab-a13e-43b2-ba1e-3a07682d53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst   =   []\n",
    "\n",
    "for i in range(len(annotations_ann)):\n",
    "\n",
    "    bbox   =   annotations_ann[i][\"bbox\"]\n",
    "\n",
    "    for j in range(len(bbox)):\n",
    "\n",
    "        label   =   bbox[j][\"class\"] \n",
    "\n",
    "        if label  not  in  lst :\n",
    "    \n",
    "            lst.append(label) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a97f54f7-49ca-4902-8687-ffb7d98ad210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 3, 5, 7, 4, 6]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c171f512-2be0-4c28-ab72-6c1f201c5dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5319f-3ae3-462a-aad4-7110dbfdb66e",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Preprocess Data</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d056d8ed-6808-4e1d-83fc-63aa62b881ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7155d80b-928f-47c3-9a13-43fd4c1b6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_annotations(annotations_ann):\n",
    "\n",
    "    all_bboxes = []\n",
    "    all_images = [] \n",
    "\n",
    "    path   =   \"/home/turgay/Turgay/Academic/2024-2025/Spring/Transformers/Assignments/Assignment_2/auair2019/images/\"\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(annotations_ann))):\n",
    "\n",
    "        \n",
    "        image_name    =     annotations_ann[i][\"image_name\"] \n",
    "        img_path      =     os.path.join(path ,  image_name)   \n",
    "\n",
    "\n",
    "        all_images.append(img_path)\n",
    "\n",
    "\n",
    "        \n",
    "        len_bbox   =  len(annotations_ann[i][\"bbox\"])\n",
    "        bbox_list  =  []\n",
    "\n",
    "        for j in range(len_bbox):\n",
    "\n",
    "            top       =   annotations_ann[i][\"bbox\"][j][\"top\"]\n",
    "            left      =   annotations_ann[i][\"bbox\"][j][\"left\"]\n",
    "            height    =   annotations_ann[i][\"bbox\"][j][\"height\"]\n",
    "            width     =   annotations_ann[i][\"bbox\"][j][\"width\"]\n",
    "            clas      =   annotations_ann[i][\"bbox\"][j][\"class\"]\n",
    "\n",
    "            bbox_list.append([top, left, height, width, clas])\n",
    "\n",
    "        bbox_arr   =   np.array(bbox_list, dtype=np.float32)\n",
    "\n",
    "        if  len_bbox  <  100:\n",
    "\n",
    "            pad       =   np.zeros((100 - len_bbox, 5), dtype=np.float32)\n",
    "            pad[:, 4] =   8      # padding class as 8 (\"no object\")\n",
    "            bbox_arr  =   np.concatenate([bbox_arr, pad], axis=0)\n",
    "\n",
    "        elif len_bbox  >  100:\n",
    "\n",
    "            bbox_arr  =   bbox_arr[:100]\n",
    "\n",
    "        all_bboxes.append(bbox_arr) \n",
    "    \n",
    "\n",
    "    all_bboxes = np.stack(all_bboxes, axis=0)\n",
    "    all_images = np.stack(all_images, axis=0)\n",
    "\n",
    "    return    all_images,    all_bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1dd85-76cd-4959-88ac-45c2cce3f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images,     all_bboxes    =    preprocess_annotations(annotations_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cc224-8821-436f-91a4-608106e13998",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images.shape,  all_bboxes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc5b54-668b-4bcb-8339-0ac2689e0644",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Check the Preprocessed Data</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1a569-795f-48a6-8b21-b38ee3ea20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e26c21-fd42-4603-a74a-f06f4cebd0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bboxes[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71cddd9-6b23-40f2-84e8-ba42b141f574",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Original Dimensions </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9e94a-fbb5-4ce2-801b-3ee6d59c1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images,     all_bboxes    =    preprocess_annotations(annotations_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ca073-fc37-4db4-9aa9-c7adb359d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num  =  4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af93cf-d942-48e9-82ce-3d8f5239febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img   =   Image.open(all_images[num]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27798442-ad6a-42f7-80eb-599c7fc1f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transforms.ToTensor()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f1695-79fb-4be6-a409-b6d98f3cc319",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a2609-73c3-4201-a8b3-c8ddfdabf50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bboxes[num][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad14cba-bc21-4d98-8847-50c82cf9cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "\n",
    "ax.imshow(img)\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    rect = patches.Rectangle((all_bboxes[num][i][1], all_bboxes[num][i][0]), all_bboxes[num][i][3], all_bboxes[num][i][2], linewidth=2, edgecolor='r', facecolor='none')\n",
    "    \n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5dbc53-96bc-47fc-94c8-04c109dd7328",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Small Dimensions</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55392f64-2a9d-4d54-b158-de87dda8ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images,     all_bboxes    =    preprocess_annotations(annotations_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309e682-f4f0-4d49-a779-11143e45dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num  =  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d99da-946f-4c99-81bb-f3e195d0e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img     =   Image.open(all_images[num]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a4e5b-57f8-47d8-ac8b-f653c1fa6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "t       =    transforms.Resize((480, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbca77-a09d-4fc5-8617-34b138e865dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2    =   t(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1597e-689e-4172-8453-569c7dc34ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first   =  1080 / 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc785f9-1f8f-4d01-8355-0a4be01bd980",
   "metadata": {},
   "outputs": [],
   "source": [
    "second  =  1920 / 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1404d95-6ffe-4caa-b6f9-41d1bcb0fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images,     all_bboxes    =    preprocess_annotations(annotations_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e72c9e-4cf2-442e-aa6a-c4ac09c2d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bboxes[:,  :,  0]   =  all_bboxes[:,  :,  0] / first\n",
    "all_bboxes[:,  :,  2]   =  all_bboxes[:,  :,  2] / first\n",
    "\n",
    "all_bboxes[:,  :,  1]   =  all_bboxes[:,  :,  1] / second\n",
    "all_bboxes[:,  :,  3]   =  all_bboxes[:,  :,  3] / second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dffb7bb-705f-4d7a-a256-722532cabd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "\n",
    "ax.imshow(img2)\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    rect = patches.Rectangle((all_bboxes[num][i][1], all_bboxes[num][i][0]), all_bboxes[num][i][3], all_bboxes[num][i][2], linewidth=2, edgecolor='r', facecolor='none')\n",
    "    \n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d754cd-68d7-4fea-ab5f-d51d1860c95e",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Split Dataset according to the Report Paper </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0285f95-9ec5-4914-b0ed-6b3500a3e48d",
   "metadata": {},
   "source": [
    "```python\n",
    "annotations_train_val        =    all_bboxes[:30000,    :,  :]\n",
    "annotations_test             =    all_bboxes[30000:-1,  :,  :] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5dba2-8fe4-42c0-a7e0-6af907130514",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Prepare Dataset for Training </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e23d468-accb-453d-bede-56974a599e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUAIRDataset(Dataset):\n",
    "\n",
    "    def __init__(self,  test = False,  transform=None):\n",
    "\n",
    "        \n",
    "        self.transform          =   transform \n",
    "        first                   =   1080 / 480\n",
    "        second                  =   1920 / 640\n",
    "\n",
    "        all_images,     all_bboxes    =    preprocess_annotations(annotations_ann) \n",
    "\n",
    "        all_bboxes[:,  :,  0]   =  all_bboxes[:,  :,  0] / first\n",
    "        all_bboxes[:,  :,  2]   =  all_bboxes[:,  :,  2] / first \n",
    "        \n",
    "        all_bboxes[:,  :,  1]   =  all_bboxes[:,  :,  1] / second\n",
    "        all_bboxes[:,  :,  3]   =  all_bboxes[:,  :,  3] / second\n",
    "        \n",
    "        if test:\n",
    "\n",
    "            self.annotations    =   all_bboxes[30000:-1,  :,  :]  \n",
    "            self.img_paths      =   all_images[30000:-1] \n",
    "\n",
    "        else:\n",
    "\n",
    "            self.annotations    =   all_bboxes[:30000,    :,  :]\n",
    "            self.img_paths      =   all_images[:30000]  \n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return    len(self.annotations)  \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data          =     self.annotations[idx]   \n",
    "        img_path      =     self.img_paths[idx]  \n",
    "            \n",
    "        image         =     Image.open(img_path).convert(\"RGB\")   \n",
    "\n",
    "                    \n",
    "        \n",
    "        if self.transform:\n",
    "            \n",
    "            image = self.transform(image) \n",
    "            \n",
    "\n",
    "        image         =     np.array(image, dtype=np.float32)\n",
    "\n",
    "        \n",
    "        return     image,       data  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca248499-0e9e-465e-b076-f89ed82e5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size   =   (480, 640) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bcacbb6-5ca5-4f46-b2e9-994893c6580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    \n",
    "    transforms.Resize(img_size),  \n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae930b-ea8b-4377-b892-d4724e90f8d4",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  For Training   </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbc3f6b9-5028-4967-937b-a77010a343b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32823/32823 [00:00<00:00, 119423.00it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset      =   AUAIRDataset(    test=False,    transform  =  data_transform) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26f5013a-cdae-4b5d-a9d3-f6e574f75505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08975b0e-2557-4b67-b64e-1c49f294e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size   =    int(0.9 * len(dataset))\n",
    "val_size     =    len(dataset) - train_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4527697d-4ae4-4c1c-ae35-7f7eff235605",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4d7ed03-25db-426e-bf48-c421baf447ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader   =   DataLoader(train_dataset, batch_size=16, num_workers  =   8,   pin_memory = True,    shuffle=True)\n",
    "val_loader     =   DataLoader(val_dataset,   batch_size=16, num_workers  =   8,   pin_memory = True,    shuffle=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef997cdd-2333-4fcf-897b-8e1a370dde28",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  For Testing (Unseen Data)   </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa909376-4f95-45bf-9f3c-1d47a82f58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset2       =   AUAIRDataset(all_images,  all_bboxes,      test=True,    transform  =  data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a6865-6bb0-4b6d-a98d-85922e43b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069dafd8-3e6d-415d-8138-5da0ce1afef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loader   =   DataLoader(dataset2,  batch_size=16, num_workers  =   8,   pin_memory = True,   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f2b53-7e0a-4315-9d41-ac9277ce48e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fdcaafa-5f09-4991-9e7f-37f453dad855",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Dimensions  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e5682-5d83-437b-a6d1-adc5b1744831",
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, bboxes in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf1a02-a9cd-47f0-8e5c-e62b1737e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eec666-e85f-4b91-94e7-e31092f083a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61efbb-aad0-4522-bbfb-9dfffbe621a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num  =  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5534b8-b04a-4ba9-9ddd-76fc0aeb2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "\n",
    "ax.imshow(imgs[num]/255.0)\n",
    "\n",
    "for k in range(100):\n",
    "    \n",
    "    rect = patches.Rectangle((bboxes[num][k][1], bboxes[num][k][0]), bboxes[num][k][3], bboxes[num][k][2], linewidth=2, edgecolor='r', facecolor='none')\n",
    "    \n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3d489-bdea-4808-b98b-b7de8219d6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae7a6873-d35d-432b-9561-ac02c4d6d812",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Custom Loss   </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6acb20ed-9cb8-4ad0-aaf7-7798780cadd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bbox_images(images, bboxes, preds):         #  b x 3 x 480 x 640             #  b x 100 x 4\n",
    "    \n",
    "    imgs_org                      =   images.cpu().numpy() \n",
    "    batch_size, H, W, channels    =   imgs_org.shape\n",
    "    output_org                    =   np.zeros((batch_size, H, W, channels)) \n",
    "    output_pred                   =   np.zeros((batch_size, H, W, channels)) \n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        for j in range(100):\n",
    "        \n",
    "            if bboxes[i][j].sum()  == 0:\n",
    "                \n",
    "                for k in range(j):\n",
    "#--------------------------------------------------------------------------------------------------------------#\n",
    "                    \n",
    "                    top_org              =   int(round(bboxes[i][k][0].item()))\n",
    "                    left_org             =   int(round(bboxes[i][k][1].item()))\n",
    "                    height_org           =   int(round(bboxes[i][k][2].item()))\n",
    "                    width_org            =   int(round(bboxes[i][k][3].item()))\n",
    "        \n",
    "                    \n",
    "                    if top_org  < 0:\n",
    "                        top_org                =   0\n",
    "                    if left_org  < 0:\n",
    "                        left_org               =   0\n",
    "                    if top_org  + height_org  > H:\n",
    "                        height_org             =   H - top_org\n",
    "                    if left_org  + width_org  > W:\n",
    "                        width_org              =   W - left_org \n",
    "        \n",
    "                    \n",
    "                    output_org[i, top_org:top_org+height_org, left_org:left_org+width_org, :]    =   imgs_org[i, top_org:top_org+height_org, left_org:left_org+width_org, :]\n",
    "    \n",
    " \n",
    "#--------------------------------------------------------------------------------------------------------------#\n",
    "                    \n",
    "                    top                   =   int(round(preds[i][k][0].item()))\n",
    "                    left                  =   int(round(preds[i][k][1].item()))\n",
    "                    height                =   int(round(preds[i][k][2].item()))\n",
    "                    width                 =   int(round(preds[i][k][3].item())) \n",
    "                \n",
    "                    if top < 0:\n",
    "                        top               =   0\n",
    "                    if left < 0:\n",
    "                        left              =   0\n",
    "                    if top + height > H:\n",
    "                        height            =   H - top\n",
    "                    if left + width > W:\n",
    "                        width             =   W - left\n",
    "                \n",
    "                    output_pred[i, top_org:top_org+height_org, left_org:left_org+width_org, :]    =   imgs_org[i, top_org:top_org+height_org, left_org:left_org+width_org, :]\n",
    "\n",
    "                break \n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------#  \n",
    "    \n",
    "    return torch.tensor(output_org),   torch.tensor(output_pred) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21e1c2-7686-427e-9e6c-71ed0650ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_org,  out_pred   =   create_bbox_images(imgs, bboxes, bboxes ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e9026-9d42-495e-a877-a31883107d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_org.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8a032-495f-4c03-ac04-892ff2ecf1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num  =  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c6e57-2176-4fed-811f-3246efd32ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax    =   plt.subplots(1,3, figsize=(10,12))\n",
    "\n",
    "\n",
    "ax[0].imshow(imgs[num]/255.0)\n",
    "ax[1].imshow(out_org[num]/255.0 )\n",
    "ax[2].imshow(out_pred[num]/255.0 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1bf6d-63fd-4f23-bfc0-cc4201631674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a136250-bc7f-498b-b9b9-cea839acb473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0f989-d30c-465d-8aef-919033cccd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea78e341-6d98-4aee-a844-efcc8b86e351",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:10px;\">\n",
    "<h2>  Model </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06064435-07ee-4d5c-baf5-3c00306f8bca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Patch Embeddings </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b680f5-9189-4688-be55-f1af9b4851fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_non_overlapping_patches(images, patch_size):\n",
    "    \n",
    "    batch_size, channels, height, width    =   images.shape\n",
    "    num_patches_h                          =   height // patch_size  \n",
    "    num_patches_w                          =   width // patch_size  \n",
    "    num_patches                            =   num_patches_h * num_patches_w  \n",
    "\n",
    "    # Initialize a list to store patches\n",
    "    patches                              =   []\n",
    "\n",
    "    for img in images:\n",
    "        \n",
    "        img_patches                    =   []\n",
    "            \n",
    "        for i in range(num_patches_h):\n",
    "            for j in range(num_patches_w):\n",
    "                # Extract a patch with all channels\n",
    "                patch            =   img[:, i * patch_size : (i + 1) * patch_size, j * patch_size : (j + 1) * patch_size]\n",
    "                # Add the patch to the list\n",
    "                img_patches.append(patch)\n",
    "         \n",
    "        patches.append(torch.stack(img_patches))\n",
    "\n",
    "    patches                              =   torch.stack(patches)\n",
    "\n",
    "    return patches.reshape(batch_size, num_patches, channels * (patch_size ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a96c8-dc47-4d89-ac32-6318cf030c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    dummy   =   torch.zeros(8, 3, 480, 640)\n",
    "    out     =   extract_non_overlapping_patches(dummy, 32)\n",
    "    \n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e6576-23e0-48f0-b3e0-0743a6b50936",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  CNN  </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14823a14-2a0b-4359-8e94-f90b6998cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_words=0,   embed_dim=0):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_words  =  num_words\n",
    "        self.embed_dim  =  embed_dim \n",
    "        \n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "    \n",
    "                nn.Conv2d(3, 6, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(6),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),  \n",
    "                \n",
    "                nn.Conv2d(6, 9, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(9),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "            \n",
    "                nn.Conv2d(9, 12, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(12), \n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),  \n",
    "\n",
    "            )\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "        self.flatten    = nn.Flatten()\n",
    "#--------------------------------------------------------------------------------------------------------------------#        \n",
    "        self.apply(self.init_weights)\n",
    "#--------------------------------------------------------------------------------------------------------------------#    \n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        \"\"\"\n",
    "        Initialize weights for Linear and Conv2d/ConvTranspose2d layers using Kaiming initialization.\n",
    "        \"\"\"\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "#--------------------------------------------------------------------------------------------------------------------#    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x_enc  = self.encoder(x)           \n",
    "        x_flat = self.flatten(x_enc)       \n",
    "          \n",
    "        \n",
    "        return  x_flat.view(-1, self.num_words , self.embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3380c1f-4e2e-4b85-9f76-3834be3e858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    dummy   =   torch.zeros(8, 3, 480, 640)\n",
    "    e       =   CNN(100, 48 * 32) \n",
    "    #print(e.encoder(dummy).shape)\n",
    "    print(e.forward(dummy).shape)\n",
    "\n",
    "    num_params = sum(p.numel() for p in e.parameters() if p.requires_grad)\n",
    "\n",
    "    print(\"Number of trainable parameters in the model:\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca626a9c-e1ea-4175-8038-e8809427f866",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Embeddings </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9f1fb39-595b-4086-86a7-726cc73d444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_words, embed_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.num_words            =   num_words\n",
    "        self.embed_dim            =   embed_dim\n",
    "\n",
    "        self.encoder              =   CNN( num_words=num_words,   embed_dim=embed_dim)\n",
    "\n",
    "        self.position_embeddings  =   nn.Embedding(self.num_words,  self.embed_dim)    \n",
    "        self.layer_norm           =   nn.LayerNorm(self.embed_dim, eps=1e-12)              \n",
    "    \n",
    "    def forward(self, x):   \n",
    "        \n",
    "        position_ids         =  torch.arange(self.num_words, device=x.device).unsqueeze(0) \n",
    "        position_embeddings  =  self.position_embeddings(position_ids)\n",
    "\n",
    "        \n",
    "        #patches             =  extract_non_overlapping_patches(x, 32) \n",
    "        patches              =  self.encoder(x) \n",
    "\n",
    "        \n",
    "        # Combine token and position embeddings\n",
    "        \n",
    "        embeddings = patches + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        \n",
    "        return embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b979d527-5549-43d9-96fa-c205b3f25978",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    dummy   =   torch.zeros(8, 3, 480, 640)\n",
    "    e       =   Embeddings(128, 256)\n",
    "    print(e.forward(dummy).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d988ff-bd46-4b7c-817e-e8992d4d3cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a416b8-4bbd-4d27-808f-cbd9d2fb9faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a72990e1-9396-4737-9d31-a0f722bf67e9",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Scaled Dot Product </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38830803-fbfd-473b-b480-43170412639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    \n",
    "    dim_k   =  query.size(-1)\n",
    "    scores  =  torch.bmm(query, key.transpose(1, 2)) / np.sqrt(dim_k)\n",
    "    weights =  F.softmax(scores, dim=-1)\n",
    "    \n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9274fd5-eca5-4ee8-94e5-8991ebe45587",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Attention Head</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93a23dc6-f741-407c-92e0-e0fcddea260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_word, head_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_word  = dim_word\n",
    "        self.head_dim  = head_dim \n",
    "        \n",
    "        self.q = nn.Linear(self.dim_word, self.head_dim)     \n",
    "        self.k = nn.Linear(self.dim_word, self.head_dim)\n",
    "        self.v = nn.Linear(self.dim_word, self.head_dim)  \n",
    "        \n",
    "    \n",
    "    def forward(self, x):  \n",
    "     \n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        \n",
    "        attn_outputs = scaled_dot_product_attention(q, k, v)\n",
    "        \n",
    "        return attn_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097383a-60dc-4d67-9fb4-929e7393154c",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Multi-Head Attention </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5cdae9c-b1f8-427c-9a8c-805c6c7b3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim  = embed_dim // num_heads\n",
    "        self.heads     = nn.ModuleList([AttentionHead(embed_dim, self.head_dim) for _ in range(num_heads)])\n",
    "        \n",
    "        self.output_linear  = nn.Linear(embed_dim, embed_dim)\n",
    "        self.norm1          = nn.LayerNorm(embed_dim) \n",
    "        self.gelu1          = nn.GELU()\n",
    "    \n",
    "    def forward(self, hidden_state):\n",
    "    \n",
    "        head_outputs = [h(hidden_state) for h in self.heads]\n",
    "        \n",
    "        x = torch.cat(head_outputs, dim=-1)\n",
    "        \n",
    "        x = self.gelu1(self.norm1(self.output_linear(x)))   \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc7a8fa-df0f-4f67-86a0-054ad404d4d4",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  FeedForward </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb1f88e0-78cc-4c9b-b544-4a498bb00c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        \n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim \n",
    "        \n",
    "        self.linear_1       =  nn.Linear(self.embed_dim, self.embed_dim * 4)   \n",
    "        self.norm1          =  nn.LayerNorm(self.embed_dim * 4) \n",
    "        self.gelu1          =  nn.GELU()\n",
    "        \n",
    "        self.linear_2       =  nn.Linear(self.embed_dim * 4, self.embed_dim) \n",
    "        self.norm2          =  nn.LayerNorm(self.embed_dim) \n",
    "        self.gelu2          =  nn.GELU()\n",
    "        \n",
    "        self.dropout  = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        x = x.view(batch_size * seq_len, -1)  \n",
    "        \n",
    "        x = self.gelu1(self.norm1(self.linear_1(x)))\n",
    "        x = self.gelu2(self.norm2(self.linear_2(x)))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(batch_size, seq_len, -1)  \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d19e6e-3819-42d7-8129-d78770e14fe8",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Transformer Encoder Layer</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32523dfb-fa4b-4197-a40f-efb1c2cd8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim  = embed_dim\n",
    "        self.num_heads  = num_heads\n",
    "        \n",
    "        self.layer_norm_1  =  nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm_2  =  nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm_3  =  nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.attention     =  MultiHeadAttention(embed_dim, num_heads)       \n",
    "        self.feed_forward  =  FeedForward(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        \n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        \n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "\n",
    "        x = self.layer_norm_3(x) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80130fc5-6817-436b-9b98-21a1c3f11f0f",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Transformer Encoder </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e36c43e1-067d-414f-8afd-877e52c7e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_words=0, embed_dim=0, num_heads=0, n_layer=0):     \n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        if (embed_dim % num_heads) != 0:\n",
    "            print(\"Embed dimension must be divisible by num_head!\")\n",
    "            \n",
    "        self.num_words            =  num_words\n",
    "        self.embed_dim            =  embed_dim\n",
    "        self.num_heads            =  num_heads\n",
    "\n",
    "        self.embeddings = Embeddings(self.num_words, self.embed_dim) \n",
    "        self.layers     = nn.ModuleList([TransformerEncoderLayer(self.embed_dim, self.num_heads) for _ in range(n_layer)])\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embeddings(x)   \n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ceed4e41-5475-4110-902d-7e3fd5a68a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    dummy   =   torch.zeros(8, 3, 480, 640)\n",
    "    e       =   TransformerEncoder(100, 576, 8, 6)\n",
    "    print(e.forward(dummy).shape)\n",
    "\n",
    "    num_params = sum(p.numel() for p in e.parameters() if p.requires_grad)\n",
    "\n",
    "    print(\"Number of trainable parameters in the model:\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918e6d4-fe7d-4112-b711-123147c7f046",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  ResNET as Backbone </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2ac06b8-ab2e-4180-b504-f4096efa1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "\n",
    "    import torchvision.models as models\n",
    "    \n",
    "    class ResNetBackbone(nn.Module):\n",
    "        \n",
    "        def __init__(self, embed_dim):\n",
    "            super().__init__()\n",
    "            self.resnet       =   models.resnet50(weights=True)\n",
    "            self.resnet       =   nn.Sequential(*list(self.resnet.children())[:-2])\n",
    "            self.projection   =   nn.Linear(2048, embed_dim)\n",
    "            \n",
    "        def forward(self, x: Tensor) -> Tensor:\n",
    "            x                =   self.resnet(x)                      # B x 2048 x H' x W'\n",
    "            B, C, H, W       =   x.shape\n",
    "            x                =   x.view(B, C, H * W).permute(0, 2, 1)  # B x (H'*W') x 2048\n",
    "            x                =   self.projection(x)                  # B x (H'*W') x embed_dim\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5125d2-8655-468f-a3b8-889fe5493d22",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  ViT </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85eda2cd-95db-41c1-9121-70b7b90937d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \n",
    "    def __init__(self,   num_words = 0, embed_dim = 0, num_heads = 0, n_layer = 0):\n",
    "        \n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        self.embed_dim     =   embed_dim \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#\n",
    "        \n",
    "        self.encoder       =   TransformerEncoder(num_words,  embed_dim,  num_heads, n_layer)    #  input=aoutput=    batch   x   num_words   x   embed_dim      \n",
    "        \n",
    "        self.classifier1   =   nn.Linear(2 * embed_dim ,   900)  \n",
    "        \n",
    "        self.classifier2   =   nn.Linear(2 * embed_dim ,   400)\n",
    "        \n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear): \n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        \n",
    "        x   =   self.encoder(x)[:, :2,  :].view(-1,   2*self.embed_dim )                           \n",
    "        \n",
    "        clas = self.classifier1(x).sigmoid() \n",
    "        \n",
    "        bbox = self.classifier2(x)\n",
    "        \n",
    "        return    bbox.view(-1, 100, 4) ,    clas.view(-1, 100, 9) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef69272-d29d-4428-b96c-11a7bb9e5b2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Checking the Dimensions </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53751806-224f-4438-9768-b016663bc3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    m       =   ViT( num_words = 128, embed_dim = 768, num_heads=6 , n_layer=6) \n",
    "\n",
    "    dummy   =   torch.zeros(8, 3, 480, 640)\n",
    "\n",
    "    x       =   m.forward(dummy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42223bb7-0860-4401-8604-546684abdf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    dummy   =   torch.zeros(8, 3, 480, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946e6ee-fbeb-418c-ae3d-1086cc5044a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    x       =  m.forward(dummy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27257d5f-b353-40ad-a78a-a5f71f832cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    print(x[0].shape),  print(x[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41a385-386f-4a70-aa2b-df67c480cac6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Check Number of  Parameters :</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b877dd3-5ee4-4087-ac91-1baad2afca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model   =  ViT( num_words = 100, embed_dim = 576, num_heads=8 , n_layer=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b577ff-bdc3-43d4-8669-a48c99abaea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of trainable parameters in the model:\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a645fb8-5f81-4a83-9d66-1afcabef8b6c",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Employ GPU</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bd3b354-73a2-4df5-9af0-7698241676ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f680eb70-2d96-40e1-934a-1e97bece03b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c4e632c-a43a-474b-abcd-768a0c7b0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model   =  ViT( num_words = 100, embed_dim = 576, num_heads=8 , n_layer=6).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d259514-7ed2-44f1-b7e6-31ae8cdc9c77",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  To Save Weights and Losses : </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79bb52f6-5bcb-4fc0-8850-9e53a1064933",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model   =  \"/home/turgay/Turgay/Academic/2024-2025/Spring/Transformers/Assignments/Assignment_2/weights/Transformer_weight_1.pth\"\n",
    "path_losses  =  \"/home/turgay/Turgay/Academic/2024-2025/Spring/Transformers/Assignments/Assignment_2/weights/Transformer_losses_1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8994d1c-a829-4299-be70-9586104373d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    \n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'best_loss'       : 9999999999999,\n",
    "        }, path_model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b23b2f2f-5527-4a1c-869a-974b5c3ffe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    \n",
    "    torch.save({\n",
    "        \n",
    "            'train_loss'    : [],\n",
    "            'val_loss'      : [], \n",
    "    \n",
    "            'epochs'        : [],\n",
    "    \n",
    "        }, path_losses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8a37900-3496-4487-9b72-429ed0c87a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    \n",
    "    checkpoint_weights   =   torch.load(path_model, weights_only=True) \n",
    "    checkpoint_losses    =   torch.load(path_losses, weights_only=True)  \n",
    "    \n",
    "    model.load_state_dict(checkpoint_weights['model_state_dict'])\n",
    "    \n",
    "    best_loss       =  checkpoint_weights['best_loss'] \n",
    "    train_loss      =  checkpoint_losses['train_loss']\n",
    "    val_loss        =  checkpoint_losses['val_loss']\n",
    "    \n",
    "    epochs          =  checkpoint_losses['epochs'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce11b4-cc0b-4da8-944c-2780f5ae6f55",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Training : </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "11e713b7-4145-4adb-a53d-4f0b1835f362",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train2(num_epochs=0, patience=0, counter=0, learning_rate=0,     best_loss=best_loss):\n",
    "    \n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    run.log({\"LRate\": learning_rate})  \n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5) \n",
    "    scaler    = GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        model.train()\n",
    "        \n",
    "        total_loss_train = 0\n",
    "        \n",
    "        for batch in tqdm.tqdm(train_loader):\n",
    "            \n",
    "            inputs   =   batch[\"image\"]\n",
    "            labels   =   batch[\"bbox\"]      #  top, left, height, width, class \n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                \n",
    "                bbox_preds, class_preds = model(inputs)\n",
    "\n",
    "                loss(labels[\"top\"], )\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    \n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "        total_loss_train     /= len(train_loader)\n",
    "\n",
    "        train_loss.append(total_loss_train)\n",
    "    \n",
    "\n",
    "        # Log metrics to wandb.\n",
    "        run.log({\"Train Loss\": total_loss_train})\n",
    "    \n",
    "\n",
    "        if torch.isnan(torch.tensor(total_loss_train)):\n",
    "            print(\"NaN value encountered!\")\n",
    "            break\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total Train Loss : {total_loss_train:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        total_loss_val      =   0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm.tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "\n",
    "                inputs   =   batch[\"input_ids\"]\n",
    "                labels   =   batch[\"labels\"]\n",
    "            \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "\n",
    "                logits, loss = model(inputs, labels)\n",
    "\n",
    "                total_loss_val     += loss.item()\n",
    "\n",
    "        total_loss_val     /= len(val_loader)\n",
    "\n",
    "        val_loss.append(total_loss_val)\n",
    "\n",
    "        run.log({\"Val Loss\": total_loss_val})\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total Validation Loss : {total_loss_val:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        \n",
    "        if len(val_loss) >= 2:\n",
    "            res = ((val_loss[-2] - val_loss[-1]) / val_loss[-2]) * 100\n",
    "            print(\"-------------------------------------------------------------------------------\")\n",
    "            print(f\"|              Change in loss is      %   {res:.2f}                               |\")\n",
    "            print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        if  total_loss_val < best_loss:\n",
    "            \n",
    "            print(\"*************...saving best model *************\")\n",
    "            \n",
    "            best_loss = total_loss_val\n",
    "            \n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'best_loss'       : best_loss,\n",
    "            }, path_model)\n",
    "             \n",
    "\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        torch.save({\n",
    "        \n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'epochs': epochs,\n",
    "        }, path_losses)\n",
    "\n",
    "        if (len(val_loss) >= 2) and (val_loss[-2] > val_loss[-1]):\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "        scheduler.step()  \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd1ee1b2-e501-49fb-a7f3-521e8aea0331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs=0, patience=0, counter=0, learning_rate=0, best_loss=best_loss):\n",
    "    \n",
    "    optimizer       =   AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion       =   nn.BCEWithLogitsLoss() \n",
    "    \n",
    "    run.log({\"LRate\": learning_rate})  \n",
    "    \n",
    "    scheduler       =   torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.5) \n",
    "    scaler          =   GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        model.train()\n",
    "        \n",
    "        total_loss_train   =   0\n",
    "        \n",
    "        for img, bbox in tqdm.tqdm(train_loader):\n",
    "            \n",
    "            img       =     img.to(device)\n",
    "            bbox      =     bbox.to(device) \n",
    "\n",
    "            run.log({\"Batch_size\": img.shape[0]})   \n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                \n",
    "                bbox_preds, class_preds   =   model(img.view(-1, 3, 480, 640))\n",
    "                bbox_preds                =   bbox_preds.squeeze() \n",
    "                class_preds               =   class_preds.squeeze()\n",
    "                gt_bbox                   =   bbox[:, :, :4]\n",
    "                gt_class                  =   bbox[:, :, 4].long() \n",
    "                \n",
    "                gt_class_one_hot          =   F.one_hot(gt_class, num_classes=9).float()\n",
    "\n",
    "                out_org,  out_pred        =   create_bbox_images(img, gt_bbox, bbox_preds ) \n",
    "                out_org                   =   out_org.to(device)\n",
    "                out_pred                  =   out_pred.to(device) \n",
    "\n",
    "                \n",
    "                loss_imgs  =   F.mse_loss(out_org, out_pred) \n",
    "                loss_bb    =   F.mse_loss(bbox_preds  , gt_bbox)  \n",
    "                loss_cl    =   criterion(class_preds , gt_class_one_hot )  \n",
    "       \n",
    "\n",
    "                loss   =  loss_imgs +  loss_bb + loss_cl\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    \n",
    "            total_loss_train   +=   loss.item()\n",
    "\n",
    "        total_loss_train   /=   len(train_loader)\n",
    "\n",
    "        train_loss.append(total_loss_train)\n",
    "    \n",
    "        run.log({\"Train Loss\": total_loss_train})\n",
    "    \n",
    "        if torch.isnan(torch.tensor(total_loss_train)):\n",
    "            print(\"NaN value encountered!\")\n",
    "            break\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total Train Loss : {total_loss_train:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total Train Loss IMGS : {loss_imgs:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        \n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total Train Loss BBOX : {loss_bb:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |        Total Train Loss  CLS: {loss_cl:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        total_loss_val   =   0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for img, bbox in tqdm.tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "\n",
    "                img       =     img.to(device)\n",
    "                bbox      =     bbox.to(device) \n",
    "            \n",
    "                with autocast(device_type='cuda'):\n",
    "                    \n",
    "                    bbox_preds, class_preds   =   model(img.view(-1, 3, 480, 640))\n",
    "                    bbox_preds                =   bbox_preds.squeeze() \n",
    "                    class_preds               =   class_preds.squeeze()\n",
    "                    gt_bbox                   =   bbox[:, :, :4]\n",
    "                    gt_class                  =   bbox[:, :, 4].long() \n",
    "                    \n",
    "                    gt_class_one_hot          =   F.one_hot(gt_class, num_classes=9).float()\n",
    "    \n",
    "                    out_org,  out_pred        =   create_bbox_images(img, gt_bbox, bbox_preds ) \n",
    "                    out_org                   =   out_org.to(device)\n",
    "                    out_pred                  =   out_pred.to(device) \n",
    "    \n",
    "                    \n",
    "                    loss_imgs  =   F.mse_loss(out_org, out_pred) \n",
    "                    loss_bb    =   F.mse_loss(bbox_preds  , gt_bbox)  \n",
    "                    loss_cl    =   criterion(class_preds , gt_class_one_hot )  \n",
    "           \n",
    "    \n",
    "                    loss   =  loss_imgs +  loss_bb + loss_cl\n",
    "    \n",
    "                total_loss_val   +=   loss.item()\n",
    "\n",
    "        total_loss_val   /=   len(val_loader)\n",
    "\n",
    "        val_loss.append(total_loss_val)\n",
    "\n",
    "        run.log({\"Val Loss\": total_loss_val})\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]         |       Total Validation Loss : {total_loss_val:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |       Total Validation Loss IMGS : {loss_imgs:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        \n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |       Total Validation Loss BBOX : {loss_bb:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"|  Epoch [{epoch+1}/{num_epochs}]          |       Total Validation Loss  CLS: {loss_cl:.4f}           |\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        if len(val_loss) >= 2:\n",
    "            res       =   ((val_loss[-2] - val_loss[-1]) / val_loss[-2]) * 100\n",
    "            print(\"-------------------------------------------------------------------------------\")\n",
    "            print(f\"|              Change in loss is      %   {res:.2f}                               |\")\n",
    "            print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        if total_loss_val < best_loss:\n",
    "            \n",
    "            print(\"*************...saving best model *************\")\n",
    "            \n",
    "            best_loss       =   total_loss_val\n",
    "            \n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'best_loss'       : best_loss,\n",
    "            }, path_model)\n",
    "             \n",
    "\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        torch.save({\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'epochs': epochs,\n",
    "        }, path_losses)\n",
    "\n",
    "        if (len(val_loss) >= 2) and (val_loss[-2] > val_loss[-1]):\n",
    "            counter       =   0\n",
    "        else:\n",
    "            counter       +=   1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "        scheduler.step()  \n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "979442a9-97ec-419a-befd-a07663a6d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe1695-8dad-4efa-8659-db6e12c33f38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1688/1688 [08:35<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [1/10]          |        Total Train Loss : 967.3368           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [1/10]          |        Total Train Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [1/10]          |        Total Train Loss BBOX : 1518.8981           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [1/10]          |        Total Train Loss  CLS: 0.6544           |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 188/188 [00:39<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [1/10]         |       Total Validation Loss : 847.5268           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [1/10]          |       Total Validation Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [1/10]          |       Total Validation Loss BBOX : 730.4803           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [1/10]          |       Total Validation Loss  CLS: 0.6540           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|              Change in loss is      %   89.54                               |\n",
      "-------------------------------------------------------------------------------\n",
      "*************...saving best model *************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1688/1688 [08:25<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [2/10]          |        Total Train Loss : 837.2869           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [2/10]          |        Total Train Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [2/10]          |        Total Train Loss BBOX : 951.1184           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [2/10]          |        Total Train Loss  CLS: 0.6538           |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 188/188 [00:38<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [2/10]         |       Total Validation Loss : 822.4151           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [2/10]          |       Total Validation Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [2/10]          |       Total Validation Loss BBOX : 1038.1418           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [2/10]          |       Total Validation Loss  CLS: 0.6546           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|              Change in loss is      %   2.96                               |\n",
      "-------------------------------------------------------------------------------\n",
      "*************...saving best model *************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1688/1688 [08:20<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [3/10]          |        Total Train Loss : 816.1088           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [3/10]          |        Total Train Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [3/10]          |        Total Train Loss BBOX : 503.0398           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [3/10]          |        Total Train Loss  CLS: 0.6524           |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 188/188 [00:40<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [3/10]         |       Total Validation Loss : 814.2363           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [3/10]          |       Total Validation Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [3/10]          |       Total Validation Loss BBOX : 618.2286           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [3/10]          |       Total Validation Loss  CLS: 0.6529           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|              Change in loss is      %   0.99                               |\n",
      "-------------------------------------------------------------------------------\n",
      "*************...saving best model *************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1688/1688 [08:23<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [4/10]          |        Total Train Loss : 805.7710           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [4/10]          |        Total Train Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [4/10]          |        Total Train Loss BBOX : 615.3063           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [4/10]          |        Total Train Loss  CLS: 0.6533           |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 188/188 [00:39<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [4/10]         |       Total Validation Loss : 792.8203           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [4/10]          |       Total Validation Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [4/10]          |       Total Validation Loss BBOX : 889.8297           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [4/10]          |       Total Validation Loss  CLS: 0.6533           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|              Change in loss is      %   2.63                               |\n",
      "-------------------------------------------------------------------------------\n",
      "*************...saving best model *************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1688/1688 [08:17<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [5/10]          |        Total Train Loss : 795.9007           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [5/10]          |        Total Train Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [5/10]          |        Total Train Loss BBOX : 528.0934           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [5/10]          |        Total Train Loss  CLS: 0.6532           |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 188/188 [00:39<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [5/10]         |       Total Validation Loss : 789.7721           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [5/10]          |       Total Validation Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [5/10]          |       Total Validation Loss BBOX : 403.8218           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [5/10]          |       Total Validation Loss  CLS: 0.6526           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|              Change in loss is      %   0.38                               |\n",
      "-------------------------------------------------------------------------------\n",
      "*************...saving best model *************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1688/1688 [08:22<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [6/10]          |        Total Train Loss : 785.7426           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [6/10]          |        Total Train Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [6/10]          |        Total Train Loss BBOX : 311.3593           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [6/10]          |        Total Train Loss  CLS: 0.6525           |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 188/188 [00:39<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [6/10]         |       Total Validation Loss : 801.7642           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [6/10]          |       Total Validation Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [6/10]          |       Total Validation Loss BBOX : 487.1327           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [6/10]          |       Total Validation Loss  CLS: 0.6530           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|              Change in loss is      %   -1.52                               |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1688/1688 [08:19<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [7/10]          |        Total Train Loss : 776.7966           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [7/10]          |        Total Train Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [7/10]          |        Total Train Loss BBOX : 480.9206           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [7/10]          |        Total Train Loss  CLS: 0.6521           |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 188/188 [00:39<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [7/10]         |       Total Validation Loss : 770.5653           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [7/10]          |       Total Validation Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [7/10]          |       Total Validation Loss BBOX : 772.5240           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [7/10]          |       Total Validation Loss  CLS: 0.6535           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|              Change in loss is      %   3.89                               |\n",
      "-------------------------------------------------------------------------------\n",
      "*************...saving best model *************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1688/1688 [08:18<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [8/10]          |        Total Train Loss : 770.7769           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [8/10]          |        Total Train Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [8/10]          |        Total Train Loss BBOX : 319.0444           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [8/10]          |        Total Train Loss  CLS: 0.6524           |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 188/188 [00:40<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [8/10]         |       Total Validation Loss : 778.4312           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [8/10]          |       Total Validation Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [8/10]          |       Total Validation Loss BBOX : 1106.0686           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [8/10]          |       Total Validation Loss  CLS: 0.6562           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|              Change in loss is      %   -1.02                               |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1688/1688 [08:12<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [9/10]          |        Total Train Loss : 757.7382           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [9/10]          |        Total Train Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [9/10]          |        Total Train Loss BBOX : 916.2335           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [9/10]          |        Total Train Loss  CLS: 0.6544           |\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 188/188 [00:42<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [9/10]         |       Total Validation Loss : 769.2489           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [9/10]          |       Total Validation Loss IMGS : 0.0000           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [9/10]          |       Total Validation Loss BBOX : 645.5340           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|  Epoch [9/10]          |       Total Validation Loss  CLS: 0.6536           |\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "|              Change in loss is      %   1.18                               |\n",
      "-------------------------------------------------------------------------------\n",
      "*************...saving best model *************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 944/1688 [04:49<03:48,  3.26it/s]"
     ]
    }
   ],
   "source": [
    "train(num_epochs=10, patience=10, counter=0, learning_rate=0.001, best_loss=best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338c52e-962a-4f18-b319-d96efda454fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "851949d8-e7a4-4ffc-968c-e29997c66819",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Results : </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418b3bc-38be-4c64-98a2-17f6c3491811",
   "metadata": {},
   "outputs": [],
   "source": [
    "e  =  model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516cc157-16a1-4df9-a80e-95447885528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e06bd-1fb5-4c49-a57b-c0751b32976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40364fe8-7e25-4a61-a5ed-4fdc8bcb6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69915388-c7a5-4ffa-95ef-d4a6e5e62a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num  =  4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f687fde-4239-46a6-9563-6d09442faf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "\n",
    "ax.imshow(i[num]/255.0)\n",
    "\n",
    "for k in range(100):\n",
    "    \n",
    "    rect = patches.Rectangle((j[num][k][1], j[num][k][0]), j[num][k][3], j[num][k][2], linewidth=2, edgecolor='r', facecolor='none')\n",
    "    \n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0605d-e28e-4d07-9b1f-0df5cdaa2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    img       =     (i[num]/255.0).to(device) \n",
    "\n",
    "    bbox_preds, class_preds   =   model(img.view(-1, 3, 480, 640))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f39c34-3212-455c-9413-ed172078c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb4b1f-c509-42ca-8e17-8688935077db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "\n",
    "ax.imshow(i[num]/255.0)\n",
    "\n",
    "for k in range(100):\n",
    "    \n",
    "    rect = patches.Rectangle((j[num][k][1], j[num][k][0]), j[num][k][3], j[num][k][2], linewidth=2, edgecolor='r', facecolor='none')\n",
    "    \n",
    "    ax.add_patch(rect)\n",
    "\n",
    "\n",
    "for k in range(100):\n",
    "    \n",
    "    rect = patches.Rectangle((bbox_preds[0][k][1].cpu(), bbox_preds[0][k][0].cpu()), bbox_preds[0][k][3].cpu(), bbox_preds[0][k][2].cpu(), linewidth=2, edgecolor='b', facecolor='none')\n",
    "    \n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d13860-2a73-4d9c-9346-b8e5860d217f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef05cd-14a1-4ac7-bea8-dbc20233ecd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f664d10-cbf1-4856-ae44-101ca96fc087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5307a629-28db-4631-97c2-51b599bbec7d",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Evaluation Metrices</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c994e3-02ba-4bb0-a5f1-9c68312d8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    # box1: [N, 4], box2: [M, 4] in format (top, left, height, width)\n",
    "    box1_y1  = box1[:, 0]\n",
    "    box1_x1  = box1[:, 1]\n",
    "    box1_y2  = box1[:, 0] + box1[:, 2]\n",
    "    box1_x2  = box1[:, 1] + box1[:, 3]\n",
    "\n",
    "    box2_y1  = box2[:, 0]\n",
    "    box2_x1  = box2[:, 1]\n",
    "    box2_y2  = box2[:, 0] + box2[:, 2]\n",
    "    box2_x2  = box2[:, 1] + box2[:, 3]\n",
    "\n",
    "    y1 = torch.max(box1_y1.unsqueeze(1), box2_y1.unsqueeze(0))\n",
    "    x1 = torch.max(box1_x1.unsqueeze(1), box2_x1.unsqueeze(0))\n",
    "    y2 = torch.min(box1_y2.unsqueeze(1), box2_y2.unsqueeze(0))\n",
    "    x2 = torch.min(box1_x2.unsqueeze(1), box2_x2.unsqueeze(0))\n",
    "    \n",
    "    inter_h = (y2 - y1).clamp(min=0)\n",
    "    inter_w = (x2 - x1).clamp(min=0)\n",
    "    inter_area = inter_h * inter_w\n",
    "\n",
    "    area1 = box1[:, 2] * box1[:, 3]\n",
    "    area2 = box2[:, 2] * box2[:, 3]\n",
    "    union_area = area1.unsqueeze(1) + area2.unsqueeze(0) - inter_area\n",
    "\n",
    "    iou = inter_area / (union_area + 1e-6)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91293c2-63d7-4b9c-a707-4045e7acfa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image(pred_boxes, pred_scores, pred_classes, gt_boxes, gt_classes, iou_thresh=0.5):\n",
    "    # pred_boxes: [N, 4], pred_scores: [N], pred_classes: [N]\n",
    "    # gt_boxes: [M, 4], gt_classes: [M]\n",
    "    N = pred_boxes.shape[0]\n",
    "    M = gt_boxes.shape[0]\n",
    "    if N == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    iou_matrix = compute_iou(pred_boxes, gt_boxes)\n",
    "    matches = []\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            if iou_matrix[i, j] >= iou_thresh and pred_classes[i] == gt_classes[j]:\n",
    "                matches.append((pred_scores[i].item(), i, j, iou_matrix[i, j].item()))\n",
    "    matches = sorted(matches, key=lambda x: x[0], reverse=True)\n",
    "    used_gt = set()\n",
    "    tp = 0\n",
    "    for score, i, j, iou_val in matches:\n",
    "        if j not in used_gt:\n",
    "            tp += 1\n",
    "            used_gt.add(j)\n",
    "    fp = N - tp\n",
    "    fn = M - len(used_gt)\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall    = tp / (tp + fn + 1e-6)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0fcc4-a702-4742-92f4-96f4a5330967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ap(precisions, recalls):\n",
    "    # 11-point interpolated average precision\n",
    "    ap = 0.0\n",
    "    for t in np.linspace(0, 1, 11):\n",
    "        p = precisions[recalls >= t]\n",
    "        if p.numel() > 0:\n",
    "            p_max = p.max().item()\n",
    "        else:\n",
    "            p_max = 0.0\n",
    "        ap += p_max\n",
    "    ap = ap / 11.0\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9af567-9ba9-4ce8-828f-e14033ea97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(preds, gts, iou_thresh=0.5, num_classes=9):\n",
    "    # preds: list of dict, each with keys: 'boxes': [N,4], 'scores': [N], 'classes': [N]\n",
    "    # gts:   list of dict, each with keys: 'boxes': [M,4], 'classes': [M]\n",
    "    aps = []\n",
    "    for c in range(num_classes):\n",
    "        all_precisions = []\n",
    "        all_recalls    = []\n",
    "        for pred, gt in zip(preds, gts):\n",
    "            if pred['boxes'].numel() == 0 or gt['boxes'].numel() == 0:\n",
    "                continue\n",
    "            # Select class c predictions and ground truth\n",
    "            idx_pred = (pred['classes'] == c).nonzero(as_tuple=True)[0]\n",
    "            idx_gt   = (gt['classes'] == c).nonzero(as_tuple=True)[0]\n",
    "            if idx_pred.numel() == 0 or idx_gt.numel() == 0:\n",
    "                continue\n",
    "            pred_boxes   = pred['boxes'][idx_pred]\n",
    "            pred_scores  = pred['scores'][idx_pred]\n",
    "            pred_cls     = pred['classes'][idx_pred]\n",
    "            gt_boxes     = gt['boxes'][idx_gt]\n",
    "            gt_cls       = gt['classes'][idx_gt]\n",
    "            p, r, _      = evaluate_image(pred_boxes, pred_scores, pred_cls, gt_boxes, gt_cls, iou_thresh)\n",
    "            all_precisions.append(p)\n",
    "            all_recalls.append(r)\n",
    "        if len(all_precisions) > 0:\n",
    "            precisions = torch.tensor(all_precisions)\n",
    "            recalls    = torch.tensor(all_recalls)\n",
    "            ap = compute_ap(precisions, recalls)\n",
    "        else:\n",
    "            ap = 0.0\n",
    "        aps.append(ap)\n",
    "    mAP = np.mean(aps)\n",
    "    return mAP, aps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c28e09-819c-430a-baee-685f53cc17ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b591af-a8ea-41a1-ad80-a45d804894f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80e8a5-3aca-4f28-9866-0cc633f9dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Suppose preds_list and gts_list are lists over the validation dataset, each element is a dict:\n",
    "# preds_list[i] = {'boxes': tensor of shape [N,4], 'scores': tensor of shape [N], 'classes': tensor of shape [N]}\n",
    "# gts_list[i]   = {'boxes': tensor of shape [M,4], 'classes': tensor of shape [M]}\n",
    "# mAP, per_class_ap = evaluate_dataset(preds_list, gts_list, iou_thresh=0.5, num_classes=9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8afd358-4f32-4b69-b8b2-25d61fbf881f",
   "metadata": {},
   "source": [
    "<div span style=\"background-color:yellow;    color:red;      text-align:center;    padding:5px;\">\n",
    "<h2>  Sync for WANDB</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff60ce-0cd6-4de3-bf37-14260195a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c71cef-138d-4560-9044-a048bd2b620c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
