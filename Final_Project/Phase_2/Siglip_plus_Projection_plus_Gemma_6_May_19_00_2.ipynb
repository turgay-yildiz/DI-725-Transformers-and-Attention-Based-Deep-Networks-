{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:39:52.803053Z",
     "iopub.status.busy": "2025-05-06T07:39:52.802494Z",
     "iopub.status.idle": "2025-05-06T07:39:52.809924Z",
     "shell.execute_reply": "2025-05-06T07:39:52.809167Z",
     "shell.execute_reply.started": "2025-05-06T07:39:52.803032Z"
    },
    "id": "UHflqji43qh9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:47:50.075028Z",
     "iopub.status.busy": "2025-05-02T06:47:50.074537Z",
     "iopub.status.idle": "2025-05-02T06:47:50.080173Z",
     "shell.execute_reply": "2025-05-02T06:47:50.079280Z",
     "shell.execute_reply.started": "2025-05-02T06:47:50.075003Z"
    },
    "id": "UN1gBnmQ4LAj",
    "outputId": "a6ca43c0-0430-484f-bc05-665711eea867"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h1 style=\"color:red;\">DI-725 : Transformers and Attention-Based Deep Networks</h1>\n",
    "  <h2 style=\"color:red;\">Final Project : Phase - 2</h2>\n",
    "  <br>\n",
    "  <h2 style=\"color:red;\">Siglip + Projection + Gemma </h2>\n",
    "    <br><br> \n",
    "  <h4 style=\"color:red;\">Turgay Yıldız</h4>\n",
    "  <br>\n",
    "  <h4 style=\"color:red;\">Graduate School of Informatics, Middle East Technical University (METU)</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:47:11.957103Z",
     "iopub.status.busy": "2025-05-02T06:47:11.956430Z",
     "iopub.status.idle": "2025-05-02T06:47:11.961543Z",
     "shell.execute_reply": "2025-05-02T06:47:11.960914Z",
     "shell.execute_reply.started": "2025-05-02T06:47:11.957080Z"
    },
    "id": "lM_sMe2M4uhH",
    "outputId": "fb87cfd2-03e6-4c9e-b395-064dac894355"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\">Fetch big_vision code and install dependencies</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:41:00.375147Z",
     "iopub.status.busy": "2025-05-06T07:41:00.374981Z",
     "iopub.status.idle": "2025-05-06T07:41:00.379571Z",
     "shell.execute_reply": "2025-05-06T07:41:00.378928Z",
     "shell.execute_reply.started": "2025-05-06T07:41:00.375133Z"
    },
    "id": "0EJMa-2k-UuK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import kagglehub\n",
    "from google.colab import userdata\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:41:00.380506Z",
     "iopub.status.busy": "2025-05-06T07:41:00.380313Z",
     "iopub.status.idle": "2025-05-06T07:41:00.395508Z",
     "shell.execute_reply": "2025-05-06T07:41:00.394808Z",
     "shell.execute_reply.started": "2025-05-06T07:41:00.380491Z"
    },
    "id": "JlP2ZBh3-UoM"
   },
   "outputs": [],
   "source": [
    ";# Fetch big_vision repository if python doesn't know about it and install\n",
    "# dependencies needed for this notebook.\n",
    "if not os.path.exists(\"big_vision_repo\"):\n",
    "  !git clone --quiet --branch=main --depth=1 \\\n",
    "     https://github.com/google-research/big_vision big_vision_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:41:00.396434Z",
     "iopub.status.busy": "2025-05-06T07:41:00.396237Z",
     "iopub.status.idle": "2025-05-06T07:41:00.408594Z",
     "shell.execute_reply": "2025-05-06T07:41:00.408033Z",
     "shell.execute_reply.started": "2025-05-06T07:41:00.396418Z"
    },
    "id": "hjlUuo-E-UlP"
   },
   "outputs": [],
   "source": [
    "# Append big_vision code to python import path\n",
    "if \"big_vision_repo\" not in sys.path:\n",
    "  sys.path.append(\"big_vision_repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:41:00.409479Z",
     "iopub.status.busy": "2025-05-06T07:41:00.409285Z",
     "iopub.status.idle": "2025-05-06T07:41:03.394105Z",
     "shell.execute_reply": "2025-05-06T07:41:03.393015Z",
     "shell.execute_reply.started": "2025-05-06T07:41:00.409463Z"
    },
    "id": "xAKDOz6_-UKV"
   },
   "outputs": [],
   "source": [
    "# Install missing dependencies. Assume jax~=0.4.25 with GPU available.\n",
    "!pip3 install -q \"overrides\" \"ml_collections\" \"einops~=0.7\" \"sentencepiece\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHH3Ja8F5DoX",
    "outputId": "515a14ea-dfc9-4fa9-fa18-e3d391f0effc"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Model and Pre-trained weights : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:41:03.396517Z",
     "iopub.status.busy": "2025-05-06T07:41:03.395515Z",
     "iopub.status.idle": "2025-05-06T07:41:03.400667Z",
     "shell.execute_reply": "2025-05-06T07:41:03.399883Z",
     "shell.execute_reply.started": "2025-05-06T07:41:03.396479Z"
    },
    "id": "cxuGG9Ni-GyY"
   },
   "outputs": [],
   "source": [
    "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
    "# vars as appropriate or make your credentials available in ~/.kaggle/kaggle.json\n",
    "\n",
    "os.environ[\"...\"]      =    '...'\n",
    "os.environ[\"...\"]      =    '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:41:03.401801Z",
     "iopub.status.busy": "2025-05-06T07:41:03.401481Z",
     "iopub.status.idle": "2025-05-06T07:41:03.505488Z",
     "shell.execute_reply": "2025-05-06T07:41:03.504972Z",
     "shell.execute_reply.started": "2025-05-06T07:41:03.401751Z"
    },
    "id": "TZ2gt5i4IrXs"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# replace \"hf_XXX\" with your actual token (keep it secret!)\n",
    "login(token=\"...\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:41:03.506283Z",
     "iopub.status.busy": "2025-05-06T07:41:03.506093Z",
     "iopub.status.idle": "2025-05-06T07:41:03.509837Z",
     "shell.execute_reply": "2025-05-06T07:41:03.509064Z",
     "shell.execute_reply.started": "2025-05-06T07:41:03.506268Z"
    },
    "id": "Lbpsmqbd-GvR"
   },
   "outputs": [],
   "source": [
    "# The T4 runtime is tight on memory to finetune this model. Preallocate\n",
    "# all memory ahead of time to avoid OOM'ing due to fragmentation.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:41:03.510882Z",
     "iopub.status.busy": "2025-05-06T07:41:03.510625Z",
     "iopub.status.idle": "2025-05-06T07:41:03.522742Z",
     "shell.execute_reply": "2025-05-06T07:41:03.522042Z",
     "shell.execute_reply.started": "2025-05-06T07:41:03.510865Z"
    },
    "id": "5GOVNxRG-Gpf"
   },
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = \"./paligemma_tokenizer.model\"\n",
    "if not os.path.exists(TOKENIZER_PATH):\n",
    "    print(\"Downloading the model tokenizer...\")\n",
    "    !wget https://storage.googleapis.com/big_vision/paligemma_tokenizer.model -O {TOKENIZER_PATH}\n",
    "    print(f\"Tokenizer path: {TOKENIZER_PATH}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHnt7Ak6FdLd",
    "outputId": "366fa51a-ffde-49dc-bebf-e7f36fd8ee7d"
   },
   "outputs": [],
   "source": [
    "#!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> SIGLIP</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:01:03.629943Z",
     "iopub.status.busy": "2025-05-06T08:01:03.629450Z",
     "iopub.status.idle": "2025-05-06T08:01:04.433880Z",
     "shell.execute_reply": "2025-05-06T08:01:04.433261Z",
     "shell.execute_reply.started": "2025-05-06T08:01:03.629920Z"
    },
    "id": "lwS8F7Y5-Gmy"
   },
   "outputs": [],
   "source": [
    "siglip = AutoModel.from_pretrained(\n",
    "                                  \"google/siglip2-base-patch16-224\",\n",
    "                                  #quantization_config = bnb_config,\n",
    "                                  torch_dtype         = torch.float16,\n",
    "                                  device_map          = \"cpu\",\n",
    "                                  attn_implementation = \"sdpa\",\n",
    "                              ).eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:41:04.447282Z",
     "iopub.status.busy": "2025-05-06T07:41:04.447093Z",
     "iopub.status.idle": "2025-05-06T07:41:08.893140Z",
     "shell.execute_reply": "2025-05-06T07:41:08.892524Z",
     "shell.execute_reply.started": "2025-05-06T07:41:04.447267Z"
    },
    "id": "F1Q8wDi5lpfV"
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\",  use_fast=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T06:18:55.202913Z",
     "iopub.status.busy": "2025-05-05T06:18:55.202230Z",
     "iopub.status.idle": "2025-05-05T06:18:55.208589Z",
     "shell.execute_reply": "2025-05-05T06:18:55.207643Z",
     "shell.execute_reply.started": "2025-05-05T06:18:55.202891Z"
    }
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Freeze : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-06T07:43:25.973627Z",
     "iopub.status.busy": "2025-05-06T07:43:25.972883Z",
     "iopub.status.idle": "2025-05-06T07:43:25.980433Z",
     "shell.execute_reply": "2025-05-06T07:43:25.979630Z",
     "shell.execute_reply.started": "2025-05-06T07:43:25.973587Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_scale\n",
      "logit_bias\n",
      "text_model.embeddings.token_embedding.weight\n",
      "text_model.embeddings.position_embedding.weight\n",
      "text_model.encoder.layers.0.layer_norm1.weight\n",
      "text_model.encoder.layers.0.layer_norm1.bias\n",
      "text_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.0.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.0.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.0.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.0.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.0.layer_norm2.weight\n",
      "text_model.encoder.layers.0.layer_norm2.bias\n",
      "text_model.encoder.layers.0.mlp.fc1.weight\n",
      "text_model.encoder.layers.0.mlp.fc1.bias\n",
      "text_model.encoder.layers.0.mlp.fc2.weight\n",
      "text_model.encoder.layers.0.mlp.fc2.bias\n",
      "text_model.encoder.layers.1.layer_norm1.weight\n",
      "text_model.encoder.layers.1.layer_norm1.bias\n",
      "text_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.1.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.1.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.1.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.1.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.1.layer_norm2.weight\n",
      "text_model.encoder.layers.1.layer_norm2.bias\n",
      "text_model.encoder.layers.1.mlp.fc1.weight\n",
      "text_model.encoder.layers.1.mlp.fc1.bias\n",
      "text_model.encoder.layers.1.mlp.fc2.weight\n",
      "text_model.encoder.layers.1.mlp.fc2.bias\n",
      "text_model.encoder.layers.2.layer_norm1.weight\n",
      "text_model.encoder.layers.2.layer_norm1.bias\n",
      "text_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.2.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.2.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.2.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.2.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.2.layer_norm2.weight\n",
      "text_model.encoder.layers.2.layer_norm2.bias\n",
      "text_model.encoder.layers.2.mlp.fc1.weight\n",
      "text_model.encoder.layers.2.mlp.fc1.bias\n",
      "text_model.encoder.layers.2.mlp.fc2.weight\n",
      "text_model.encoder.layers.2.mlp.fc2.bias\n",
      "text_model.encoder.layers.3.layer_norm1.weight\n",
      "text_model.encoder.layers.3.layer_norm1.bias\n",
      "text_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.3.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.3.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.3.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.3.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.3.layer_norm2.weight\n",
      "text_model.encoder.layers.3.layer_norm2.bias\n",
      "text_model.encoder.layers.3.mlp.fc1.weight\n",
      "text_model.encoder.layers.3.mlp.fc1.bias\n",
      "text_model.encoder.layers.3.mlp.fc2.weight\n",
      "text_model.encoder.layers.3.mlp.fc2.bias\n",
      "text_model.encoder.layers.4.layer_norm1.weight\n",
      "text_model.encoder.layers.4.layer_norm1.bias\n",
      "text_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.4.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.4.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.4.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.4.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.4.layer_norm2.weight\n",
      "text_model.encoder.layers.4.layer_norm2.bias\n",
      "text_model.encoder.layers.4.mlp.fc1.weight\n",
      "text_model.encoder.layers.4.mlp.fc1.bias\n",
      "text_model.encoder.layers.4.mlp.fc2.weight\n",
      "text_model.encoder.layers.4.mlp.fc2.bias\n",
      "text_model.encoder.layers.5.layer_norm1.weight\n",
      "text_model.encoder.layers.5.layer_norm1.bias\n",
      "text_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.5.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.5.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.5.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.5.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.5.layer_norm2.weight\n",
      "text_model.encoder.layers.5.layer_norm2.bias\n",
      "text_model.encoder.layers.5.mlp.fc1.weight\n",
      "text_model.encoder.layers.5.mlp.fc1.bias\n",
      "text_model.encoder.layers.5.mlp.fc2.weight\n",
      "text_model.encoder.layers.5.mlp.fc2.bias\n",
      "text_model.encoder.layers.6.layer_norm1.weight\n",
      "text_model.encoder.layers.6.layer_norm1.bias\n",
      "text_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.6.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.6.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.6.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.6.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.6.layer_norm2.weight\n",
      "text_model.encoder.layers.6.layer_norm2.bias\n",
      "text_model.encoder.layers.6.mlp.fc1.weight\n",
      "text_model.encoder.layers.6.mlp.fc1.bias\n",
      "text_model.encoder.layers.6.mlp.fc2.weight\n",
      "text_model.encoder.layers.6.mlp.fc2.bias\n",
      "text_model.encoder.layers.7.layer_norm1.weight\n",
      "text_model.encoder.layers.7.layer_norm1.bias\n",
      "text_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.7.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.7.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.7.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.7.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.7.layer_norm2.weight\n",
      "text_model.encoder.layers.7.layer_norm2.bias\n",
      "text_model.encoder.layers.7.mlp.fc1.weight\n",
      "text_model.encoder.layers.7.mlp.fc1.bias\n",
      "text_model.encoder.layers.7.mlp.fc2.weight\n",
      "text_model.encoder.layers.7.mlp.fc2.bias\n",
      "text_model.encoder.layers.8.layer_norm1.weight\n",
      "text_model.encoder.layers.8.layer_norm1.bias\n",
      "text_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.8.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.8.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.8.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.8.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.8.layer_norm2.weight\n",
      "text_model.encoder.layers.8.layer_norm2.bias\n",
      "text_model.encoder.layers.8.mlp.fc1.weight\n",
      "text_model.encoder.layers.8.mlp.fc1.bias\n",
      "text_model.encoder.layers.8.mlp.fc2.weight\n",
      "text_model.encoder.layers.8.mlp.fc2.bias\n",
      "text_model.encoder.layers.9.layer_norm1.weight\n",
      "text_model.encoder.layers.9.layer_norm1.bias\n",
      "text_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.9.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.9.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.9.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.9.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.9.layer_norm2.weight\n",
      "text_model.encoder.layers.9.layer_norm2.bias\n",
      "text_model.encoder.layers.9.mlp.fc1.weight\n",
      "text_model.encoder.layers.9.mlp.fc1.bias\n",
      "text_model.encoder.layers.9.mlp.fc2.weight\n",
      "text_model.encoder.layers.9.mlp.fc2.bias\n",
      "text_model.encoder.layers.10.layer_norm1.weight\n",
      "text_model.encoder.layers.10.layer_norm1.bias\n",
      "text_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.10.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.10.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.10.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.10.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.10.layer_norm2.weight\n",
      "text_model.encoder.layers.10.layer_norm2.bias\n",
      "text_model.encoder.layers.10.mlp.fc1.weight\n",
      "text_model.encoder.layers.10.mlp.fc1.bias\n",
      "text_model.encoder.layers.10.mlp.fc2.weight\n",
      "text_model.encoder.layers.10.mlp.fc2.bias\n",
      "text_model.encoder.layers.11.layer_norm1.weight\n",
      "text_model.encoder.layers.11.layer_norm1.bias\n",
      "text_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "text_model.encoder.layers.11.self_attn.k_proj.bias\n",
      "text_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "text_model.encoder.layers.11.self_attn.v_proj.bias\n",
      "text_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "text_model.encoder.layers.11.self_attn.q_proj.bias\n",
      "text_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "text_model.encoder.layers.11.self_attn.out_proj.bias\n",
      "text_model.encoder.layers.11.layer_norm2.weight\n",
      "text_model.encoder.layers.11.layer_norm2.bias\n",
      "text_model.encoder.layers.11.mlp.fc1.weight\n",
      "text_model.encoder.layers.11.mlp.fc1.bias\n",
      "text_model.encoder.layers.11.mlp.fc2.weight\n",
      "text_model.encoder.layers.11.mlp.fc2.bias\n",
      "text_model.final_layer_norm.weight\n",
      "text_model.final_layer_norm.bias\n",
      "text_model.head.weight\n",
      "text_model.head.bias\n",
      "vision_model.embeddings.patch_embedding.weight\n",
      "vision_model.embeddings.patch_embedding.bias\n",
      "vision_model.embeddings.position_embedding.weight\n",
      "vision_model.encoder.layers.0.layer_norm1.weight\n",
      "vision_model.encoder.layers.0.layer_norm1.bias\n",
      "vision_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.0.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.0.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.0.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.0.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.0.layer_norm2.weight\n",
      "vision_model.encoder.layers.0.layer_norm2.bias\n",
      "vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "vision_model.encoder.layers.0.mlp.fc1.bias\n",
      "vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "vision_model.encoder.layers.0.mlp.fc2.bias\n",
      "vision_model.encoder.layers.1.layer_norm1.weight\n",
      "vision_model.encoder.layers.1.layer_norm1.bias\n",
      "vision_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.1.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.1.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.1.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.1.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.1.layer_norm2.weight\n",
      "vision_model.encoder.layers.1.layer_norm2.bias\n",
      "vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "vision_model.encoder.layers.1.mlp.fc1.bias\n",
      "vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "vision_model.encoder.layers.1.mlp.fc2.bias\n",
      "vision_model.encoder.layers.2.layer_norm1.weight\n",
      "vision_model.encoder.layers.2.layer_norm1.bias\n",
      "vision_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.2.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.2.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.2.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.2.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.2.layer_norm2.weight\n",
      "vision_model.encoder.layers.2.layer_norm2.bias\n",
      "vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "vision_model.encoder.layers.2.mlp.fc1.bias\n",
      "vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "vision_model.encoder.layers.2.mlp.fc2.bias\n",
      "vision_model.encoder.layers.3.layer_norm1.weight\n",
      "vision_model.encoder.layers.3.layer_norm1.bias\n",
      "vision_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.3.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.3.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.3.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.3.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.3.layer_norm2.weight\n",
      "vision_model.encoder.layers.3.layer_norm2.bias\n",
      "vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "vision_model.encoder.layers.3.mlp.fc1.bias\n",
      "vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "vision_model.encoder.layers.3.mlp.fc2.bias\n",
      "vision_model.encoder.layers.4.layer_norm1.weight\n",
      "vision_model.encoder.layers.4.layer_norm1.bias\n",
      "vision_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.4.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.4.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.4.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.4.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.4.layer_norm2.weight\n",
      "vision_model.encoder.layers.4.layer_norm2.bias\n",
      "vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "vision_model.encoder.layers.4.mlp.fc1.bias\n",
      "vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "vision_model.encoder.layers.4.mlp.fc2.bias\n",
      "vision_model.encoder.layers.5.layer_norm1.weight\n",
      "vision_model.encoder.layers.5.layer_norm1.bias\n",
      "vision_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.5.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.5.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.5.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.5.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.5.layer_norm2.weight\n",
      "vision_model.encoder.layers.5.layer_norm2.bias\n",
      "vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "vision_model.encoder.layers.5.mlp.fc1.bias\n",
      "vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "vision_model.encoder.layers.5.mlp.fc2.bias\n",
      "vision_model.encoder.layers.6.layer_norm1.weight\n",
      "vision_model.encoder.layers.6.layer_norm1.bias\n",
      "vision_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.6.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.6.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.6.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.6.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.6.layer_norm2.weight\n",
      "vision_model.encoder.layers.6.layer_norm2.bias\n",
      "vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "vision_model.encoder.layers.6.mlp.fc1.bias\n",
      "vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "vision_model.encoder.layers.6.mlp.fc2.bias\n",
      "vision_model.encoder.layers.7.layer_norm1.weight\n",
      "vision_model.encoder.layers.7.layer_norm1.bias\n",
      "vision_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.7.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.7.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.7.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.7.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.7.layer_norm2.weight\n",
      "vision_model.encoder.layers.7.layer_norm2.bias\n",
      "vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "vision_model.encoder.layers.7.mlp.fc1.bias\n",
      "vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "vision_model.encoder.layers.7.mlp.fc2.bias\n",
      "vision_model.encoder.layers.8.layer_norm1.weight\n",
      "vision_model.encoder.layers.8.layer_norm1.bias\n",
      "vision_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.8.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.8.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.8.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.8.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.8.layer_norm2.weight\n",
      "vision_model.encoder.layers.8.layer_norm2.bias\n",
      "vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "vision_model.encoder.layers.8.mlp.fc1.bias\n",
      "vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "vision_model.encoder.layers.8.mlp.fc2.bias\n",
      "vision_model.encoder.layers.9.layer_norm1.weight\n",
      "vision_model.encoder.layers.9.layer_norm1.bias\n",
      "vision_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.9.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.9.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.9.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.9.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.9.layer_norm2.weight\n",
      "vision_model.encoder.layers.9.layer_norm2.bias\n",
      "vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "vision_model.encoder.layers.9.mlp.fc1.bias\n",
      "vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "vision_model.encoder.layers.9.mlp.fc2.bias\n",
      "vision_model.encoder.layers.10.layer_norm1.weight\n",
      "vision_model.encoder.layers.10.layer_norm1.bias\n",
      "vision_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.10.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.10.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.10.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.10.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.10.layer_norm2.weight\n",
      "vision_model.encoder.layers.10.layer_norm2.bias\n",
      "vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "vision_model.encoder.layers.10.mlp.fc1.bias\n",
      "vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "vision_model.encoder.layers.10.mlp.fc2.bias\n",
      "vision_model.encoder.layers.11.layer_norm1.weight\n",
      "vision_model.encoder.layers.11.layer_norm1.bias\n",
      "vision_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "vision_model.encoder.layers.11.self_attn.k_proj.bias\n",
      "vision_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "vision_model.encoder.layers.11.self_attn.v_proj.bias\n",
      "vision_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "vision_model.encoder.layers.11.self_attn.q_proj.bias\n",
      "vision_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "vision_model.encoder.layers.11.self_attn.out_proj.bias\n",
      "vision_model.encoder.layers.11.layer_norm2.weight\n",
      "vision_model.encoder.layers.11.layer_norm2.bias\n",
      "vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "vision_model.encoder.layers.11.mlp.fc1.bias\n",
      "vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "vision_model.encoder.layers.11.mlp.fc2.bias\n",
      "vision_model.post_layernorm.weight\n",
      "vision_model.post_layernorm.bias\n",
      "vision_model.head.probe\n",
      "vision_model.head.attention.in_proj_weight\n",
      "vision_model.head.attention.in_proj_bias\n",
      "vision_model.head.attention.out_proj.weight\n",
      "vision_model.head.attention.out_proj.bias\n",
      "vision_model.head.layernorm.weight\n",
      "vision_model.head.layernorm.bias\n",
      "vision_model.head.mlp.fc1.weight\n",
      "vision_model.head.mlp.fc1.bias\n",
      "vision_model.head.mlp.fc2.weight\n",
      "vision_model.head.mlp.fc2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in siglip.named_parameters():\n",
    "\n",
    "    print(name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:01:09.688984Z",
     "iopub.status.busy": "2025-05-06T08:01:09.688225Z",
     "iopub.status.idle": "2025-05-06T08:01:09.693929Z",
     "shell.execute_reply": "2025-05-06T08:01:09.693120Z",
     "shell.execute_reply.started": "2025-05-06T08:01:09.688959Z"
    }
   },
   "outputs": [],
   "source": [
    "def freeze(model): \n",
    "    \n",
    "    def freeze_with_mask(model, is_trainable_param):\n",
    "        trainable_mask = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            trainable = is_trainable_param(name, param)\n",
    "            param.requires_grad = trainable\n",
    "            trainable_mask[name] = trainable\n",
    "            # Optional debug:\n",
    "            # print(f\"{name}: {'TRAIN' if trainable else 'FREEZE'}\")\n",
    "        return trainable_mask\n",
    "    \n",
    "    \n",
    "    # your predicate:\n",
    "    def is_trainable_param(name, param):\n",
    "        \n",
    "        if name.startswith(\"vision_model.head\") :\n",
    "            return True\n",
    "\n",
    "        if \"layers.17\" in name:\n",
    "            return True \n",
    "            \n",
    "        if \"model.norm.weight\"  in name:\n",
    "            return True\n",
    "            \n",
    "        if name.startswith(\"proj\"):  \n",
    "            return True\n",
    "            \n",
    "        # freeze everything else\n",
    "        return False\n",
    "\n",
    "    return freeze_with_mask(model, is_trainable_param)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:01:10.862247Z",
     "iopub.status.busy": "2025-05-06T08:01:10.861510Z",
     "iopub.status.idle": "2025-05-06T08:01:10.867005Z",
     "shell.execute_reply": "2025-05-06T08:01:10.866340Z",
     "shell.execute_reply.started": "2025-05-06T08:01:10.862216Z"
    }
   },
   "outputs": [],
   "source": [
    "m1  =  freeze(siglip) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Number of Parameters: </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:01:13.889923Z",
     "iopub.status.busy": "2025-05-06T08:01:13.889360Z",
     "iopub.status.idle": "2025-05-06T08:01:13.894082Z",
     "shell.execute_reply": "2025-05-06T08:01:13.893293Z",
     "shell.execute_reply.started": "2025-05-06T08:01:13.889899Z"
    }
   },
   "outputs": [],
   "source": [
    "def num_params(model):\n",
    "    \n",
    "    # Total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:01:14.217095Z",
     "iopub.status.busy": "2025-05-06T08:01:14.216844Z",
     "iopub.status.idle": "2025-05-06T08:01:14.223043Z",
     "shell.execute_reply": "2025-05-06T08:01:14.222365Z",
     "shell.execute_reply.started": "2025-05-06T08:01:14.217075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 375,187,970\n",
      "Trainable params: 7,087,104\n"
     ]
    }
   ],
   "source": [
    "num_params(siglip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:02:09.917579Z",
     "iopub.status.busy": "2025-05-06T08:02:09.917301Z",
     "iopub.status.idle": "2025-05-06T08:02:09.922512Z",
     "shell.execute_reply": "2025-05-06T08:02:09.921548Z",
     "shell.execute_reply.started": "2025-05-06T08:02:09.917559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head.probe\n",
      "head.attention.in_proj_weight\n",
      "head.attention.in_proj_bias\n",
      "head.attention.out_proj.weight\n",
      "head.attention.out_proj.bias\n",
      "head.layernorm.weight\n",
      "head.layernorm.bias\n",
      "head.mlp.fc1.weight\n",
      "head.mlp.fc1.bias\n",
      "head.mlp.fc2.weight\n",
      "head.mlp.fc2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in siglip.vision_model.named_parameters():\n",
    "\n",
    "    if param.requires_grad:\n",
    "        print(name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Siglip outputs : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:50:18.030911Z",
     "iopub.status.busy": "2025-05-06T07:50:18.030262Z",
     "iopub.status.idle": "2025-05-06T07:50:18.034588Z",
     "shell.execute_reply": "2025-05-06T07:50:18.033707Z",
     "shell.execute_reply.started": "2025-05-06T07:50:18.030884Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "\n",
    "    dev_name  =  \"cuda\"\n",
    "\n",
    "else: dev_name = \"cpu\"\n",
    "\n",
    "\n",
    "device  =  torch.device(dev_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:50:18.353679Z",
     "iopub.status.busy": "2025-05-06T07:50:18.353228Z",
     "iopub.status.idle": "2025-05-06T07:50:18.357380Z",
     "shell.execute_reply": "2025-05-06T07:50:18.356816Z",
     "shell.execute_reply.started": "2025-05-06T07:50:18.353658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_utils import load_image\n",
    "\n",
    "image    =   load_image(\"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\")\n",
    "inputs   =   processor(images=[image], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run infernece\n",
    "with torch.no_grad():\n",
    "    \n",
    "    outputs = siglip.vision_model(pixel_values=inputs[\"pixel_values\"])\n",
    "\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_state.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> GEMMA : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "f4159e4955cb4d1abfd96903b188f667",
      "b6a36a10d9524f599530e9df02879b23",
      "bb9f07ff632b4a778988cabc89bf4033",
      "f92ff29f69bd456583178a9d557fe531",
      "e9ec78ea19cd42e58bc2b80df25f7af2",
      "f182e67c8e89403198aae302fec98e2f",
      "4fe0c3e51a5648659ad9b2a5b165d4f1",
      "4f7478c6db2246328a0cf3a028e99f86",
      "45e338651fdb4b5e8f37863ca3b8b28a",
      "e6e980713e1c490ba92d6d91dd39f5fa",
      "7a69f9647af146c7bfbd8da45ec177d4"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-05-06T07:58:09.342673Z",
     "iopub.status.busy": "2025-05-06T07:58:09.342059Z",
     "iopub.status.idle": "2025-05-06T07:58:12.976429Z",
     "shell.execute_reply": "2025-05-06T07:58:12.975817Z",
     "shell.execute_reply.started": "2025-05-06T07:58:09.342648Z"
    },
    "id": "MJKsbiCM-GkN",
    "outputId": "11c525e3-1bcf-4857-8ca3-64ec0281809e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf814aa83c142abaad6ed3ab4b5c2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\n",
    "                                            \"google/gemma-2b\",\n",
    "                                            #quantization_config           = bnb_config,\n",
    "                                            torch_dtype                   = torch.float16,\n",
    "                                            device_map                    = \"cpu\", \n",
    "                                        ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-06T07:58:12.977663Z",
     "iopub.status.busy": "2025-05-06T07:58:12.977339Z",
     "iopub.status.idle": "2025-05-06T07:58:12.982843Z",
     "shell.execute_reply": "2025-05-06T07:58:12.982056Z",
     "shell.execute_reply.started": "2025-05-06T07:58:12.977645Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in gemma.named_parameters():\n",
    "\n",
    "    print(name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:58:28.325544Z",
     "iopub.status.busy": "2025-05-06T07:58:28.325265Z",
     "iopub.status.idle": "2025-05-06T07:58:28.329982Z",
     "shell.execute_reply": "2025-05-06T07:58:28.329266Z",
     "shell.execute_reply.started": "2025-05-06T07:58:28.325524Z"
    }
   },
   "outputs": [],
   "source": [
    "m2   =   freeze(gemma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:58:28.560015Z",
     "iopub.status.busy": "2025-05-06T07:58:28.559795Z",
     "iopub.status.idle": "2025-05-06T07:58:28.565020Z",
     "shell.execute_reply": "2025-05-06T07:58:28.564410Z",
     "shell.execute_reply.started": "2025-05-06T07:58:28.559999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 2,506,172,416\n",
      "Trainable params: 110,106,624\n"
     ]
    }
   ],
   "source": [
    "num_params(gemma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T07:50:33.227656Z",
     "iopub.status.busy": "2025-05-06T07:50:33.226986Z",
     "iopub.status.idle": "2025-05-06T07:50:33.401665Z",
     "shell.execute_reply": "2025-05-06T07:50:33.400827Z",
     "shell.execute_reply.started": "2025-05-06T07:50:33.227635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\") \n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.load(TOKENIZER_PATH)  # TOKENIZER_PATH should be a .model file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> GEMMA's outputs : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text   = \" en : Define the image?\"\n",
    "\n",
    "#ids    = tokenizer.encode(text)                # Returns a list of token ids\n",
    "#tokens = tokenizer.encode(text, out_type=str)  # Returns list of tokens \n",
    "\n",
    "\n",
    "encoded = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Access input IDs and attention masks\n",
    "input_ids      = encoded[\"input_ids\"]\n",
    "attention_mask = encoded[\"attention_mask\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape, attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_embeds = gemma.model.embed_tokens(input_ids)\n",
    "\n",
    "print(text_embeds.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = gemma(\n",
    "    inputs_embeds   =  text_embeds,\n",
    "    attention_mask  =  attention_mask,\n",
    ")\n",
    "\n",
    "# You can generate by continuing this input (if needed)\n",
    "logits = outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids   = gemma.generate(\n",
    "                                                inputs_embeds   =  text_embeds,\n",
    "                                                attention_mask  =  attention_mask,\n",
    "                                                max_new_tokens  =  64, \n",
    "                                                num_beams       =  1,\n",
    "                                                do_sample       =  False,\n",
    "                                                eos_token_id    =  tokenizer.eos_token_id  \n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Projection after Siglip : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = nn.Linear(768, 2048).half().to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_image_embeds  =  proj(last_hidden_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_image_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Concate Proj + Prompt : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNTagnIEeTd6"
   },
   "outputs": [],
   "source": [
    "combined_embeds = torch.cat([projected_image_embeds, text_embeds], dim=1)  # [1, 197 + seq_len, 2048]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Then, Put them into the GEMMA : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matching attention mask\n",
    "image_attention_mask    = torch.ones(projected_image_embeds.shape[:-1], dtype=torch.long).to(device)\n",
    "combined_attention_mask = torch.cat([image_attention_mask, tokens[\"attention_mask\"]], dim=1)\n",
    "\n",
    "# Forward pass (important: use inputs_embeds instead of input_ids)\n",
    "outputs = gemma(\n",
    "    inputs_embeds=combined_embeds,\n",
    "    attention_mask=combined_attention_mask,\n",
    ")\n",
    "\n",
    "# You can generate by continuing this input (if needed)\n",
    "logits = outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Combine all parts into one : </h3>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, siglip, gemma, tokenizer, proj_dim=2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.siglip      =   siglip.vision_model   # expects pixel_values [B,3,H,W]\n",
    "        self.proj        =   nn.Linear(768, proj_dim, bias=False).half()\n",
    "        self.gemma       =   gemma\n",
    "        self.tokenizer   =   tokenizer\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        images:         torch.FloatTensor of shape [B, 3, H, W]\n",
    "        input_ids:      torch.LongTensor  of shape [B, seq_len]\n",
    "        attention_mask: torch.LongTensor  of shape [B, seq_len]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Image → SigLIP patch embeddings\n",
    "        vision_out = self.siglip(pixel_values=images)         # [B, 197, 768]\n",
    "        img_tokens = vision_out.last_hidden_state.half()      # ensure same dtype as proj\n",
    "\n",
    "        # 2) Project to Gemma's hidden size\n",
    "        proj_img = self.proj(img_tokens)                      # [B, 197, 2048]\n",
    "\n",
    "        # 3) Embed text tokens\n",
    "        txt_embeds = self.gemma.get_input_embeddings()(input_ids)  # [B, seq_len, 2048]\n",
    "\n",
    "        # 4) Concatenate and build attention mask\n",
    "        combined        =    torch.cat([proj_img, txt_embeds], dim=1)       # [B, 197+seq_len, 2048]\n",
    "        bsz, n_img, _   =    proj_img.size()\n",
    "        img_mask        =    torch.ones(bsz, n_img, device=images.device, dtype=torch.long)\n",
    "        attn_mask       =    torch.cat([img_mask, attention_mask], dim=1)  # [B, 197+seq_len] \n",
    "\n",
    "        generated_ids   =    self.gemma.generate(\n",
    "                                                inputs_embeds   =  combined,\n",
    "                                                attention_mask  =  attn_mask,\n",
    "                                                max_new_tokens  =  64, \n",
    "                                                num_beams       =  1,\n",
    "                                                do_sample       =  False,\n",
    "                                                eos_token_id    =  self.tokenizer.eos_token_id  \n",
    "                                            )\n",
    "        \n",
    "        captions = [self.tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "        \n",
    "        return  captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model   =  CombinedModel(siglip, gemma, tokenizer, proj_dim=2048).to(device)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Sanity Check on only text : </h3>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text   = \"Write a short story about giants.\"\n",
    "\n",
    "ids    = tokenizer.encode(text)  \n",
    "\n",
    "input_ids = torch.tensor(ids).unsqueeze(0)  # [1, seq_len]\n",
    "\n",
    "# Manually create attention mask (1 for real tokens, 0 for padding)\n",
    "attention_mask = torch.ones_like(input_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids          =   input_ids.to(\"cpu\")\n",
    "attention_mask     =   attention_mask.to(\"cpu\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_embeds = gemma.get_input_embeddings()(input_ids)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.piece_to_id('<eos>') \n",
    "pad_token_id = tokenizer.piece_to_id('<pad>')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids   = gemma.generate(\n",
    "                                                inputs_embeds   =  txt_embeds,\n",
    "                                                attention_mask  =  attention_mask,\n",
    "                                                max_new_tokens  =  64, \n",
    "                                                num_beams       =  1,\n",
    "                                                do_sample       =  False,\n",
    "                                                eos_token_id    =  eos_token_id  \n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Sanity Check for visual-tokens from SigLip + text-tokens into gemma: </h3>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text   = \"What does the bear in the image do?\"\n",
    "\n",
    "tokens = tokenizer(text, return_tensors=\"pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(inputs[\"pixel_values\"].cpu().view(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[\"input_ids\"]       =   tokens[\"input_ids\"].to(device)\n",
    "tokens[\"attention_mask\"]  =   tokens[\"attention_mask\"].to(device)   \n",
    "inputs[\"pixel_values\"]    =   inputs[\"pixel_values\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out  =  model(inputs[\"pixel_values\"], tokens[\"input_ids\"], tokens[\"attention_mask\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T05:19:17.768413Z",
     "iopub.status.busy": "2025-05-05T05:19:17.767749Z",
     "iopub.status.idle": "2025-05-05T05:19:17.774514Z",
     "shell.execute_reply": "2025-05-05T05:19:17.773603Z",
     "shell.execute_reply.started": "2025-05-05T05:19:17.768382Z"
    }
   },
   "source": [
    "### It is understandable getting gibberish . \n",
    "### Because we concatenated some visual tokens which are continuous and do not correspond to any real word. \n",
    "### Therefore, gemma can not interpret them. So, we need to train the whole model. Because gemma can not interpret continuous vectors.\n",
    "### Even LORA adapters did not work becasue of out of memory errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Model for training: </h3>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:02:46.855110Z",
     "iopub.status.busy": "2025-05-06T08:02:46.854821Z",
     "iopub.status.idle": "2025-05-06T08:02:46.863636Z",
     "shell.execute_reply": "2025-05-06T08:02:46.862882Z",
     "shell.execute_reply.started": "2025-05-06T08:02:46.855090Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, siglip, gemma, tokenizer, proj_dim=2048):\n",
    "        super().__init__()\n",
    "        self.siglip    = siglip.vision_model\n",
    "        self.proj      = nn.Linear(768, proj_dim, bias=False).half()\n",
    "        self.gemma     = gemma\n",
    "        self.tokenizer = tokenizer \n",
    "\n",
    "    def _build_combined(self, proj_img, txt_embeds, attention_mask):\n",
    "        bsz, n_img, _ = proj_img.size()\n",
    "        img_mask      = torch.ones(bsz, n_img,\n",
    "                                   device=proj_img.device,\n",
    "                                   dtype=torch.long)\n",
    "        attn_mask     = torch.cat([img_mask, attention_mask], dim=1)\n",
    "        combined      = torch.cat([proj_img, txt_embeds], dim=1)\n",
    "        \n",
    "        return combined, attn_mask\n",
    "\n",
    "    def forward(self,\n",
    "                images: torch.FloatTensor,\n",
    "                input_ids: torch.LongTensor,\n",
    "                attention_mask: torch.LongTensor):\n",
    "        \n",
    "        # 1) Image → SigLIP patch embeddings\n",
    "        vision_out = self.siglip(pixel_values=images)\n",
    "\n",
    "        img_tokens = vision_out.last_hidden_state.half()          # [B,197,768]\n",
    "\n",
    "        # 2) Project to Gemma’s hidden size\n",
    "        proj_img   = self.proj(img_tokens)                       # [B,197,2048]\n",
    "\n",
    "        # 3) Embed text tokens\n",
    "        txt_embeds = self.gemma.get_input_embeddings()(input_ids) # [B,seq_len,2048]\n",
    "\n",
    "        # 4) Concat and build attention mask\n",
    "        combined, attn_mask = self._build_combined(proj_img, txt_embeds, attention_mask)\n",
    "\n",
    "        # 5) Forward through LM head WITHOUT labels\n",
    "        outputs = self.gemma(\n",
    "            inputs_embeds  = combined,\n",
    "            attention_mask = attn_mask,\n",
    "            return_dict    = True\n",
    "        )\n",
    "        logits = outputs.logits                                   # [B,197+seq_len, V]\n",
    "\n",
    "        # 6) Slice out only the text portion and compute CE loss\n",
    "        B, total_len, V = logits.size()\n",
    "        seq_len         = input_ids.size(1)\n",
    "        L_in            = seq_len - 1                             # predict next token\n",
    "        N               = total_len - L_in                        # offset = 197\n",
    "        text_logits     = logits[:, N : N + L_in, :].reshape(-1, V)\n",
    "        targets         = input_ids[:, 1:].reshape(-1)\n",
    "\n",
    "        pad_id          = 0 \n",
    "        loss            = F.cross_entropy(text_logits,\n",
    "                                         targets,\n",
    "                                         ignore_index=pad_id)\n",
    "\n",
    "\n",
    "\n",
    "        return loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = tokenizer.pad_id()\n",
    "print(\"PAD token ID:\", pad_id)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:02:58.942842Z",
     "iopub.status.busy": "2025-05-06T08:02:58.942530Z",
     "iopub.status.idle": "2025-05-06T08:02:59.015157Z",
     "shell.execute_reply": "2025-05-06T08:02:59.014635Z",
     "shell.execute_reply.started": "2025-05-06T08:02:58.942819Z"
    }
   },
   "outputs": [],
   "source": [
    "model   =   CombinedModel(siglip, gemma, tokenizer).to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:03:02.559287Z",
     "iopub.status.busy": "2025-05-06T08:03:02.558997Z",
     "iopub.status.idle": "2025-05-06T08:03:02.565080Z",
     "shell.execute_reply": "2025-05-06T08:03:02.564444Z",
     "shell.execute_reply.started": "2025-05-06T08:03:02.559264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siglip.head.probe\n",
      "siglip.head.attention.in_proj_weight\n",
      "siglip.head.attention.in_proj_bias\n",
      "siglip.head.attention.out_proj.weight\n",
      "siglip.head.attention.out_proj.bias\n",
      "siglip.head.layernorm.weight\n",
      "siglip.head.layernorm.bias\n",
      "siglip.head.mlp.fc1.weight\n",
      "siglip.head.mlp.fc1.bias\n",
      "siglip.head.mlp.fc2.weight\n",
      "siglip.head.mlp.fc2.bias\n",
      "proj.weight\n",
      "gemma.model.layers.17.self_attn.q_proj.weight\n",
      "gemma.model.layers.17.self_attn.k_proj.weight\n",
      "gemma.model.layers.17.self_attn.v_proj.weight\n",
      "gemma.model.layers.17.self_attn.o_proj.weight\n",
      "gemma.model.layers.17.mlp.gate_proj.weight\n",
      "gemma.model.layers.17.mlp.up_proj.weight\n",
      "gemma.model.layers.17.mlp.down_proj.weight\n",
      "gemma.model.layers.17.input_layernorm.weight\n",
      "gemma.model.layers.17.post_attention_layernorm.weight\n",
      "gemma.model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "\n",
    "    if param.requires_grad:\n",
    "        print(name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:03:56.759605Z",
     "iopub.status.busy": "2025-05-06T08:03:56.759370Z",
     "iopub.status.idle": "2025-05-06T08:03:56.765884Z",
     "shell.execute_reply": "2025-05-06T08:03:56.765237Z",
     "shell.execute_reply.started": "2025-05-06T08:03:56.759582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 2,600,629,504\n",
      "Trainable params: 118,766,592\n"
     ]
    }
   ],
   "source": [
    "num_params(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text    =   \"Write a short story about giants.\"\n",
    "\n",
    "encoded = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Access input IDs and attention masks\n",
    "input_ids      = encoded[\"input_ids\"].to(device)\n",
    "attention_mask = encoded[\"attention_mask\"].to(device) \n",
    "\n",
    "text_embeds = gemma.model.embed_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids   = gemma.generate(\n",
    "                                                inputs_embeds   =  text_embeds,\n",
    "                                                attention_mask  =  attention_mask,\n",
    "                                                max_new_tokens  =  64, \n",
    "                                                num_beams       =  1,\n",
    "                                                do_sample       =  False,\n",
    "                                                eos_token_id    =  tokenizer.eos_token_id  \n",
    "                                            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = model.parameters() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZdrd59PL_82"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Core Library Imports : </h3>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:06.977643Z",
     "iopub.status.busy": "2025-05-06T08:04:06.977047Z",
     "iopub.status.idle": "2025-05-06T08:04:07.072231Z",
     "shell.execute_reply": "2025-05-06T08:04:07.071667Z",
     "shell.execute_reply.started": "2025-05-06T08:04:06.977619Z"
    },
    "id": "zE7BtRvz-Ga3"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import functools\n",
    "import html\n",
    "import io\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "\n",
    "import tensorflow as tf\n",
    "import sentencepiece\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from PIL import Image\n",
    "\n",
    "# Import model definition from big_vision\n",
    "from big_vision.models.proj.paligemma import paligemma\n",
    "from big_vision.trainers.proj.paligemma import predict_fns\n",
    "\n",
    "# Import big vision utilities\n",
    "import big_vision.datasets.jsonl\n",
    "import big_vision.utils\n",
    "import big_vision.sharding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_2PUPia51SX",
    "outputId": "87163725-49dc-4124-ddd9-dacee11e15ae"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Reserve GPU/TPU for JAX </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:09.243125Z",
     "iopub.status.busy": "2025-05-06T08:04:09.241413Z",
     "iopub.status.idle": "2025-05-06T08:04:09.312015Z",
     "shell.execute_reply": "2025-05-06T08:04:09.311278Z",
     "shell.execute_reply.started": "2025-05-06T08:04:09.243096Z"
    },
    "id": "kV8GDGIP51Pn",
    "outputId": "03b414c0-b168-4463-bb47-9c859f94452b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version:  0.4.33\n",
      "JAX platform: gpu\n",
      "JAX devices:  1\n"
     ]
    }
   ],
   "source": [
    "# Don't let TF use the GPU or TPUs\n",
    "# Disables TensorFlow’s access to GPUs/TPUs so JAX can fully utilize them without resource contention.\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "tf.config.set_visible_devices([], \"TPU\")\n",
    "\n",
    "backend = jax.extend.backend.get_backend()\n",
    "print(f\"JAX version:  {jax.__version__}\")\n",
    "print(f\"JAX platform: {backend.platform}\")\n",
    "print(f\"JAX devices:  {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YQdjtXR5OB9",
    "outputId": "0bd31fa4-f61c-4832-b5d0-5d8b1bede748"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\">Construct model and load params into RAM </h3>\n",
    "\n",
    " <h5 style=\"color:red;\"> model_config: hyperparameters for both the vision encoder and text decoder.\n",
    "<br>\n",
    "                          Instantiate the combined Vision+LLM model.\n",
    "<br>\n",
    "                          Load pretrained weights into a parameter tree.\n",
    "<br>\n",
    "                          Build a decode function for efficient batched generation.\n",
    "                          </h5>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EOS token:\", tokenizer.eos_token)\n",
    "print(\"EOS token ID:\", tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:12.769501Z",
     "iopub.status.busy": "2025-05-06T08:04:12.769014Z",
     "iopub.status.idle": "2025-05-06T08:04:12.773403Z",
     "shell.execute_reply": "2025-05-06T08:04:12.772594Z",
     "shell.execute_reply.started": "2025-05-06T08:04:12.769474Z"
    },
    "id": "ZTUsR5eK-GIU"
   },
   "outputs": [],
   "source": [
    "# Define `decode` function to sample outputs from the model.\n",
    "decode_fn  =   predict_fns.get_all(model)['decode']\n",
    "decode     =   functools.partial(decode_fn, devices=jax.devices(), eos_token=tokenizer.eos_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI7uthrl7YnL",
    "outputId": "9488b99a-2eed-40f3-f9b3-cd474b5ca0da"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Sharding & Casting Parameters :  </h3>\n",
    "\n",
    "  <h5 style=\"color:red;\">  Sharding: split tensors across devices (if you had >1 GPU).\n",
    "<br>\n",
    "                        maybe_cast_to_f32: keep the frozen weights in fp16 to save memory; cast the few trainable ones to fp32 so their gradients remain stable.\n",
    "<br>\n",
    "                        The loop unpacks the parameter tree, reshares & casts each leaf, and reassembles it.\n",
    "</h5>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:13.844064Z",
     "iopub.status.busy": "2025-05-06T08:04:13.843741Z",
     "iopub.status.idle": "2025-05-06T08:04:13.847936Z",
     "shell.execute_reply": "2025-05-06T08:04:13.847210Z",
     "shell.execute_reply.started": "2025-05-06T08:04:13.844045Z"
    },
    "id": "RZK4rvokI-fc"
   },
   "outputs": [],
   "source": [
    "# If more than one device is available (e.g. multiple GPUs) the parameters can\n",
    "# be sharded across them to reduce HBM usage per device.\n",
    "mesh = jax.sharding.Mesh(jax.devices(), (\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:15.752089Z",
     "iopub.status.busy": "2025-05-06T08:04:15.751801Z",
     "iopub.status.idle": "2025-05-06T08:04:15.756618Z",
     "shell.execute_reply": "2025-05-06T08:04:15.755891Z",
     "shell.execute_reply.started": "2025-05-06T08:04:15.752071Z"
    },
    "id": "SMp3esfH-F6X"
   },
   "outputs": [],
   "source": [
    "data_sharding = jax.sharding.NamedSharding(\n",
    "    mesh, jax.sharding.PartitionSpec(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sharding = big_vision.sharding.infer_sharding(\n",
    "    params, strategy=[('.*', 'fsdp(axis=\"data\")')], mesh=mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes: Some donated buffers are not usable.\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"Some donated buffers were not usable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, donate_argnums=(0,), static_argnums=(1,))\n",
    "def maybe_cast_to_f32(params, trainable):\n",
    "  # Cast others to float16, since some GPUs don't support bf16.\n",
    "  return jax.tree.map(lambda p, m: p.astype(jnp.float32)\n",
    "                      if m else p.astype(jnp.float16),\n",
    "                      params, trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy_safe(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_flat, treedef       = jax.tree.flatten(params)\n",
    "sharding_leaves            = jax.tree.leaves(params_sharding)\n",
    "trainable_leaves           = jax.tree.leaves(trainable_mask)\n",
    "\n",
    "new_params_flat = []\n",
    "\n",
    "for idx, (param, sharding, trainable) in enumerate(zip(params_flat, sharding_leaves, trainable_leaves)):\n",
    "    param = to_numpy_safe(param)\n",
    "    param = big_vision.utils.reshard(param, sharding)\n",
    "    param = maybe_cast_to_f32(param, trainable)\n",
    "    param.block_until_ready()\n",
    "    new_params_flat.append(param)\n",
    "\n",
    "params = jax.tree.unflatten(treedef, new_params_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print params to show what the model is made of.\n",
    "def parameter_overview(params):\n",
    "  for path, arr in big_vision.utils.tree_flatten_with_names(params)[0]:\n",
    "    print(f\"{path:80s} {str(arr.shape):22s} {arr.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" == Model params == \")\n",
    "parameter_overview(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T21:05:03.618581Z",
     "iopub.status.busy": "2025-05-01T21:05:03.618308Z",
     "iopub.status.idle": "2025-05-01T21:05:03.623664Z",
     "shell.execute_reply": "2025-05-01T21:05:03.623032Z",
     "shell.execute_reply.started": "2025-05-01T21:05:03.618560Z"
    },
    "id": "WH_1HRpMqhuW",
    "outputId": "a6037f1d-e8cf-436c-d18a-47404ff945eb"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Define preprocess functions to create inputs to the model :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:19.734625Z",
     "iopub.status.busy": "2025-05-06T08:04:19.734368Z",
     "iopub.status.idle": "2025-05-06T08:04:19.742378Z",
     "shell.execute_reply": "2025-05-06T08:04:19.741624Z",
     "shell.execute_reply.started": "2025-05-06T08:04:19.734608Z"
    },
    "id": "0zrFyYeVEAHD"
   },
   "outputs": [],
   "source": [
    "# @title Define preprocess functions to create inputs to the model.\n",
    "\n",
    "def preprocess_image(image, size=224):\n",
    "  # Model has been trained to handle images of different aspects ratios\n",
    "  # resized to 224x224 in the range [-1, 1]. Bilinear and antialias resize\n",
    "  # options are helpful to improve quality in some tasks.\n",
    "  image = np.asarray(image)\n",
    "  if image.ndim == 2:  # Convert image without last channel into greyscale.\n",
    "    image = np.stack((image,)*3, axis=-1)\n",
    "  image = image[..., :3]  # Remove alpha layer.\n",
    "  assert image.shape[-1] == 3\n",
    "\n",
    "  image = tf.constant(image)\n",
    "  image = tf.image.resize(image, (size, size), method='bilinear', antialias=True)\n",
    "  return image.numpy() / 127.5 - 1.0  # [0, 255]->[-1,1]\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "def preprocess_tokens(prefix, suffix=None, seqlen=None):\n",
    "  # Model has been trained to handle tokenized text composed of a prefix with\n",
    "  # full attention and a suffix with causal attention.\n",
    "  separator = \"\\n\"\n",
    "  tokens = tokenizer.encode(prefix, add_bos=True) + tokenizer.encode(separator)\n",
    "  mask_ar = [0] * len(tokens)    # 0 to use full attention for prefix.\n",
    "  mask_loss = [0] * len(tokens)  # 0 to not use prefix tokens in the loss.\n",
    "\n",
    "  if suffix:\n",
    "    suffix = tokenizer.encode(suffix, add_eos=True)\n",
    "    tokens += suffix\n",
    "    mask_ar += [1] * len(suffix)    # 1 to use causal attention for suffix.\n",
    "    mask_loss += [1] * len(suffix)  # 1 to use suffix tokens in the loss.\n",
    "\n",
    "  mask_input = [1] * len(tokens)    # 1 if its a token, 0 if padding.\n",
    "  if seqlen:\n",
    "    padding = [0] * max(0, seqlen - len(tokens))\n",
    "    tokens = tokens[:seqlen] + padding\n",
    "    mask_ar = mask_ar[:seqlen] + padding\n",
    "    mask_loss = mask_loss[:seqlen] + padding\n",
    "    mask_input = mask_input[:seqlen] + padding\n",
    "\n",
    "  return jax.tree.map(np.array, (tokens, mask_ar, mask_loss, mask_input))\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "def postprocess_tokens(tokens):\n",
    "  tokens = tokens.tolist()  # np.array to list[int]\n",
    "  try:  # Remove tokens at and after EOS if any.\n",
    "    eos_pos = tokens.index(tokenizer.eos_id())\n",
    "    tokens = tokens[:eos_pos]\n",
    "  except ValueError:\n",
    "    pass\n",
    "  return tokenizer.decode(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SoyORZgquEe",
    "outputId": "7f3a2fa9-144c-4deb-e7b0-9466ffe8d1bf"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Import Data  :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:23.332975Z",
     "iopub.status.busy": "2025-05-06T08:04:23.332319Z",
     "iopub.status.idle": "2025-05-06T08:04:23.336462Z",
     "shell.execute_reply": "2025-05-06T08:04:23.335588Z",
     "shell.execute_reply.started": "2025-05-06T08:04:23.332950Z"
    },
    "id": "_HgszSHUGy0A"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:23.586347Z",
     "iopub.status.busy": "2025-05-06T08:04:23.585590Z",
     "iopub.status.idle": "2025-05-06T08:04:23.808689Z",
     "shell.execute_reply": "2025-05-06T08:04:23.808047Z",
     "shell.execute_reply.started": "2025-05-06T08:04:23.586317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source split           image  \\\n",
      "0   NWPU  test  NWPU_31430.jpg   \n",
      "1   NWPU  test  NWPU_31431.jpg   \n",
      "2   NWPU  test  NWPU_31432.jpg   \n",
      "3   NWPU  test  NWPU_31433.jpg   \n",
      "4   NWPU  test  NWPU_31434.jpg   \n",
      "\n",
      "                                           caption_1  \\\n",
      "0   A gray plane on the runway and the lawn beside .   \n",
      "1  Three small planes parked in a line on the air...   \n",
      "2  A plane parked in a line on the airport with s...   \n",
      "3  A small plane and a big plane parked next to b...   \n",
      "4       Two planes parked next to boarding bridges .   \n",
      "\n",
      "                                           caption_2  \\\n",
      "0        A grey plane is on the runway by the lawn .   \n",
      "1  There are four aircraft on the open ground, Th...   \n",
      "2  A white plane was parked on the instruction li...   \n",
      "3  A white plane and a gray plane parked at the b...   \n",
      "4  Two aircraft were parked at the departure gates .   \n",
      "\n",
      "                                           caption_3  \\\n",
      "0  There is an airplane on the runway with a larg...   \n",
      "1  There are many planes of different sizes in a ...   \n",
      "2  An airplane parked in an open area with many c...   \n",
      "3  Two planes of different sizes are neatly parke...   \n",
      "4  Two planes of different sizes are neatly parke...   \n",
      "\n",
      "                                           caption_4  \\\n",
      "0  A plane is parked on the runway next to the gr...   \n",
      "1             Four planes are parked on the runway .   \n",
      "2              A plane is parked on the open space .   \n",
      "3  A large plane and a small plane are parked nea...   \n",
      "4       Two planes are parked next to the terminal .   \n",
      "\n",
      "                                           caption_5  \n",
      "0  There is a plane on the runway beside the grass .  \n",
      "1  Four planes of different sizes were on the mar...  \n",
      "2            There is 1 plane on the ground marked .  \n",
      "3              Two planes are on the marked ground .  \n",
      "4              Two planes are on the marked ground .  \n"
     ]
    }
   ],
   "source": [
    "captions_df = pd.read_csv('/kaggle/input/rsics-dataset/captions.csv')\n",
    "print(captions_df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:23.810137Z",
     "iopub.status.busy": "2025-05-06T08:04:23.809864Z",
     "iopub.status.idle": "2025-05-06T08:04:23.813969Z",
     "shell.execute_reply": "2025-05-06T08:04:23.813160Z",
     "shell.execute_reply.started": "2025-05-06T08:04:23.810112Z"
    },
    "id": "vy0cc5BzI-fe"
   },
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_ROOT = \"/kaggle/input/rsics-dataset/resized\" \n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE   = (224, 224)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:24.026377Z",
     "iopub.status.busy": "2025-05-06T08:04:24.026168Z",
     "iopub.status.idle": "2025-05-06T08:04:24.036398Z",
     "shell.execute_reply": "2025-05-06T08:04:24.035818Z",
     "shell.execute_reply.started": "2025-05-06T08:04:24.026362Z"
    },
    "id": "-F5zSNzPKHOu",
    "outputId": "07b979e3-5d53-4451-ff4d-611f111de16a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All split labels: ['test' 'val' 'train']\n",
      "Counts:\n",
      " split\n",
      "train    35614\n",
      "test      4454\n",
      "val       4453\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"All split labels:\", captions_df['split'].unique())\n",
    "print(\"Counts:\\n\", captions_df['split'].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:24.300594Z",
     "iopub.status.busy": "2025-05-06T08:04:24.300351Z",
     "iopub.status.idle": "2025-05-06T08:04:24.319454Z",
     "shell.execute_reply": "2025-05-06T08:04:24.318977Z",
     "shell.execute_reply.started": "2025-05-06T08:04:24.300574Z"
    },
    "id": "bYrO8v1UKHL4"
   },
   "outputs": [],
   "source": [
    "# 4. Filter into splits\n",
    "splits = {}\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    splits[split_name] = captions_df[captions_df['split'] == split_name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:26.987246Z",
     "iopub.status.busy": "2025-05-06T08:04:26.986680Z",
     "iopub.status.idle": "2025-05-06T08:04:26.991494Z",
     "shell.execute_reply": "2025-05-06T08:04:26.990788Z",
     "shell.execute_reply.started": "2025-05-06T08:04:26.987221Z"
    },
    "id": "9boO9ocTKGiO"
   },
   "outputs": [],
   "source": [
    "# 5. Convert each split-DataFrame into (paths, captions)\n",
    "def df_to_paths_and_captions(split_df):\n",
    "    # Full paths\n",
    "    paths = split_df['image'].apply(lambda fn: os.path.join(IMAGE_ROOT, fn)).tolist()\n",
    "    # List-of-captions per example\n",
    "    captions_cols = [f'caption_{i}' for i in range(1,6)]\n",
    "    captions = split_df[captions_cols].values.tolist()\n",
    "    return paths, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:27.233111Z",
     "iopub.status.busy": "2025-05-06T08:04:27.232911Z",
     "iopub.status.idle": "2025-05-06T08:04:27.298920Z",
     "shell.execute_reply": "2025-05-06T08:04:27.298158Z",
     "shell.execute_reply.started": "2025-05-06T08:04:27.233097Z"
    },
    "id": "UTtccQ1oIP7S"
   },
   "outputs": [],
   "source": [
    "train_paths, train_caps = df_to_paths_and_captions(splits['train'])\n",
    "val_paths,   val_caps   = df_to_paths_and_captions(splits['val'])\n",
    "test_paths,  test_caps  = df_to_paths_and_captions(splits['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:27.573047Z",
     "iopub.status.busy": "2025-05-06T08:04:27.572538Z",
     "iopub.status.idle": "2025-05-06T08:04:27.578583Z",
     "shell.execute_reply": "2025-05-06T08:04:27.577693Z",
     "shell.execute_reply.started": "2025-05-06T08:04:27.573023Z"
    },
    "id": "Wgpm8HTEISvx"
   },
   "outputs": [],
   "source": [
    "# 6. Preprocessing fn: load image + return captions list\n",
    "def _load_and_preprocess(path, captions):\n",
    "    # Read & decode\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    # Resize & normalize\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img, captions\n",
    "\n",
    "# 7. Build the tf.data pipeline\n",
    "def make_dataset(paths, captions, shuffle=False):\n",
    "    # turn your Python list-of-strings into a tf.string tensor\n",
    "    paths_ds = tf.data.Dataset.from_tensor_slices(tf.constant(paths, dtype=tf.string))\n",
    "    # turn your list-of-lists-of-strings into a [5] tf.string tensor\n",
    "    caps_ds  = tf.data.Dataset.from_tensor_slices(tf.constant(captions, dtype=tf.string))\n",
    "\n",
    "    ds = tf.data.Dataset.zip((paths_ds, caps_ds))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(paths))\n",
    "    ds = ( ds\n",
    "           .map(_load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "           .batch(BATCH_SIZE)\n",
    "           .prefetch(tf.data.AUTOTUNE)\n",
    "         )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:30.219065Z",
     "iopub.status.busy": "2025-05-06T08:04:30.218773Z",
     "iopub.status.idle": "2025-05-06T08:04:30.386365Z",
     "shell.execute_reply": "2025-05-06T08:04:30.385807Z",
     "shell.execute_reply.started": "2025-05-06T08:04:30.219044Z"
    },
    "id": "oWuvduvbIE-H"
   },
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_paths, train_caps, shuffle=True)\n",
    "val_ds   = make_dataset(val_paths,   val_caps,   shuffle=True)\n",
    "test_ds  = make_dataset(test_paths,  test_caps,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:30.459864Z",
     "iopub.status.busy": "2025-05-06T08:04:30.459353Z",
     "iopub.status.idle": "2025-05-06T08:04:30.515727Z",
     "shell.execute_reply": "2025-05-06T08:04:30.515094Z",
     "shell.execute_reply.started": "2025-05-06T08:04:30.459839Z"
    },
    "id": "mHiB8o5aIE7W",
    "outputId": "7a6f4417-8b73-48bb-cadd-8fb969006344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images batch shape: (32, 224, 224, 3)\n",
      "Captions batch shape: 32 examples\n",
      "First example captions: tf.Tensor(\n",
      "[b'A gray plane on the runway and the lawn beside .'\n",
      " b'A grey plane is on the runway by the lawn .'\n",
      " b'There is an airplane on the runway with a large lawn by the runway .'\n",
      " b'A plane is parked on the runway next to the grass .'\n",
      " b'There is a plane on the runway beside the grass .'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 8. Quick sanity check\n",
    "for imgs, caps in test_ds.take(1):\n",
    "    print(\"Images batch shape:\", imgs.shape)            # (BATCH_SIZE, H, W, 3)\n",
    "    print(\"Captions batch shape:\", len(caps), \"examples\")\n",
    "    print(\"First example captions:\", caps[0])            # a list of 5 strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:30.725309Z",
     "iopub.status.busy": "2025-05-06T08:04:30.725009Z",
     "iopub.status.idle": "2025-05-06T08:04:30.733587Z",
     "shell.execute_reply": "2025-05-06T08:04:30.732835Z",
     "shell.execute_reply.started": "2025-05-06T08:04:30.725288Z"
    },
    "id": "emSWd_NsEAEA"
   },
   "outputs": [],
   "source": [
    "def train_data_iterator():\n",
    "    \"\"\"Never-ending iterator over training examples from train_ds.\"\"\"\n",
    "    while True:\n",
    "        for image_batch, caps_batch in train_ds:\n",
    "            images_np = image_batch.numpy()      # [B,H,W,3]\n",
    "            caps_np   = caps_batch.numpy()       # [B,5] bytes\n",
    "\n",
    "            for img, caps in zip(images_np, caps_np):\n",
    "                # Decode all 5 captions into Python strings\n",
    "                decoded_caps = [c.decode('utf-8') for c in caps]\n",
    "\n",
    "                # Prepare the model input from a random caption\n",
    "                suffix = decoded_caps[np.random.randint(5)].lower()\n",
    "                prefix = \"caption en\"\n",
    "\n",
    "                tokens, mask_ar, mask_loss, _ = preprocess_tokens(\n",
    "                    prefix, suffix, SEQLEN\n",
    "                )\n",
    "\n",
    "                img_proc = img * 2.0 - 1.0\n",
    "                yield [{\n",
    "                    \"image\":     img_proc,\n",
    "                    \"text\":      np.asarray(tokens,    dtype=np.int32),\n",
    "                    \"mask_ar\":   np.asarray(mask_ar,    dtype=np.int32),\n",
    "                    \"mask_loss\": np.asarray(mask_loss,  dtype=np.int32),\n",
    "                },\n",
    "                    decoded_caps,\n",
    "                      ]\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------#\n",
    "#                                              Validation :\n",
    "#----------------------------------------------------------------------------------------------------------------#\n",
    "def validation_data_iterator():\n",
    "    \"\"\"Single-pass iterator over validation examples from val_ds.\"\"\"\n",
    "    for image_batch, caps_batch in val_ds:\n",
    "\n",
    "         images_np = image_batch.numpy()\n",
    "         caps_np   = caps_batch.numpy()\n",
    "\n",
    "         for img, caps in zip(images_np, caps_np):\n",
    "\n",
    "            decoded_caps =   [c.decode('utf-8') for c in caps]\n",
    "            suffix       =   decoded_caps[np.random.randint(5)].lower()\n",
    "            prefix       =   \"caption en\"\n",
    "\n",
    "            tokens, mask_ar, _, mask_input = preprocess_tokens(\n",
    "                 prefix,  seqlen=SEQLEN)\n",
    "\n",
    "            img_proc = img * 2. - 1.\n",
    "            yield [{\n",
    "                 \"image\":     img_proc,\n",
    "                 \"text\":      np.asarray(tokens,     dtype=np.int32),\n",
    "                 \"mask_input\": np.asarray(mask_input, dtype=np.int32),\n",
    "                 \"mask_ar\":    np.asarray(mask_ar,    dtype=np.int32),\n",
    "            },\n",
    "                 decoded_caps,\n",
    "                  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEauChmYI-fh"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Check Iterators :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:38.473124Z",
     "iopub.status.busy": "2025-05-06T08:04:38.472826Z",
     "iopub.status.idle": "2025-05-06T08:04:38.476794Z",
     "shell.execute_reply": "2025-05-06T08:04:38.475950Z",
     "shell.execute_reply.started": "2025-05-06T08:04:38.473101Z"
    },
    "id": "oZoIUr62LeWX"
   },
   "outputs": [],
   "source": [
    "SEQLEN   =   128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NY0f9D5I-fh"
   },
   "outputs": [],
   "source": [
    "val_it = validation_data_iterator()\n",
    "\n",
    "for _ in range(1):\n",
    "    ex   = next(val_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16H9-ugFI-fh",
    "outputId": "23155adb-4253-4fca-9001-2f4081b5d454"
   },
   "outputs": [],
   "source": [
    "ex[0][\"image\"].shape,   ex[0][\"text\"].shape,  ex[0][\"mask_input\"].shape,   ex[0][\"mask_ar\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQ90yvl0I-fi",
    "outputId": "cfd72a0d-78fc-4f55-a3b4-55815c8a0439"
   },
   "outputs": [],
   "source": [
    "ex[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python```\n",
    "    def forward(self,\n",
    "                image: torch.FloatTensor,        # [3, H, W]\n",
    "                input_ids: torch.LongTensor,     # [seq_len]\n",
    "                attention_mask: torch.LongTensor # [seq_len]\n",
    "               ):\n",
    "        # --- 1) Image → SigLIP patches ---\n",
    "        # add batch dim\n",
    "        pixels = image.unsqueeze(0)                     # [1,3,H,W]\n",
    "        vision_out = self.siglip(pixel_values=pixels)   # HF API: pixel_values\n",
    "        img_tokens = vision_out.last_hidden_state.half()# [1,197,768]\n",
    "\n",
    "        # --- 2) Project to Gemma’s hidden size ---\n",
    "        proj_img = self.proj(img_tokens)                # [1,197,2048]\n",
    "\n",
    "        # --- 3) Embed text tokens ---\n",
    "        # add batch dim\n",
    "        ids  = input_ids.unsqueeze(0)                   # [1, seq_len]\n",
    "        mask = attention_mask.unsqueeze(0)              # [1, seq_len]\n",
    "        txt_embeds = self.gemma.get_input_embeddings()(ids)  # [1,seq_len,2048]\n",
    "\n",
    "        # --- 4) Concat and build attention mask ---\n",
    "        combined = torch.cat([proj_img, txt_embeds], dim=1)  \n",
    "        img_mask = torch.ones(proj_img.size()[:2], dtype=torch.long, device=combined.device)\n",
    "        attn_mask = torch.cat([img_mask, mask], dim=1)  \n",
    "\n",
    "        # --- 5) Forward into Gemma as continuous embeddings ---\n",
    "        return self.gemma(inputs_embeds=combined, attention_mask=attn_mask)\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor(ex[0][\"image\"].reshape(1,3,224,224)).to(device), torch.tensor(ex[0][\"text\"].reshape(1,128)).long().to(device), torch.tensor(ex[0][\"mask_input\"].reshape(1,128)).long().to(device)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrE27d6rEABL"
   },
   "outputs": [],
   "source": [
    "# @title Inspect training examples.\n",
    "def render_inline(image, resize=(128, 128)):\n",
    "  \"\"\"Convert image into inline html.\"\"\"\n",
    "  image = Image.fromarray(image)\n",
    "  image.resize(resize)\n",
    "  with io.BytesIO() as buffer:\n",
    "    image.save(buffer, format='jpeg')\n",
    "    image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n",
    "    return f\"data:image/jpeg;base64,{image_b64}\"\n",
    "\n",
    "def render_example(image, caption):\n",
    "  image = ((image + 1)/2 * 255).astype(np.uint8)  # [-1,1] -> [0, 255]\n",
    "  return f\"\"\"\n",
    "    <div style=\"display: inline-flex; align-items: center; justify-content: center;\">\n",
    "        <img style=\"width:128px; height:128px;\" src=\"{render_inline(image, resize=(64,64))}\" />\n",
    "        <p style=\"width:256px; margin:10px; font-size:small;\">{html.escape(caption)}</p>\n",
    "    </div>\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:44.413432Z",
     "iopub.status.busy": "2025-05-06T08:04:44.412654Z",
     "iopub.status.idle": "2025-05-06T08:04:44.416582Z",
     "shell.execute_reply": "2025-05-06T08:04:44.415970Z",
     "shell.execute_reply.started": "2025-05-06T08:04:44.413404Z"
    },
    "id": "cPxzL2EJI-fi"
   },
   "outputs": [],
   "source": [
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "807hpxWVK2Ee",
    "outputId": "f1bd3809-288d-4914-b235-828703806224"
   },
   "outputs": [],
   "source": [
    "html_out = \"\"\n",
    "for idx, example in zip(range(4), train_data_iterator()):\n",
    "    img = example[0][\"image\"]\n",
    "\n",
    "    html_out += render_example(img, example[1][0])\n",
    "    # optionally a divider between images:\n",
    "    html_out += \"<hr style='width:100%;'>\"\n",
    "\n",
    "display(HTML(\"<h3>Training examples </h3>\" + html_out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4tWuAD5hrBs",
    "outputId": "0fd7ff49-11a1-4ccd-aedf-1ff9df65c91d"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Define the training step and evaluation loop :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:46.847014Z",
     "iopub.status.busy": "2025-05-06T08:04:46.846376Z",
     "iopub.status.idle": "2025-05-06T08:04:52.902052Z",
     "shell.execute_reply": "2025-05-06T08:04:52.901094Z",
     "shell.execute_reply.started": "2025-05-06T08:04:46.846986Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:37:59.333628Z",
     "iopub.status.busy": "2025-05-02T07:37:59.333126Z",
     "iopub.status.idle": "2025-05-02T07:37:59.339059Z",
     "shell.execute_reply": "2025-05-02T07:37:59.337947Z",
     "shell.execute_reply.started": "2025-05-02T07:37:59.333604Z"
    },
    "id": "PuqC8L--tMhq",
    "outputId": "bcb50f18-7b1b-4b57-fd84-2b3156af0f0f"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> WANDB :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:54.017944Z",
     "iopub.status.busy": "2025-05-06T08:04:54.017598Z",
     "iopub.status.idle": "2025-05-06T08:04:54.022233Z",
     "shell.execute_reply": "2025-05-06T08:04:54.021344Z",
     "shell.execute_reply.started": "2025-05-06T08:04:54.017915Z"
    },
    "id": "5aDEF3c_VE0U"
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:54.317589Z",
     "iopub.status.busy": "2025-05-06T08:04:54.317351Z",
     "iopub.status.idle": "2025-05-06T08:04:54.321161Z",
     "shell.execute_reply": "2025-05-06T08:04:54.320452Z",
     "shell.execute_reply.started": "2025-05-06T08:04:54.317570Z"
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:04:55.459395Z",
     "iopub.status.busy": "2025-05-06T08:04:55.458645Z",
     "iopub.status.idle": "2025-05-06T08:05:08.020234Z",
     "shell.execute_reply": "2025-05-06T08:05:08.019618Z",
     "shell.execute_reply.started": "2025-05-06T08:04:55.459368Z"
    },
    "id": "pJd2cBlEVBuQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mturgay-yildiz-phi-e-pi\u001b[0m (\u001b[33mDI_725___Final_Project\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250506_080501-o4my338z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DI_725___Final_Project/Phase_2___Siglip_plus_Proj_plus_Gemma/runs/o4my338z' target=\"_blank\">smart-fog-8</a></strong> to <a href='https://wandb.ai/DI_725___Final_Project/Phase_2___Siglip_plus_Proj_plus_Gemma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DI_725___Final_Project/Phase_2___Siglip_plus_Proj_plus_Gemma' target=\"_blank\">https://wandb.ai/DI_725___Final_Project/Phase_2___Siglip_plus_Proj_plus_Gemma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DI_725___Final_Project/Phase_2___Siglip_plus_Proj_plus_Gemma/runs/o4my338z' target=\"_blank\">https://wandb.ai/DI_725___Final_Project/Phase_2___Siglip_plus_Proj_plus_Gemma/runs/o4my338z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"...\"]       = \"...\"\n",
    "\n",
    "run  =  wandb.init(project=\"...\", entity=\"...\" ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:40:50.298843Z",
     "iopub.status.busy": "2025-05-02T07:40:50.298628Z",
     "iopub.status.idle": "2025-05-02T07:40:50.304331Z",
     "shell.execute_reply": "2025-05-02T07:40:50.303471Z",
     "shell.execute_reply.started": "2025-05-02T07:40:50.298819Z"
    },
    "id": "JaoT9rNIt2wO",
    "outputId": "6efe8793-ec84-40ac-d48e-8fca0ebd52cb"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Helper functions :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T08:05:08.021456Z",
     "iopub.status.busy": "2025-05-06T08:05:08.021249Z",
     "iopub.status.idle": "2025-05-06T08:05:08.027650Z",
     "shell.execute_reply": "2025-05-06T08:05:08.027029Z",
     "shell.execute_reply.started": "2025-05-06T08:05:08.021438Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_predictions(model, batch, num_examples=1, batch_size=1, max_length=64, **gen_kwargs):\n",
    "    \"\"\"\n",
    "    batch: list of (example, reference) tuples.\n",
    "    Returns a list of (image_id, predicted_string) pairs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    hyps = []\n",
    "    # we assume each example tuple has .text input ids/mask prepared\n",
    "    for (example, ref) in batch:\n",
    "        # pack inputs\n",
    "        input_ids       = torch.from_numpy(example[\"text\"]).unsqueeze(0).to(device)     # [1, seq]\n",
    "        attention_mask  = torch.from_numpy(example[\"mask_input\"]).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            # adjust generation kwargs to your model (e.g. num_beams, do_sample, etc.)\n",
    "            out = model.gemma.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=60, \n",
    "                **gen_kwargs\n",
    "            )\n",
    "        # decode (you may need your tokenizer.decode here)\n",
    "        hyp = tokenizer.decode(out[0].tolist())\n",
    "\n",
    "        hyps.append(hyp) \n",
    "    return hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "# DataLoader wrapper\n",
    "class TorchDataset(torch.utils.data.IterableDataset):\n",
    "    def __iter__(self):\n",
    "        for example, _ in train_data_iterator():\n",
    "            yield example\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "train_loader = DataLoader(TorchDataset(), batch_size=1)\n",
    "#------------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "opt = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to convert numpy or tensor to torch.Tensor\n",
    "def to_torch(x, dtype=None, device=None):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        t = x\n",
    "    else:\n",
    "        t = torch.from_numpy(x)\n",
    "    if dtype is not None:\n",
    "        t = t.to(dtype)\n",
    "    if device is not None:\n",
    "        t = t.to(device)\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "for batch in train_loader:\n",
    "    \n",
    "    if step >= 100:\n",
    "        break\n",
    "        \n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "   \n",
    "    # Move to device\n",
    "    imgs = to_torch(batch[\"image\"].view(1, 3, 224, 224), dtype=torch.float32, device=device)   # [B,3,H,W]\n",
    "    ids  = to_torch(batch[\"text\"],  dtype=torch.long,  device=device)     # [B, seq_len]\n",
    "    mask = to_torch(batch[\"mask_ar\"],dtype=torch.long,  device=device)    # [B, seq_len]\n",
    "\n",
    "    # Forward (drop last token for inputs)\n",
    "    outputs = model(imgs, ids[:, :-1], mask[:, :-1])\n",
    "    loss    = outputs\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "demo_samples = []\n",
    "for (example, caps), hyp in zip(validation_data_iterator(), all_hyps):\n",
    "    if len(demo_samples) >= N:\n",
    "        break\n",
    "    img = example[\"image\"]  # float array in [-1, 1]\n",
    "\n",
    "    demo_samples.append((img, hyp, caps)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "demo_samples = []\n",
    "for (example, caps), hyp in zip(validation_data_iterator(), all_hyps):\n",
    "    if len(demo_samples) >= N:\n",
    "        break\n",
    "    img = example[\"image\"]  # float array in [-1, 1]\n",
    "\n",
    "    demo_samples.append((img, hyp, caps)) \n",
    "\n",
    "\n",
    "# 2) Display inline HTML gallery\n",
    "\n",
    "imgs    =    [demo_samples[i][0]       for i in range(8)]\n",
    "preds   =    [demo_samples[i][1]       for i in range(8)]\n",
    "refs    =    [demo_samples[i][2]       for i in range(8)] \n",
    "\n",
    "\n",
    "html = [\"<div style='display:flex; gap:16px;'>\"]\n",
    "for img, pred, ref in zip(imgs, preds, refs):\n",
    "\n",
    "    if img.shape[0] == 3:  # Assuming CHW format\n",
    "        img = img.transpose(1, 2, 0)\n",
    "    \n",
    "    img_uint8 = ((img + 1) * 127.5).clip(0, 255).astype(\"uint8\")\n",
    "    buf = io.BytesIO()\n",
    "    Image.fromarray(img_uint8).save(buf, \"PNG\")\n",
    "    b64 = base64.b64encode(buf.getvalue()).decode()\n",
    "    html.append(f\"\"\"\n",
    "      <div style='text-align:center;'>\n",
    "        <img src=\"data:image/png;base64,{b64}\" width=200 /><br/>\n",
    "        <strong>Pred:</strong> {pred}<br/>\n",
    "        <strong>Ref:</strong> {ref}\n",
    "      </div>\"\"\")\n",
    "html.append(\"</div>\")\n",
    "display(HTML(\"\".join(html)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del imgs\n",
    "#del ids\n",
    "#del mask\n",
    "\n",
    "del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Train and Validation :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YamfwMqsD_hM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from IPython.display import HTML, display\n",
    "import io, base64\n",
    "import tqdm\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "# ──────────────── SETUP ──────────────── #\n",
    "\n",
    "# Metrics\n",
    "bleu_metric   = evaluate.load(\"bleu\")\n",
    "rouge_metric  = evaluate.load(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "# Hyperparams\n",
    "BATCH_SIZE     = 4\n",
    "TRAIN_EXAMPLES = 35614\n",
    "LEARNING_RATE  = 1e-4\n",
    "TRAIN_STEPS    = TRAIN_EXAMPLES // BATCH_SIZE\n",
    "EVAL_EVERY     = 32  \n",
    "num_epochs     = 10\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "run.config.update({\n",
    "    \"model_name\"     : \"Siglip + Projection + Gemma\",\n",
    "    \"batch_size\":     BATCH_SIZE,\n",
    "    \"train_examples\": TRAIN_EXAMPLES,\n",
    "    \"learning_rate\":  LEARNING_RATE,\n",
    "    \"train_steps\":    TRAIN_STEPS,\n",
    "})\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "opt = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "# DataLoader wrapper\n",
    "class TorchDataset(torch.utils.data.IterableDataset):\n",
    "    def __iter__(self):\n",
    "        for example, _ in train_data_iterator():\n",
    "            yield example\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "train_loader = DataLoader(TorchDataset(), batch_size=BATCH_SIZE)\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "# Helper to convert numpy or tensor to torch.Tensor\n",
    "def to_torch(x, dtype=None, device=None):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        t = x\n",
    "    else:\n",
    "        t = torch.from_numpy(x)\n",
    "    if dtype is not None:\n",
    "        t = t.to(dtype)\n",
    "    if device is not None:\n",
    "        t = t.to(device)\n",
    "    return t\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "# ──────────────── TRAIN + EVAL LOOP ──────────────── #\n",
    "step = 0\n",
    "\n",
    "for epoch in tqdm.tqdm(range(num_epochs)): \n",
    "    \n",
    "    for batch in train_loader: \n",
    "        \n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "       \n",
    "        # Move to device\n",
    "        imgs = to_torch(batch[\"image\"].view(BATCH_SIZE, 3, 224, 224), dtype=torch.float32, device=device)   # [B,3,H,W]\n",
    "        ids  = to_torch(batch[\"text\"],  dtype=torch.long,  device=device)     # [B, seq_len]\n",
    "        mask = to_torch(batch[\"mask_ar\"],dtype=torch.long,  device=device)    # [B, seq_len]\n",
    "    \n",
    "        # Forward (drop last token for inputs)\n",
    "        outputs = model(imgs, ids[:, :-1], mask[:, :-1])\n",
    "        loss    = outputs\n",
    "    #------------------------------------------------------------------------------------------------#\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        # Log training loss\n",
    "        step += 1\n",
    "    #------------------------------------------------------------------------------------------------#   \n",
    "        wandb.log({\"train/loss\": loss.item()}, step=step)\n",
    "    #------------------------------------------------------------------------------------------------#\n",
    "    #------------------------------------------------------------------------------------------------#\n",
    "    #------------------------------------------------------------------------------------------------#\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # ───── EVALUATION ───── #\n",
    "        if step == 1 or step % EVAL_EVERY == 0:\n",
    "            model.eval()\n",
    "            all_hyps, all_refs = [], []\n",
    "        \n",
    "            for (example, caps), _ in zip(validation_data_iterator(), range(8)):\n",
    "                # Ensure example is preprocessed with .text and .mask_input\n",
    "                with torch.no_grad():\n",
    "                    pred = make_predictions(\n",
    "                        model,\n",
    "                        [(example, caps)],   # this is already the correct format for `batch`\n",
    "                        num_examples=1,\n",
    "                        batch_size=1,\n",
    "                        max_length=128\n",
    "                    )[0]  # single prediction string\n",
    "        \n",
    "                all_hyps.append(pred)\n",
    "                all_refs.append([caps])  # wrap in list for BLEU/METEOR etc.\n",
    "    \n",
    "            # compute metrics\n",
    "            bleu_res   = bleu_metric.compute(predictions=all_hyps, references=all_refs, smooth=True)\n",
    "            rouge_res  = rouge_metric.compute(predictions=all_hyps, references=all_refs)\n",
    "            meteor_res = meteor_metric.compute(predictions=all_hyps, references=all_refs)\n",
    "    \n",
    "            print(f\"   BLEU:   {bleu_res['bleu']:.2f}\")\n",
    "            print(f\"   ROUGE1: {rouge_res['rouge1']:.2f}, ROUGE2: {rouge_res['rouge2']:.2f}, ROUGEL: {rouge_res['rougeL']:.2f}\")\n",
    "            print(f\"   METEOR: {meteor_res['meteor']:.2f}\")\n",
    "    \n",
    "            wandb.log({\n",
    "                \"eval/bleu\":    bleu_res[\"bleu\"],\n",
    "                \"eval/rouge1\":  rouge_res[\"rouge1\"],\n",
    "                \"eval/rouge2\":  rouge_res[\"rouge2\"],\n",
    "                \"eval/rougeL\":  rouge_res[\"rougeL\"],\n",
    "                \"eval/meteor\":  meteor_res[\"meteor\"],\n",
    "            }, step=step)\n",
    "    #---------------------------------------------------------------------------------------------------------------------------#\n",
    "    #                   ---- Demo‐sample pass via make_predictions ----\n",
    "    #---------------------------------------------------------------------------------------------------------------------------#\n",
    "            # (inside your eval block, after computing all_hyps & all_refs)\n",
    "            \n",
    "            # 1) Collect N demo samples\n",
    "            N = 8\n",
    "            demo_samples = []\n",
    "            for (example, caps), hyp in zip(validation_data_iterator(), all_hyps):\n",
    "                if len(demo_samples) >= N:\n",
    "                    break\n",
    "                img = example[\"image\"]  # float array in [-1, 1]\n",
    "            \n",
    "                demo_samples.append((img, hyp, caps)) \n",
    "            \n",
    "            \n",
    "            # 2) Display inline HTML gallery\n",
    "            \n",
    "            imgs    =    [demo_samples[i][0]       for i in range(8)]\n",
    "            preds   =    [demo_samples[i][1]       for i in range(8)]\n",
    "            refs    =    [demo_samples[i][2]       for i in range(8)] \n",
    "            \n",
    "            \n",
    "            html = [\"<div style='display:flex; gap:16px;'>\"]\n",
    "            for img, pred, ref in zip(imgs, preds, refs):\n",
    "            \n",
    "                if img.shape[0] == 3:  # Assuming CHW format\n",
    "                    img = img.transpose(1, 2, 0)\n",
    "                \n",
    "                img_uint8 = ((img + 1) * 127.5).clip(0, 255).astype(\"uint8\")\n",
    "                buf = io.BytesIO()\n",
    "                Image.fromarray(img_uint8).save(buf, \"PNG\")\n",
    "                b64 = base64.b64encode(buf.getvalue()).decode()\n",
    "                html.append(f\"\"\"\n",
    "                  <div style='text-align:center;'>\n",
    "                    <img src=\"data:image/png;base64,{b64}\" width=200 /><br/>\n",
    "                    <strong>Pred:</strong> {pred}<br/>\n",
    "                    <strong>Ref:</strong> {ref}\n",
    "                  </div>\"\"\")\n",
    "            html.append(\"</div>\")\n",
    "            display(HTML(\"\".join(html)))\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------------#        \n",
    "            # 3) Log to W&B\n",
    "            table = wandb.Table(columns=[\"image\",\"predicted\",\"reference\"])\n",
    "            for img, pred, ref in demo_samples:\n",
    "                table.add_data(wandb.Image(img), pred, ref)\n",
    "            wandb.log({\"eval/samples_table\": table}, step=step)\n",
    "    #------------------------------------------------------------------------------------------------#\n",
    "            torch.cuda.empty_cache()\n",
    "#------------------------------------------------------------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
