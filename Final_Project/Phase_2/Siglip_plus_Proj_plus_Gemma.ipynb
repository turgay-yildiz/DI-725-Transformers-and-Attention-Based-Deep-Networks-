{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:32.598431Z",
     "iopub.status.busy": "2025-05-04T14:20:32.598209Z",
     "iopub.status.idle": "2025-05-04T14:20:32.602192Z",
     "shell.execute_reply": "2025-05-04T14:20:32.601506Z",
     "shell.execute_reply.started": "2025-05-04T14:20:32.598414Z"
    },
    "id": "UHflqji43qh9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:47:50.075028Z",
     "iopub.status.busy": "2025-05-02T06:47:50.074537Z",
     "iopub.status.idle": "2025-05-02T06:47:50.080173Z",
     "shell.execute_reply": "2025-05-02T06:47:50.079280Z",
     "shell.execute_reply.started": "2025-05-02T06:47:50.075003Z"
    },
    "id": "UN1gBnmQ4LAj",
    "outputId": "a6ca43c0-0430-484f-bc05-665711eea867"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h1 style=\"color:red;\">DI-725 : Transformers and Attention-Based Deep Networks</h1>\n",
    "  <h2 style=\"color:red;\">Final Project : Phase - 2</h2>\n",
    "  <br><br>\n",
    "  <h4 style=\"color:red;\">Turgay Yıldız</h4>\n",
    "  <br>\n",
    "  <h4 style=\"color:red;\">Graduate School of Informatics, Middle East Technical University (METU)</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:47:11.957103Z",
     "iopub.status.busy": "2025-05-02T06:47:11.956430Z",
     "iopub.status.idle": "2025-05-02T06:47:11.961543Z",
     "shell.execute_reply": "2025-05-02T06:47:11.960914Z",
     "shell.execute_reply.started": "2025-05-02T06:47:11.957080Z"
    },
    "id": "lM_sMe2M4uhH",
    "outputId": "fb87cfd2-03e6-4c9e-b395-064dac894355"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\">Fetch big_vision code and install dependencies</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:39.376843Z",
     "iopub.status.busy": "2025-05-04T14:20:39.376120Z",
     "iopub.status.idle": "2025-05-04T14:20:41.225233Z",
     "shell.execute_reply": "2025-05-04T14:20:41.224468Z",
     "shell.execute_reply.started": "2025-05-04T14:20:39.376818Z"
    },
    "id": "0EJMa-2k-UuK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import kagglehub\n",
    "from google.colab import userdata\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:41.227000Z",
     "iopub.status.busy": "2025-05-04T14:20:41.226579Z",
     "iopub.status.idle": "2025-05-04T14:20:41.231827Z",
     "shell.execute_reply": "2025-05-04T14:20:41.231095Z",
     "shell.execute_reply.started": "2025-05-04T14:20:41.226971Z"
    },
    "id": "JlP2ZBh3-UoM"
   },
   "outputs": [],
   "source": [
    ";# Fetch big_vision repository if python doesn't know about it and install\n",
    "# dependencies needed for this notebook.\n",
    "if not os.path.exists(\"big_vision_repo\"):\n",
    "  !git clone --quiet --branch=main --depth=1 \\\n",
    "     https://github.com/google-research/big_vision big_vision_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:41.232759Z",
     "iopub.status.busy": "2025-05-04T14:20:41.232571Z",
     "iopub.status.idle": "2025-05-04T14:20:41.250854Z",
     "shell.execute_reply": "2025-05-04T14:20:41.250057Z",
     "shell.execute_reply.started": "2025-05-04T14:20:41.232744Z"
    },
    "id": "hjlUuo-E-UlP"
   },
   "outputs": [],
   "source": [
    "# Append big_vision code to python import path\n",
    "if \"big_vision_repo\" not in sys.path:\n",
    "  sys.path.append(\"big_vision_repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:41.252293Z",
     "iopub.status.busy": "2025-05-04T14:20:41.252071Z",
     "iopub.status.idle": "2025-05-04T14:20:44.175000Z",
     "shell.execute_reply": "2025-05-04T14:20:44.174004Z",
     "shell.execute_reply.started": "2025-05-04T14:20:41.252275Z"
    },
    "id": "xAKDOz6_-UKV"
   },
   "outputs": [],
   "source": [
    "# Install missing dependencies. Assume jax~=0.4.25 with GPU available.\n",
    "!pip3 install -q \"overrides\" \"ml_collections\" \"einops~=0.7\" \"sentencepiece\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHH3Ja8F5DoX",
    "outputId": "515a14ea-dfc9-4fa9-fa18-e3d391f0effc"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Model and Pre-trained weights : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:44.816297Z",
     "iopub.status.busy": "2025-05-04T14:20:44.815638Z",
     "iopub.status.idle": "2025-05-04T14:20:44.820459Z",
     "shell.execute_reply": "2025-05-04T14:20:44.819794Z",
     "shell.execute_reply.started": "2025-05-04T14:20:44.816270Z"
    },
    "id": "cxuGG9Ni-GyY"
   },
   "outputs": [],
   "source": [
    "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
    "# vars as appropriate or make your credentials available in ~/.kaggle/kaggle.json\n",
    "\n",
    "os.environ[\"...\"] =    '...'\n",
    "os.environ[\"...\"]      =    '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:45.124692Z",
     "iopub.status.busy": "2025-05-04T14:20:45.124393Z",
     "iopub.status.idle": "2025-05-04T14:20:45.913299Z",
     "shell.execute_reply": "2025-05-04T14:20:45.912662Z",
     "shell.execute_reply.started": "2025-05-04T14:20:45.124666Z"
    },
    "id": "TZ2gt5i4IrXs"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# replace \"hf_XXX\" with your actual token (keep it secret!)\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:45.914641Z",
     "iopub.status.busy": "2025-05-04T14:20:45.914382Z",
     "iopub.status.idle": "2025-05-04T14:20:45.918550Z",
     "shell.execute_reply": "2025-05-04T14:20:45.917806Z",
     "shell.execute_reply.started": "2025-05-04T14:20:45.914616Z"
    },
    "id": "Lbpsmqbd-GvR"
   },
   "outputs": [],
   "source": [
    "# The T4 runtime is tight on memory to finetune this model. Preallocate\n",
    "# all memory ahead of time to avoid OOM'ing due to fragmentation.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:48.122259Z",
     "iopub.status.busy": "2025-05-04T14:20:48.121971Z",
     "iopub.status.idle": "2025-05-04T14:20:48.127273Z",
     "shell.execute_reply": "2025-05-04T14:20:48.126570Z",
     "shell.execute_reply.started": "2025-05-04T14:20:48.122238Z"
    },
    "id": "5GOVNxRG-Gpf"
   },
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = \"./paligemma_tokenizer.model\"\n",
    "if not os.path.exists(TOKENIZER_PATH):\n",
    "    print(\"Downloading the model tokenizer...\")\n",
    "    !wget https://storage.googleapis.com/big_vision/paligemma_tokenizer.model -O {TOKENIZER_PATH}\n",
    "    print(f\"Tokenizer path: {TOKENIZER_PATH}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHnt7Ak6FdLd",
    "outputId": "366fa51a-ffde-49dc-bebf-e7f36fd8ee7d"
   },
   "outputs": [],
   "source": [
    "#!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-aaosbOKja5"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> SIGLIP</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:20:58.485472Z",
     "iopub.status.busy": "2025-05-04T14:20:58.484306Z",
     "iopub.status.idle": "2025-05-04T14:21:01.156790Z",
     "shell.execute_reply": "2025-05-04T14:21:01.156175Z",
     "shell.execute_reply.started": "2025-05-04T14:20:58.485444Z"
    },
    "id": "lwS8F7Y5-Gmy"
   },
   "outputs": [],
   "source": [
    "siglip = AutoModel.from_pretrained(\n",
    "                                  \"google/siglip2-base-patch16-224\",\n",
    "                                  #quantization_config = bnb_config,\n",
    "                                  torch_dtype         = torch.float16,\n",
    "                                  device_map          = \"cpu\",\n",
    "                                  attn_implementation = \"sdpa\",\n",
    "                              ).eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:21:07.094331Z",
     "iopub.status.busy": "2025-05-04T14:21:07.093716Z",
     "iopub.status.idle": "2025-05-04T14:21:13.567606Z",
     "shell.execute_reply": "2025-05-04T14:21:13.566727Z",
     "shell.execute_reply.started": "2025-05-04T14:21:07.094308Z"
    },
    "id": "F1Q8wDi5lpfV"
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\",  use_fast=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Siglip outputs : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:21:13.571843Z",
     "iopub.status.busy": "2025-05-04T14:21:13.571596Z",
     "iopub.status.idle": "2025-05-04T14:21:13.575890Z",
     "shell.execute_reply": "2025-05-04T14:21:13.575234Z",
     "shell.execute_reply.started": "2025-05-04T14:21:13.571825Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "\n",
    "    dev_name  =  \"cuda\"\n",
    "\n",
    "else: dev_name = \"cpu\"\n",
    "\n",
    "\n",
    "device  =  torch.device(dev_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:21:14.622063Z",
     "iopub.status.busy": "2025-05-04T14:21:14.621296Z",
     "iopub.status.idle": "2025-05-04T14:21:14.626066Z",
     "shell.execute_reply": "2025-05-04T14:21:14.625281Z",
     "shell.execute_reply.started": "2025-05-04T14:21:14.622041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_utils import load_image\n",
    "\n",
    "image    =   load_image(\"https://huggingface.co/datasets/merve/coco/resolve/main/val2017/000000000285.jpg\")\n",
    "inputs   =   processor(images=[image], return_tensors=\"pt\").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run infernece\n",
    "with torch.no_grad():\n",
    "    \n",
    "    outputs = siglip.vision_model(pixel_values=inputs[\"pixel_values\"])\n",
    "\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_state.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> GEMMA : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "f4159e4955cb4d1abfd96903b188f667",
      "b6a36a10d9524f599530e9df02879b23",
      "bb9f07ff632b4a778988cabc89bf4033",
      "f92ff29f69bd456583178a9d557fe531",
      "e9ec78ea19cd42e58bc2b80df25f7af2",
      "f182e67c8e89403198aae302fec98e2f",
      "4fe0c3e51a5648659ad9b2a5b165d4f1",
      "4f7478c6db2246328a0cf3a028e99f86",
      "45e338651fdb4b5e8f37863ca3b8b28a",
      "e6e980713e1c490ba92d6d91dd39f5fa",
      "7a69f9647af146c7bfbd8da45ec177d4"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T14:21:30.492398Z",
     "iopub.status.busy": "2025-05-04T14:21:30.491604Z",
     "iopub.status.idle": "2025-05-04T14:21:34.371024Z",
     "shell.execute_reply": "2025-05-04T14:21:34.370466Z",
     "shell.execute_reply.started": "2025-05-04T14:21:30.492372Z"
    },
    "id": "MJKsbiCM-GkN",
    "outputId": "11c525e3-1bcf-4857-8ca3-64ec0281809e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34203d22eac484cb08688c662902cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\n",
    "                                            \"google/gemma-2b\",\n",
    "                                            #quantization_config           = bnb_config,\n",
    "                                            torch_dtype                   = torch.float16,\n",
    "                                            device_map                    = \"cpu\", \n",
    "                                        ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:21:35.879518Z",
     "iopub.status.busy": "2025-05-04T14:21:35.878782Z",
     "iopub.status.idle": "2025-05-04T14:21:35.882901Z",
     "shell.execute_reply": "2025-05-04T14:21:35.882047Z",
     "shell.execute_reply.started": "2025-05-04T14:21:35.879483Z"
    }
   },
   "outputs": [],
   "source": [
    "import sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:21:38.684622Z",
     "iopub.status.busy": "2025-05-04T14:21:38.683832Z",
     "iopub.status.idle": "2025-05-04T14:21:38.852141Z",
     "shell.execute_reply": "2025-05-04T14:21:38.851247Z",
     "shell.execute_reply.started": "2025-05-04T14:21:38.684590Z"
    }
   },
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\") \n",
    "tokenizer = sentencepiece.SentencePieceProcessor(TOKENIZER_PATH)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> GEMMA's outputs : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text   = \" Define the image?\"\n",
    "\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get token embeddings: shape [1, seq_len, 2048]\n",
    "\n",
    "text_embeds = gemma.model.embed_tokens(tokens[\"input_ids\"])\n",
    "\n",
    "print(text_embeds.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Projection after Siglip : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = nn.Linear(768, 2048).half().to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_image_embeds  =  proj(last_hidden_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_image_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Concate Proj + Prompt : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNTagnIEeTd6"
   },
   "outputs": [],
   "source": [
    "combined_embeds = torch.cat([projected_image_embeds, text_embeds], dim=1)  # [1, 197 + seq_len, 2048]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Then, Put them into the GEMMA : </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matching attention mask\n",
    "image_attention_mask    = torch.ones(projected_image_embeds.shape[:-1], dtype=torch.long).to(device)\n",
    "combined_attention_mask = torch.cat([image_attention_mask, tokens[\"attention_mask\"]], dim=1)\n",
    "\n",
    "# Forward pass (important: use inputs_embeds instead of input_ids)\n",
    "outputs = gemma(\n",
    "    inputs_embeds=combined_embeds,\n",
    "    attention_mask=combined_attention_mask,\n",
    ")\n",
    "\n",
    "# You can generate by continuing this input (if needed)\n",
    "logits = outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Combine all parts into one : </h3>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:21:54.797306Z",
     "iopub.status.busy": "2025-05-04T14:21:54.796680Z",
     "iopub.status.idle": "2025-05-04T14:21:54.803679Z",
     "shell.execute_reply": "2025-05-04T14:21:54.802875Z",
     "shell.execute_reply.started": "2025-05-04T14:21:54.797283Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, siglip, gemma, proj_dim=2048):\n",
    "        super().__init__()\n",
    "        self.siglip = siglip.vision_model   # expects pixel_values [B,3,H,W]\n",
    "        self.proj   = nn.Linear(768, proj_dim, bias=False).half()\n",
    "        self.gemma  = gemma\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        images:         torch.FloatTensor of shape [B, 3, H, W]\n",
    "        input_ids:      torch.LongTensor  of shape [B, seq_len]\n",
    "        attention_mask: torch.LongTensor  of shape [B, seq_len]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Image → SigLIP patch embeddings\n",
    "        vision_out = self.siglip(pixel_values=images)         # [B, 197, 768]\n",
    "        img_tokens = vision_out.last_hidden_state.half()      # ensure same dtype as proj\n",
    "\n",
    "        # 2) Project to Gemma's hidden size\n",
    "        proj_img = self.proj(img_tokens)                      # [B, 197, 2048]\n",
    "\n",
    "        # 3) Embed text tokens\n",
    "        txt_embeds = self.gemma.get_input_embeddings()(input_ids)  # [B, seq_len, 2048]\n",
    "\n",
    "        # 4) Concatenate and build attention mask\n",
    "        combined = torch.cat([proj_img, txt_embeds], dim=1)       # [B, 197+seq_len, 2048]\n",
    "        bsz, n_img, _ = proj_img.size()\n",
    "        img_mask = torch.ones(bsz, n_img, device=images.device, dtype=torch.long)\n",
    "        attn_mask = torch.cat([img_mask, attention_mask], dim=1)  # [B, 197+seq_len]\n",
    "\n",
    "        # 5) Forward through Gemma\n",
    "        return self.gemma(inputs_embeds=combined, attention_mask=attn_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:21:59.788573Z",
     "iopub.status.busy": "2025-05-04T14:21:59.787987Z",
     "iopub.status.idle": "2025-05-04T14:21:59.811841Z",
     "shell.execute_reply": "2025-05-04T14:21:59.811016Z",
     "shell.execute_reply.started": "2025-05-04T14:21:59.788547Z"
    }
   },
   "outputs": [],
   "source": [
    "model   =  CombinedModel(siglip, gemma, proj_dim=2048).to(\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:03.366998Z",
     "iopub.status.busy": "2025-05-04T14:22:03.366422Z",
     "iopub.status.idle": "2025-05-04T14:22:03.373879Z",
     "shell.execute_reply": "2025-05-04T14:22:03.372955Z",
     "shell.execute_reply.started": "2025-05-04T14:22:03.366974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 2,600,629,504\n",
      "Trainable params: 2,600,629,504\n"
     ]
    }
   ],
   "source": [
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "print(f\"Trainable params: {trainable_params:,}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Freeze: </h3>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:06.537182Z",
     "iopub.status.busy": "2025-05-04T14:22:06.536426Z",
     "iopub.status.idle": "2025-05-04T14:22:06.541938Z",
     "shell.execute_reply": "2025-05-04T14:22:06.541044Z",
     "shell.execute_reply.started": "2025-05-04T14:22:06.537157Z"
    }
   },
   "outputs": [],
   "source": [
    "def freeze_with_mask(model, is_trainable_param):\n",
    "    trainable_mask = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        trainable = is_trainable_param(name, param)\n",
    "        param.requires_grad = trainable\n",
    "        trainable_mask[name] = trainable\n",
    "        # Optional debug:\n",
    "        # print(f\"{name}: {'TRAIN' if trainable else 'FREEZE'}\")\n",
    "    return trainable_mask\n",
    "\n",
    "\n",
    "# your predicate:\n",
    "def is_trainable_param(name, param):\n",
    "    if name.startswith(\"gemma.model.decoder.layers\") and \"attn\" in name:\n",
    "        return True\n",
    "    if name.startswith(\"proj\"):\n",
    "        return True\n",
    "    # freeze everything else\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:07.576109Z",
     "iopub.status.busy": "2025-05-04T14:22:07.575578Z",
     "iopub.status.idle": "2025-05-04T14:22:07.581083Z",
     "shell.execute_reply": "2025-05-04T14:22:07.580408Z",
     "shell.execute_reply.started": "2025-05-04T14:22:07.576085Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply it:\n",
    "trainable_mask = freeze_with_mask(model, is_trainable_param) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:15.367604Z",
     "iopub.status.busy": "2025-05-04T14:22:15.366915Z",
     "iopub.status.idle": "2025-05-04T14:22:15.376661Z",
     "shell.execute_reply": "2025-05-04T14:22:15.375802Z",
     "shell.execute_reply.started": "2025-05-04T14:22:15.367569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params:     2,600,629,504\n",
      "Trainable params: 1,572,864\n"
     ]
    }
   ],
   "source": [
    "total       =   sum(p.numel() for p in model.parameters())\n",
    "trainable   =   sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total params:     {total:,}\")\n",
    "print(f\"Trainable params: {trainable:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.parameters())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZdrd59PL_82"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Core Library Imports : </h3>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:25:11.142063Z",
     "iopub.status.busy": "2025-05-04T14:25:11.141309Z",
     "iopub.status.idle": "2025-05-04T14:25:11.235671Z",
     "shell.execute_reply": "2025-05-04T14:25:11.234833Z",
     "shell.execute_reply.started": "2025-05-04T14:25:11.142036Z"
    },
    "id": "zE7BtRvz-Ga3"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import functools\n",
    "import html\n",
    "import io\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "\n",
    "import tensorflow as tf\n",
    "import sentencepiece\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from PIL import Image\n",
    "\n",
    "# Import model definition from big_vision\n",
    "from big_vision.models.proj.paligemma import paligemma\n",
    "from big_vision.trainers.proj.paligemma import predict_fns\n",
    "\n",
    "# Import big vision utilities\n",
    "import big_vision.datasets.jsonl\n",
    "import big_vision.utils\n",
    "import big_vision.sharding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_2PUPia51SX",
    "outputId": "87163725-49dc-4124-ddd9-dacee11e15ae"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Reserve GPU/TPU for JAX </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kV8GDGIP51Pn",
    "outputId": "03b414c0-b168-4463-bb47-9c859f94452b"
   },
   "outputs": [],
   "source": [
    "# Don't let TF use the GPU or TPUs\n",
    "# Disables TensorFlow’s access to GPUs/TPUs so JAX can fully utilize them without resource contention.\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "tf.config.set_visible_devices([], \"TPU\")\n",
    "\n",
    "backend = jax.extend.backend.get_backend()\n",
    "print(f\"JAX version:  {jax.__version__}\")\n",
    "print(f\"JAX platform: {backend.platform}\")\n",
    "print(f\"JAX devices:  {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YQdjtXR5OB9",
    "outputId": "0bd31fa4-f61c-4832-b5d0-5d8b1bede748"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\">Construct model and load params into RAM </h3>\n",
    "\n",
    " <h5 style=\"color:red;\"> model_config: hyperparameters for both the vision encoder and text decoder.\n",
    "<br>\n",
    "                          Instantiate the combined Vision+LLM model.\n",
    "<br>\n",
    "                          Load pretrained weights into a parameter tree.\n",
    "<br>\n",
    "                          Build a decode function for efficient batched generation.\n",
    "                          </h5>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:25:19.731394Z",
     "iopub.status.busy": "2025-05-04T14:25:19.730033Z",
     "iopub.status.idle": "2025-05-04T14:25:19.795946Z",
     "shell.execute_reply": "2025-05-04T14:25:19.795398Z",
     "shell.execute_reply.started": "2025-05-04T14:25:19.731363Z"
    },
    "id": "ZTUsR5eK-GIU"
   },
   "outputs": [],
   "source": [
    "# Define `decode` function to sample outputs from the model.\n",
    "decode_fn  =   predict_fns.get_all(model)['decode']\n",
    "decode     =   functools.partial(decode_fn, devices=jax.devices(), eos_token=tokenizer.eos_id()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI7uthrl7YnL",
    "outputId": "9488b99a-2eed-40f3-f9b3-cd474b5ca0da"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Sharding & Casting Parameters :  </h3>\n",
    "\n",
    "  <h5 style=\"color:red;\">  Sharding: split tensors across devices (if you had >1 GPU).\n",
    "<br>\n",
    "                        maybe_cast_to_f32: keep the frozen weights in fp16 to save memory; cast the few trainable ones to fp32 so their gradients remain stable.\n",
    "<br>\n",
    "                        The loop unpacks the parameter tree, reshares & casts each leaf, and reassembles it.\n",
    "</h5>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZK4rvokI-fc"
   },
   "outputs": [],
   "source": [
    "# If more than one device is available (e.g. multiple GPUs) the parameters can\n",
    "# be sharded across them to reduce HBM usage per device.\n",
    "mesh = jax.sharding.Mesh(jax.devices(), (\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMp3esfH-F6X"
   },
   "outputs": [],
   "source": [
    "data_sharding = jax.sharding.NamedSharding(\n",
    "    mesh, jax.sharding.PartitionSpec(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sharding = big_vision.sharding.infer_sharding(\n",
    "    params, strategy=[('.*', 'fsdp(axis=\"data\")')], mesh=mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes: Some donated buffers are not usable.\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"Some donated buffers were not usable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, donate_argnums=(0,), static_argnums=(1,))\n",
    "def maybe_cast_to_f32(params, trainable):\n",
    "  # Cast others to float16, since some GPUs don't support bf16.\n",
    "  return jax.tree.map(lambda p, m: p.astype(jnp.float32)\n",
    "                      if m else p.astype(jnp.float16),\n",
    "                      params, trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy_safe(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_flat, treedef       = jax.tree.flatten(params)\n",
    "sharding_leaves            = jax.tree.leaves(params_sharding)\n",
    "trainable_leaves           = jax.tree.leaves(trainable_mask)\n",
    "\n",
    "new_params_flat = []\n",
    "\n",
    "for idx, (param, sharding, trainable) in enumerate(zip(params_flat, sharding_leaves, trainable_leaves)):\n",
    "    param = to_numpy_safe(param)\n",
    "    param = big_vision.utils.reshard(param, sharding)\n",
    "    param = maybe_cast_to_f32(param, trainable)\n",
    "    param.block_until_ready()\n",
    "    new_params_flat.append(param)\n",
    "\n",
    "params = jax.tree.unflatten(treedef, new_params_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print params to show what the model is made of.\n",
    "def parameter_overview(params):\n",
    "  for path, arr in big_vision.utils.tree_flatten_with_names(params)[0]:\n",
    "    print(f\"{path:80s} {str(arr.shape):22s} {arr.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" == Model params == \")\n",
    "parameter_overview(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T21:05:03.618581Z",
     "iopub.status.busy": "2025-05-01T21:05:03.618308Z",
     "iopub.status.idle": "2025-05-01T21:05:03.623664Z",
     "shell.execute_reply": "2025-05-01T21:05:03.623032Z",
     "shell.execute_reply.started": "2025-05-01T21:05:03.618560Z"
    },
    "id": "WH_1HRpMqhuW",
    "outputId": "a6037f1d-e8cf-436c-d18a-47404ff945eb"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Define preprocess functions to create inputs to the model :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:28.453335Z",
     "iopub.status.busy": "2025-05-04T14:22:28.452811Z",
     "iopub.status.idle": "2025-05-04T14:22:28.461473Z",
     "shell.execute_reply": "2025-05-04T14:22:28.460704Z",
     "shell.execute_reply.started": "2025-05-04T14:22:28.453309Z"
    },
    "id": "0zrFyYeVEAHD"
   },
   "outputs": [],
   "source": [
    "# @title Define preprocess functions to create inputs to the model.\n",
    "\n",
    "def preprocess_image(image, size=224):\n",
    "  # Model has been trained to handle images of different aspects ratios\n",
    "  # resized to 224x224 in the range [-1, 1]. Bilinear and antialias resize\n",
    "  # options are helpful to improve quality in some tasks.\n",
    "  image = np.asarray(image)\n",
    "  if image.ndim == 2:  # Convert image without last channel into greyscale.\n",
    "    image = np.stack((image,)*3, axis=-1)\n",
    "  image = image[..., :3]  # Remove alpha layer.\n",
    "  assert image.shape[-1] == 3\n",
    "\n",
    "  image = tf.constant(image)\n",
    "  image = tf.image.resize(image, (size, size), method='bilinear', antialias=True)\n",
    "  return image.numpy() / 127.5 - 1.0  # [0, 255]->[-1,1]\n",
    "\n",
    "def preprocess_tokens(prefix, suffix=None, seqlen=None):\n",
    "  # Model has been trained to handle tokenized text composed of a prefix with\n",
    "  # full attention and a suffix with causal attention.\n",
    "  separator = \"\\n\"\n",
    "  tokens = tokenizer.encode(prefix, add_bos=True) + tokenizer.encode(separator)\n",
    "  mask_ar = [0] * len(tokens)    # 0 to use full attention for prefix.\n",
    "  mask_loss = [0] * len(tokens)  # 0 to not use prefix tokens in the loss.\n",
    "\n",
    "  if suffix:\n",
    "    suffix = tokenizer.encode(suffix, add_eos=True)\n",
    "    tokens += suffix\n",
    "    mask_ar += [1] * len(suffix)    # 1 to use causal attention for suffix.\n",
    "    mask_loss += [1] * len(suffix)  # 1 to use suffix tokens in the loss.\n",
    "\n",
    "  mask_input = [1] * len(tokens)    # 1 if its a token, 0 if padding.\n",
    "  if seqlen:\n",
    "    padding = [0] * max(0, seqlen - len(tokens))\n",
    "    tokens = tokens[:seqlen] + padding\n",
    "    mask_ar = mask_ar[:seqlen] + padding\n",
    "    mask_loss = mask_loss[:seqlen] + padding\n",
    "    mask_input = mask_input[:seqlen] + padding\n",
    "\n",
    "  return jax.tree.map(np.array, (tokens, mask_ar, mask_loss, mask_input))\n",
    "\n",
    "def postprocess_tokens(tokens):\n",
    "  tokens = tokens.tolist()  # np.array to list[int]\n",
    "  try:  # Remove tokens at and after EOS if any.\n",
    "    eos_pos = tokens.index(tokenizer.eos_id())\n",
    "    tokens = tokens[:eos_pos]\n",
    "  except ValueError:\n",
    "    pass\n",
    "  return tokenizer.decode(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SoyORZgquEe",
    "outputId": "7f3a2fa9-144c-4deb-e7b0-9466ffe8d1bf"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Import Data  :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:32.020609Z",
     "iopub.status.busy": "2025-05-04T14:22:32.020267Z",
     "iopub.status.idle": "2025-05-04T14:22:32.024779Z",
     "shell.execute_reply": "2025-05-04T14:22:32.023905Z",
     "shell.execute_reply.started": "2025-05-04T14:22:32.020587Z"
    },
    "id": "_HgszSHUGy0A"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AN1pF2QFI-fe",
    "outputId": "30c8f82c-1490-46a5-dbcb-da64c008a6eb"
   },
   "outputs": [],
   "source": [
    "# List all datasets under /kaggle/input\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    print(f\"Directory: {dirname}\")\n",
    "    for filename in filenames:\n",
    "        print(f\"  File: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:42.556547Z",
     "iopub.status.busy": "2025-05-04T14:22:42.555877Z",
     "iopub.status.idle": "2025-05-04T14:22:42.806020Z",
     "shell.execute_reply": "2025-05-04T14:22:42.805383Z",
     "shell.execute_reply.started": "2025-05-04T14:22:42.556519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source split           image  \\\n",
      "0   NWPU  test  NWPU_31430.jpg   \n",
      "1   NWPU  test  NWPU_31431.jpg   \n",
      "2   NWPU  test  NWPU_31432.jpg   \n",
      "3   NWPU  test  NWPU_31433.jpg   \n",
      "4   NWPU  test  NWPU_31434.jpg   \n",
      "\n",
      "                                           caption_1  \\\n",
      "0   A gray plane on the runway and the lawn beside .   \n",
      "1  Three small planes parked in a line on the air...   \n",
      "2  A plane parked in a line on the airport with s...   \n",
      "3  A small plane and a big plane parked next to b...   \n",
      "4       Two planes parked next to boarding bridges .   \n",
      "\n",
      "                                           caption_2  \\\n",
      "0        A grey plane is on the runway by the lawn .   \n",
      "1  There are four aircraft on the open ground, Th...   \n",
      "2  A white plane was parked on the instruction li...   \n",
      "3  A white plane and a gray plane parked at the b...   \n",
      "4  Two aircraft were parked at the departure gates .   \n",
      "\n",
      "                                           caption_3  \\\n",
      "0  There is an airplane on the runway with a larg...   \n",
      "1  There are many planes of different sizes in a ...   \n",
      "2  An airplane parked in an open area with many c...   \n",
      "3  Two planes of different sizes are neatly parke...   \n",
      "4  Two planes of different sizes are neatly parke...   \n",
      "\n",
      "                                           caption_4  \\\n",
      "0  A plane is parked on the runway next to the gr...   \n",
      "1             Four planes are parked on the runway .   \n",
      "2              A plane is parked on the open space .   \n",
      "3  A large plane and a small plane are parked nea...   \n",
      "4       Two planes are parked next to the terminal .   \n",
      "\n",
      "                                           caption_5  \n",
      "0  There is a plane on the runway beside the grass .  \n",
      "1  Four planes of different sizes were on the mar...  \n",
      "2            There is 1 plane on the ground marked .  \n",
      "3              Two planes are on the marked ground .  \n",
      "4              Two planes are on the marked ground .  \n"
     ]
    }
   ],
   "source": [
    "captions_df = pd.read_csv('/kaggle/input/rsics-dataset/captions.csv')\n",
    "print(captions_df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:45.680285Z",
     "iopub.status.busy": "2025-05-04T14:22:45.679536Z",
     "iopub.status.idle": "2025-05-04T14:22:45.683718Z",
     "shell.execute_reply": "2025-05-04T14:22:45.683036Z",
     "shell.execute_reply.started": "2025-05-04T14:22:45.680240Z"
    },
    "id": "vy0cc5BzI-fe"
   },
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_ROOT = \"/kaggle/input/rsics-dataset/resized\" \n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE   = (224, 224)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:46.352156Z",
     "iopub.status.busy": "2025-05-04T14:22:46.351327Z",
     "iopub.status.idle": "2025-05-04T14:22:46.361853Z",
     "shell.execute_reply": "2025-05-04T14:22:46.361142Z",
     "shell.execute_reply.started": "2025-05-04T14:22:46.352125Z"
    },
    "id": "-F5zSNzPKHOu",
    "outputId": "07b979e3-5d53-4451-ff4d-611f111de16a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All split labels: ['test' 'val' 'train']\n",
      "Counts:\n",
      " split\n",
      "train    35614\n",
      "test      4454\n",
      "val       4453\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"All split labels:\", captions_df['split'].unique())\n",
    "print(\"Counts:\\n\", captions_df['split'].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:47.111297Z",
     "iopub.status.busy": "2025-05-04T14:22:47.110457Z",
     "iopub.status.idle": "2025-05-04T14:22:47.141062Z",
     "shell.execute_reply": "2025-05-04T14:22:47.140407Z",
     "shell.execute_reply.started": "2025-05-04T14:22:47.111259Z"
    },
    "id": "bYrO8v1UKHL4"
   },
   "outputs": [],
   "source": [
    "# 4. Filter into splits\n",
    "splits = {}\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    splits[split_name] = captions_df[captions_df['split'] == split_name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:47.715215Z",
     "iopub.status.busy": "2025-05-04T14:22:47.714420Z",
     "iopub.status.idle": "2025-05-04T14:22:47.720638Z",
     "shell.execute_reply": "2025-05-04T14:22:47.719803Z",
     "shell.execute_reply.started": "2025-05-04T14:22:47.715185Z"
    },
    "id": "9boO9ocTKGiO"
   },
   "outputs": [],
   "source": [
    "# 5. Convert each split-DataFrame into (paths, captions)\n",
    "def df_to_paths_and_captions(split_df):\n",
    "    # Full paths\n",
    "    paths = split_df['image'].apply(lambda fn: os.path.join(IMAGE_ROOT, fn)).tolist()\n",
    "    # List-of-captions per example\n",
    "    captions_cols = [f'caption_{i}' for i in range(1,6)]\n",
    "    captions = split_df[captions_cols].values.tolist()\n",
    "    return paths, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:48.567223Z",
     "iopub.status.busy": "2025-05-04T14:22:48.566932Z",
     "iopub.status.idle": "2025-05-04T14:22:48.630893Z",
     "shell.execute_reply": "2025-05-04T14:22:48.630324Z",
     "shell.execute_reply.started": "2025-05-04T14:22:48.567202Z"
    },
    "id": "UTtccQ1oIP7S"
   },
   "outputs": [],
   "source": [
    "train_paths, train_caps = df_to_paths_and_captions(splits['train'])\n",
    "val_paths,   val_caps   = df_to_paths_and_captions(splits['val'])\n",
    "test_paths,  test_caps  = df_to_paths_and_captions(splits['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:51.383052Z",
     "iopub.status.busy": "2025-05-04T14:22:51.382764Z",
     "iopub.status.idle": "2025-05-04T14:22:51.389048Z",
     "shell.execute_reply": "2025-05-04T14:22:51.388316Z",
     "shell.execute_reply.started": "2025-05-04T14:22:51.383031Z"
    },
    "id": "Wgpm8HTEISvx"
   },
   "outputs": [],
   "source": [
    "# 6. Preprocessing fn: load image + return captions list\n",
    "def _load_and_preprocess(path, captions):\n",
    "    # Read & decode\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    # Resize & normalize\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img, captions\n",
    "\n",
    "# 7. Build the tf.data pipeline\n",
    "def make_dataset(paths, captions, shuffle=False):\n",
    "    # turn your Python list-of-strings into a tf.string tensor\n",
    "    paths_ds = tf.data.Dataset.from_tensor_slices(tf.constant(paths, dtype=tf.string))\n",
    "    # turn your list-of-lists-of-strings into a [5] tf.string tensor\n",
    "    caps_ds  = tf.data.Dataset.from_tensor_slices(tf.constant(captions, dtype=tf.string))\n",
    "\n",
    "    ds = tf.data.Dataset.zip((paths_ds, caps_ds))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(paths))\n",
    "    ds = ( ds\n",
    "           .map(_load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "           .batch(BATCH_SIZE)\n",
    "           .prefetch(tf.data.AUTOTUNE)\n",
    "         )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:22:51.805913Z",
     "iopub.status.busy": "2025-05-04T14:22:51.805627Z",
     "iopub.status.idle": "2025-05-04T14:22:52.066060Z",
     "shell.execute_reply": "2025-05-04T14:22:52.065319Z",
     "shell.execute_reply.started": "2025-05-04T14:22:51.805892Z"
    },
    "id": "oWuvduvbIE-H"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746368571.909709     365 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "train_ds = make_dataset(train_paths, train_caps, shuffle=True)\n",
    "val_ds   = make_dataset(val_paths,   val_caps,   shuffle=True)\n",
    "test_ds  = make_dataset(test_paths,  test_caps,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-04T14:23:18.291190Z",
     "iopub.status.busy": "2025-05-04T14:23:18.290616Z",
     "iopub.status.idle": "2025-05-04T14:23:18.448678Z",
     "shell.execute_reply": "2025-05-04T14:23:18.447838Z",
     "shell.execute_reply.started": "2025-05-04T14:23:18.291168Z"
    },
    "id": "mHiB8o5aIE7W",
    "outputId": "7a6f4417-8b73-48bb-cadd-8fb969006344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images batch shape: (32, 224, 224, 3)\n",
      "Captions batch shape: 32 examples\n",
      "First example captions: tf.Tensor(\n",
      "[b'A gray plane on the runway and the lawn beside .'\n",
      " b'A grey plane is on the runway by the lawn .'\n",
      " b'There is an airplane on the runway with a large lawn by the runway .'\n",
      " b'A plane is parked on the runway next to the grass .'\n",
      " b'There is a plane on the runway beside the grass .'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 8. Quick sanity check\n",
    "for imgs, caps in test_ds.take(1):\n",
    "    print(\"Images batch shape:\", imgs.shape)            # (BATCH_SIZE, H, W, 3)\n",
    "    print(\"Captions batch shape:\", len(caps), \"examples\")\n",
    "    print(\"First example captions:\", caps[0])            # a list of 5 strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:23:21.361405Z",
     "iopub.status.busy": "2025-05-04T14:23:21.360777Z",
     "iopub.status.idle": "2025-05-04T14:23:21.368696Z",
     "shell.execute_reply": "2025-05-04T14:23:21.368098Z",
     "shell.execute_reply.started": "2025-05-04T14:23:21.361378Z"
    },
    "id": "emSWd_NsEAEA"
   },
   "outputs": [],
   "source": [
    "def train_data_iterator():\n",
    "    \"\"\"Never-ending iterator over training examples from train_ds.\"\"\"\n",
    "    while True:\n",
    "        for image_batch, caps_batch in train_ds:\n",
    "            images_np = image_batch.numpy()      # [B,H,W,3]\n",
    "            caps_np   = caps_batch.numpy()       # [B,5] bytes\n",
    "\n",
    "            for img, caps in zip(images_np, caps_np):\n",
    "                # Decode all 5 captions into Python strings\n",
    "                decoded_caps = [c.decode('utf-8') for c in caps]\n",
    "\n",
    "                # Prepare the model input from a random caption\n",
    "                suffix = decoded_caps[np.random.randint(5)].lower()\n",
    "                prefix = \"caption en\"\n",
    "\n",
    "                tokens, mask_ar, mask_loss, _ = preprocess_tokens(\n",
    "                    prefix, suffix, SEQLEN\n",
    "                )\n",
    "\n",
    "                img_proc = img * 2.0 - 1.0\n",
    "                yield [{\n",
    "                    \"image\":     img_proc,\n",
    "                    \"text\":      np.asarray(tokens,    dtype=np.int32),\n",
    "                    \"mask_ar\":   np.asarray(mask_ar,    dtype=np.int32),\n",
    "                    \"mask_loss\": np.asarray(mask_loss,  dtype=np.int32),\n",
    "                },\n",
    "                    decoded_caps,\n",
    "                      ]\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------#\n",
    "#                                              Validation :\n",
    "#----------------------------------------------------------------------------------------------------------------#\n",
    "def validation_data_iterator():\n",
    "    \"\"\"Single-pass iterator over validation examples from val_ds.\"\"\"\n",
    "    for image_batch, caps_batch in val_ds:\n",
    "\n",
    "         images_np = image_batch.numpy()\n",
    "         caps_np   = caps_batch.numpy()\n",
    "\n",
    "         for img, caps in zip(images_np, caps_np):\n",
    "\n",
    "            decoded_caps =   [c.decode('utf-8') for c in caps]\n",
    "            suffix       =   decoded_caps[np.random.randint(5)].lower()\n",
    "            prefix       =   \"caption en\"\n",
    "\n",
    "            tokens, mask_ar, _, mask_input = preprocess_tokens(\n",
    "                 prefix,  seqlen=SEQLEN)\n",
    "\n",
    "            img_proc = img * 2. - 1.\n",
    "            yield [{\n",
    "                 \"image\":     img_proc,\n",
    "                 \"text\":      np.asarray(tokens,     dtype=np.int32),\n",
    "                 \"mask_input\": np.asarray(mask_input, dtype=np.int32),\n",
    "                 \"mask_ar\":    np.asarray(mask_ar,    dtype=np.int32),\n",
    "            },\n",
    "                 decoded_caps,\n",
    "                  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEauChmYI-fh"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Check Iterators :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:23:26.968211Z",
     "iopub.status.busy": "2025-05-04T14:23:26.967563Z",
     "iopub.status.idle": "2025-05-04T14:23:26.971694Z",
     "shell.execute_reply": "2025-05-04T14:23:26.971008Z",
     "shell.execute_reply.started": "2025-05-04T14:23:26.968190Z"
    },
    "id": "oZoIUr62LeWX"
   },
   "outputs": [],
   "source": [
    "SEQLEN   =   128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NY0f9D5I-fh"
   },
   "outputs": [],
   "source": [
    "val_it = validation_data_iterator()\n",
    "\n",
    "for _ in range(1):\n",
    "    ex   = next(val_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16H9-ugFI-fh",
    "outputId": "23155adb-4253-4fca-9001-2f4081b5d454"
   },
   "outputs": [],
   "source": [
    "ex[0][\"image\"].shape,   ex[0][\"text\"].shape,  ex[0][\"mask_input\"].shape,   ex[0][\"mask_ar\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQ90yvl0I-fi",
    "outputId": "cfd72a0d-78fc-4f55-a3b4-55815c8a0439"
   },
   "outputs": [],
   "source": [
    "ex[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python```\n",
    "    def forward(self,\n",
    "                image: torch.FloatTensor,        # [3, H, W]\n",
    "                input_ids: torch.LongTensor,     # [seq_len]\n",
    "                attention_mask: torch.LongTensor # [seq_len]\n",
    "               ):\n",
    "        # --- 1) Image → SigLIP patches ---\n",
    "        # add batch dim\n",
    "        pixels = image.unsqueeze(0)                     # [1,3,H,W]\n",
    "        vision_out = self.siglip(pixel_values=pixels)   # HF API: pixel_values\n",
    "        img_tokens = vision_out.last_hidden_state.half()# [1,197,768]\n",
    "\n",
    "        # --- 2) Project to Gemma’s hidden size ---\n",
    "        proj_img = self.proj(img_tokens)                # [1,197,2048]\n",
    "\n",
    "        # --- 3) Embed text tokens ---\n",
    "        # add batch dim\n",
    "        ids  = input_ids.unsqueeze(0)                   # [1, seq_len]\n",
    "        mask = attention_mask.unsqueeze(0)              # [1, seq_len]\n",
    "        txt_embeds = self.gemma.get_input_embeddings()(ids)  # [1,seq_len,2048]\n",
    "\n",
    "        # --- 4) Concat and build attention mask ---\n",
    "        combined = torch.cat([proj_img, txt_embeds], dim=1)  \n",
    "        img_mask = torch.ones(proj_img.size()[:2], dtype=torch.long, device=combined.device)\n",
    "        attn_mask = torch.cat([img_mask, mask], dim=1)  \n",
    "\n",
    "        # --- 5) Forward into Gemma as continuous embeddings ---\n",
    "        return self.gemma(inputs_embeds=combined, attention_mask=attn_mask)\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrE27d6rEABL"
   },
   "outputs": [],
   "source": [
    "# @title Inspect training examples.\n",
    "def render_inline(image, resize=(128, 128)):\n",
    "  \"\"\"Convert image into inline html.\"\"\"\n",
    "  image = Image.fromarray(image)\n",
    "  image.resize(resize)\n",
    "  with io.BytesIO() as buffer:\n",
    "    image.save(buffer, format='jpeg')\n",
    "    image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n",
    "    return f\"data:image/jpeg;base64,{image_b64}\"\n",
    "\n",
    "def render_example(image, caption):\n",
    "  image = ((image + 1)/2 * 255).astype(np.uint8)  # [-1,1] -> [0, 255]\n",
    "  return f\"\"\"\n",
    "    <div style=\"display: inline-flex; align-items: center; justify-content: center;\">\n",
    "        <img style=\"width:128px; height:128px;\" src=\"{render_inline(image, resize=(64,64))}\" />\n",
    "        <p style=\"width:256px; margin:10px; font-size:small;\">{html.escape(caption)}</p>\n",
    "    </div>\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPxzL2EJI-fi"
   },
   "outputs": [],
   "source": [
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "807hpxWVK2Ee",
    "outputId": "f1bd3809-288d-4914-b235-828703806224"
   },
   "outputs": [],
   "source": [
    "html_out = \"\"\n",
    "for idx, example in zip(range(4), train_data_iterator()):\n",
    "    img = example[0][\"image\"]\n",
    "\n",
    "    html_out += render_example(img, example[1][0])\n",
    "    # optionally a divider between images:\n",
    "    html_out += \"<hr style='width:100%;'>\"\n",
    "\n",
    "display(HTML(\"<h3>Training examples </h3>\" + html_out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4tWuAD5hrBs",
    "outputId": "0fd7ff49-11a1-4ccd-aedf-1ff9df65c91d"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Define the training step and evaluation loop :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:30:10.670876Z",
     "iopub.status.busy": "2025-05-04T14:30:10.670177Z",
     "iopub.status.idle": "2025-05-04T14:30:10.724656Z",
     "shell.execute_reply": "2025-05-04T14:30:10.724096Z",
     "shell.execute_reply.started": "2025-05-04T14:30:10.670847Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from PIL import Image\n",
    "from IPython.display import HTML, display\n",
    "import io, base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:30:14.150141Z",
     "iopub.status.busy": "2025-05-04T14:30:14.149650Z",
     "iopub.status.idle": "2025-05-04T14:30:14.172858Z",
     "shell.execute_reply": "2025-05-04T14:30:14.172129Z",
     "shell.execute_reply.started": "2025-05-04T14:30:14.150118Z"
    },
    "id": "7_QtGvX6OYUT"
   },
   "outputs": [],
   "source": [
    "model        = CombinedModel(siglip, gemma, proj_dim=2048).to(device)\n",
    "freeze_with_mask(model, is_trainable_param)  # freeze everything except attn & proj\n",
    "opt          = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:30:35.092869Z",
     "iopub.status.busy": "2025-05-04T14:30:35.092596Z",
     "iopub.status.idle": "2025-05-04T14:30:35.097392Z",
     "shell.execute_reply": "2025-05-04T14:30:35.096690Z",
     "shell.execute_reply.started": "2025-05-04T14:30:35.092852Z"
    }
   },
   "outputs": [],
   "source": [
    "class TorchDataset(torch.utils.data.IterableDataset):\n",
    "    def __iter__(self):\n",
    "        for example, _ in train_data_iterator():\n",
    "            yield example\n",
    "\n",
    "train_loader = DataLoader(TorchDataset(), batch_size=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:30:48.526633Z",
     "iopub.status.busy": "2025-05-04T14:30:48.526077Z",
     "iopub.status.idle": "2025-05-04T14:30:48.704740Z",
     "shell.execute_reply": "2025-05-04T14:30:48.704105Z",
     "shell.execute_reply.started": "2025-05-04T14:30:48.526609Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    \n",
    "    batch[\"image\"]     =    batch[\"image\"].to(device)\n",
    "    batch[\"text\"]      =    batch[\"text\"].to(device)\n",
    "    batch[\"mask_ar\"]   =    batch[\"mask_ar\"].to(device)\n",
    "    \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:31:26.415476Z",
     "iopub.status.busy": "2025-05-04T14:31:26.414729Z",
     "iopub.status.idle": "2025-05-04T14:31:26.693728Z",
     "shell.execute_reply": "2025-05-04T14:31:26.692899Z",
     "shell.execute_reply.started": "2025-05-04T14:31:26.415444Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = model(batch[\"image\"].view(8, 3, 224, 224), batch[\"text\"], batch[\"mask_ar\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:31:35.417442Z",
     "iopub.status.busy": "2025-05-04T14:31:35.416529Z",
     "iopub.status.idle": "2025-05-04T14:31:35.503509Z",
     "shell.execute_reply": "2025-05-04T14:31:35.502782Z",
     "shell.execute_reply.started": "2025-05-04T14:31:35.417408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[-25.1562, -12.0234, -77.7500,  ..., -18.4688, -20.9844, -25.0312],\n",
       "         [127.3125,  33.1562, -30.5781,  ...,  61.7812,  78.8125, 127.3750],\n",
       "         [ 19.8438,  -0.4709, -61.0938,  ...,   5.2266,   4.9102,  19.9219],\n",
       "         ...,\n",
       "         [-18.0781,   2.2031, -35.8125,  ..., -13.7188,  -9.3672, -17.9531],\n",
       "         [-19.0469,   2.8926, -41.7812,  ..., -12.9375,  -7.7930, -18.9531],\n",
       "         [-22.7812,  -0.3174, -38.5625,  ..., -15.6719, -11.5547, -22.7188]],\n",
       "\n",
       "        [[ 60.4062,  15.5938, -52.0312,  ...,  27.9375,  31.7031,  60.5312],\n",
       "         [125.4375,  33.0312, -26.3438,  ...,  60.5312,  77.2500, 125.5625],\n",
       "         [-26.9062, -12.0547, -72.3125,  ..., -26.7031, -26.4375, -26.7969],\n",
       "         ...,\n",
       "         [-11.1484,   9.9141, -14.4844,  ...,  -2.8105,  -4.3008, -11.0547],\n",
       "         [-20.3125,  -1.7432, -26.0625,  ..., -14.3906, -14.8906, -20.2969],\n",
       "         [-22.0781,  -2.7988, -21.7344,  ..., -13.4062, -16.2812, -22.0469]],\n",
       "\n",
       "        [[ 26.0312,   3.7168, -57.4062,  ...,  11.4141,  12.1406,  26.1094],\n",
       "         [116.0625,  29.7812, -34.5938,  ...,  55.9375,  71.2500, 116.1250],\n",
       "         [ 39.2812,   6.9609, -52.5312,  ...,  15.8359,  20.4219,  39.4688],\n",
       "         ...,\n",
       "         [-17.8750,   3.9336, -33.2812,  ..., -12.1953,  -7.3477, -17.7500],\n",
       "         [-16.5469,   9.6172, -29.0156,  ..., -10.8281,  -6.8906, -16.3594],\n",
       "         [-19.6250,   5.4375, -30.7344,  ..., -14.7344, -10.0000, -19.4375]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ -2.2031,  -6.9922, -64.0000,  ...,  -5.2500,  -6.4453,  -2.0332],\n",
       "         [113.1250,  29.8750, -37.4375,  ...,  54.1875,  69.5000, 113.2500],\n",
       "         [  0.1609,  -6.4414, -67.1250,  ...,  -6.7148,  -8.8828,   0.2761],\n",
       "         ...,\n",
       "         [ -9.4453,   0.2273,  -5.7656,  ..., -10.3125,  -8.3359,  -9.3828],\n",
       "         [-15.8516,  -4.0977, -18.4688,  ..., -13.8047, -12.1953, -15.8438],\n",
       "         [-14.5938,  -5.1094, -15.0156,  ..., -10.3047,  -9.3125, -14.7188]],\n",
       "\n",
       "        [[104.1875,  30.9375, -30.6250,  ...,  55.4375,  64.8750, 104.3125],\n",
       "         [118.8125,  31.5938, -30.6719,  ...,  57.6250,  73.6875, 118.8750],\n",
       "         [ 83.0000,  24.0156, -54.5938,  ...,  38.9062,  49.0000,  83.1250],\n",
       "         ...,\n",
       "         [ 22.4688,   1.7842,  -0.6133,  ...,   6.6875,   8.9453,  22.4844],\n",
       "         [ 40.3750,   2.3945,   5.9961,  ...,  16.5625,  19.3438,  40.4062],\n",
       "         [ 19.8594,   5.1094,  -8.1094,  ...,   1.9189,   8.0859,  19.9219]],\n",
       "\n",
       "        [[102.2500,  34.0312, -51.6562,  ...,  53.3125,  61.5938, 102.3125],\n",
       "         [120.6250,  32.2188, -30.4375,  ...,  58.3750,  74.8750, 120.6875],\n",
       "         [ 63.6875,  15.9062, -52.5938,  ...,  31.9375,  37.6562,  63.7812],\n",
       "         ...,\n",
       "         [-25.7656,  -7.5625, -29.5000,  ..., -20.7188, -19.5156, -25.6875],\n",
       "         [-19.0312,  -0.8315, -25.0781,  ..., -13.1328, -11.4766, -18.9531],\n",
       "         [-15.7109,   2.3105, -22.9531,  ..., -10.6406,  -7.0039, -15.6484]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x7a1537e3a690>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:32:37.389192Z",
     "iopub.status.busy": "2025-05-04T14:32:37.388908Z",
     "iopub.status.idle": "2025-05-04T14:32:37.393670Z",
     "shell.execute_reply": "2025-05-04T14:32:37.392925Z",
     "shell.execute_reply.started": "2025-05-04T14:32:37.389171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 324, 256000])\n"
     ]
    }
   ],
   "source": [
    "logits  = outputs.logits       # [B, 197+seq_len, vocab_size] \n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:38:30.735160Z",
     "iopub.status.busy": "2025-05-04T14:38:30.734286Z",
     "iopub.status.idle": "2025-05-04T14:38:30.740116Z",
     "shell.execute_reply": "2025-05-04T14:38:30.739501Z",
     "shell.execute_reply.started": "2025-05-04T14:38:30.735136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"text\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:40:00.749281Z",
     "iopub.status.busy": "2025-05-04T14:40:00.748454Z",
     "iopub.status.idle": "2025-05-04T14:40:00.767919Z",
     "shell.execute_reply": "2025-05-04T14:40:00.767322Z",
     "shell.execute_reply.started": "2025-05-04T14:40:00.749239Z"
    }
   },
   "outputs": [],
   "source": [
    "B, total_len, V = logits.size()\n",
    "seq_len         = batch[\"text\"].size(1)\n",
    "L_in            = seq_len - 1\n",
    "L_out           = L_in\n",
    "N               = total_len - L_in\n",
    "\n",
    "text_logits = logits[:, N : N + L_out, :].reshape(-1, V)  # [B*L_out, V]\n",
    "targets     = batch[\"text\"][:, 1:].reshape(-1).long()     # [B*L_out] as LongTensor\n",
    "\n",
    "pad_id = tokenizer.pad_id()\n",
    "\n",
    "loss = F.cross_entropy(text_logits, targets, ignore_index=pad_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T14:40:08.091165Z",
     "iopub.status.busy": "2025-05-04T14:40:08.090565Z",
     "iopub.status.idle": "2025-05-04T14:40:08.097322Z",
     "shell.execute_reply": "2025-05-04T14:40:08.096764Z",
     "shell.execute_reply.started": "2025-05-04T14:40:08.091143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(169., device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:37:59.333628Z",
     "iopub.status.busy": "2025-05-02T07:37:59.333126Z",
     "iopub.status.idle": "2025-05-02T07:37:59.339059Z",
     "shell.execute_reply": "2025-05-02T07:37:59.337947Z",
     "shell.execute_reply.started": "2025-05-02T07:37:59.333604Z"
    },
    "id": "PuqC8L--tMhq",
    "outputId": "bcb50f18-7b1b-4b57-fd84-2b3156af0f0f"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> WANDB :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aDEF3c_VE0U"
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T15:17:27.399701Z",
     "iopub.status.busy": "2025-05-04T15:17:27.398951Z",
     "iopub.status.idle": "2025-05-04T15:17:27.403116Z",
     "shell.execute_reply": "2025-05-04T15:17:27.402313Z",
     "shell.execute_reply.started": "2025-05-04T15:17:27.399675Z"
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJd2cBlEVBuQ"
   },
   "outputs": [],
   "source": [
    "os.environ[\"...\"]       = \"...\"\n",
    "\n",
    "run  =  wandb.init(project=\"Phase_2___Siglip_plus_Proj_plus_Gemma\", entity=\"DI_725___Final_Project\" ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:40:50.298843Z",
     "iopub.status.busy": "2025-05-02T07:40:50.298628Z",
     "iopub.status.idle": "2025-05-02T07:40:50.304331Z",
     "shell.execute_reply": "2025-05-02T07:40:50.303471Z",
     "shell.execute_reply.started": "2025-05-02T07:40:50.298819Z"
    },
    "id": "JaoT9rNIt2wO",
    "outputId": "6efe8793-ec84-40ac-d48e-8fca0ebd52cb"
   },
   "source": [
    "<div style=\"background-color:yellow; text-align:center; padding:40px; font-family:sans-serif;\">\n",
    "  <h3 style=\"color:red;\"> Train :  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T15:22:18.184027Z",
     "iopub.status.busy": "2025-05-04T15:22:18.183322Z",
     "iopub.status.idle": "2025-05-04T15:22:23.298304Z",
     "shell.execute_reply": "2025-05-04T15:22:23.296751Z",
     "shell.execute_reply.started": "2025-05-04T15:22:18.184001Z"
    },
    "id": "YamfwMqsD_hM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 35.12 MiB is free. Process 33219 has 15.85 GiB memory in use. Of the allocated memory 15.49 GiB is allocated by PyTorch, and 65.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_365/3636998597.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Forward (drop last token for inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mlogits\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m                                            \u001b[0;31m# [B, N + L_in, V]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_365/3510530306.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# 5) Forward through Gemma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    819\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m                 )\n\u001b[1;32m    568\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    570\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 35.12 MiB is free. Process 33219 has 15.85 GiB memory in use. Of the allocated memory 15.49 GiB is allocated by PyTorch, and 65.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from IPython.display import HTML, display\n",
    "import io, base64\n",
    "\n",
    "# ──────────────── SETUP ──────────────── #\n",
    "\n",
    "# Metrics\n",
    "bleu_metric   = evaluate.load(\"bleu\")\n",
    "rouge_metric  = evaluate.load(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "\n",
    "# Hyperparams\n",
    "BATCH_SIZE     = 8\n",
    "TRAIN_EXAMPLES = 35614\n",
    "LEARNING_RATE  = 1e-4\n",
    "TRAIN_STEPS    = TRAIN_EXAMPLES // BATCH_SIZE\n",
    "EVAL_EVERY     = TRAIN_STEPS // 10\n",
    "\n",
    "\n",
    "#run.config.update({\n",
    "#    \"batch_size\":     BATCH_SIZE,\n",
    "#    \"train_examples\": TRAIN_EXAMPLES,\n",
    "#    \"learning_rate\":  LEARNING_RATE,\n",
    "#    \"train_steps\":    TRAIN_STEPS,\n",
    "#})\n",
    "\n",
    "# Model + optimizer\n",
    "model = CombinedModel(siglip, gemma, proj_dim=2048).to(device)\n",
    "freeze_with_mask(model, is_trainable_param)\n",
    "opt = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "# DataLoader wrapper\n",
    "class TorchDataset(torch.utils.data.IterableDataset):\n",
    "    def __iter__(self):\n",
    "        for example, _ in train_data_iterator():\n",
    "            yield example\n",
    "\n",
    "train_loader = DataLoader(TorchDataset(), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Helper to convert numpy or tensor to torch.Tensor\n",
    "def to_torch(x, dtype=None, device=None):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        t = x\n",
    "    else:\n",
    "        t = torch.from_numpy(x)\n",
    "    if dtype is not None:\n",
    "        t = t.to(dtype)\n",
    "    if device is not None:\n",
    "        t = t.to(device)\n",
    "    return t\n",
    "\n",
    "pad_id = tokenizer.pad_id()\n",
    "\n",
    "# ──────────────── TRAIN + EVAL LOOP ──────────────── #\n",
    "step = 0\n",
    "for batch in train_loader:\n",
    "    if step >= TRAIN_STEPS:\n",
    "        break\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Move to device\n",
    "    imgs = to_torch(batch[\"image\"].view(8, 3, 224, 224), dtype=torch.float32, device=device)   # [B,3,H,W]\n",
    "    ids  = to_torch(batch[\"text\"],  dtype=torch.long,  device=device)     # [B, seq_len]\n",
    "    mask = to_torch(batch[\"mask_ar\"],dtype=torch.long,  device=device)    # [B, seq_len]\n",
    "\n",
    "    # Forward (drop last token for inputs)\n",
    "    outputs = model(imgs, ids[:, :-1], mask[:, :-1])\n",
    "    logits  = outputs.logits                                            # [B, N + L_in, V]\n",
    "\n",
    "    # —— FIXED LOSS COMPUTATION —— #\n",
    "    B, total_len, V = logits.size()\n",
    "    seq_len         = ids.size(1)        # original seq_len\n",
    "    L_in            = seq_len - 1\n",
    "    L_out           = L_in\n",
    "    N               = total_len - L_in\n",
    "\n",
    "    # 1) slice text positions\n",
    "    text_logits = logits[:, N : N + L_out, :].reshape(-1, V)            # [B*L_out, V]\n",
    "    # 2) build targets from ids[:,1:]\n",
    "    targets     = ids[:, 1:].reshape(-1).long()                         # [B*L_out]\n",
    "    # 3) compute loss\n",
    "    loss        = F.cross_entropy(text_logits, targets, ignore_index=pad_id)\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # Log training loss\n",
    "    step += 1\n",
    "\n",
    "    \n",
    "#    wandb.log({\"train/loss\": loss.item()}, step=step)\n",
    "\n",
    "    # ───── EVALUATION ───── #\n",
    "    if step == 1 or step % EVAL_EVERY == 0:\n",
    "        model.eval()\n",
    "        all_hyps, all_refs = [], []\n",
    "\n",
    "        # collect a small validation set\n",
    "        for (example, caps), _ in zip(validation_data_iterator(), range(100)):\n",
    "            img_np = example[\"image\"]\n",
    "            # build prompt inputs\n",
    "            img_t = torch.from_numpy(img_np).unsqueeze(0).to(device)\n",
    "            ids_t = torch.from_numpy(example[\"text\"]).unsqueeze(0).long().to(device)\n",
    "            msk_t = torch.from_numpy(example[\"mask_input\"]).unsqueeze(0).long().to(device)\n",
    "\n",
    "            # generate\n",
    "            with torch.no_grad():\n",
    "                # make_predictions uses model.generation under the hood\n",
    "                pred = make_predictions(model, [(example, caps)], num_examples=1, batch_size=1)[0][1]\n",
    "\n",
    "            all_hyps.append(pred)\n",
    "            all_refs.append([caps])\n",
    "\n",
    "        # compute metrics\n",
    "        bleu_res   = bleu_metric.compute(predictions=all_hyps, references=all_refs, smooth=True)\n",
    "        rouge_res  = rouge_metric.compute(predictions=all_hyps, references=all_refs)\n",
    "        meteor_res = meteor_metric.compute(predictions=all_hyps, references=all_refs)\n",
    "\n",
    "        #wandb.log({\n",
    "        #    \"eval/bleu\":   bleu_res[\"bleu\"],\n",
    "        #    \"eval/rouge1\": rouge_res[\"rouge1\"],\n",
    "        #    \"eval/rouge2\": rouge_res[\"rouge2\"],\n",
    "        #    \"eval/rougeL\": rouge_res[\"rougeL\"],\n",
    "        #    \"eval/meteor\": meteor_res[\"meteor\"],\n",
    "        #}, step=step)\n",
    "\n",
    "        # display a few samples\n",
    "        html = \"<div style='display:flex; gap:16px;'>\"\n",
    "        for (example, _), hyp in zip(validation_data_iterator(), all_hyps):\n",
    "            img = example[\"image\"]\n",
    "            u8  = ((img + 1) * 127.5).astype(\"uint8\").transpose(1,2,0)\n",
    "            buf = io.BytesIO(); Image.fromarray(u8).save(buf, \"PNG\")\n",
    "            b64 = base64.b64encode(buf.getvalue()).decode()\n",
    "            html += f\"\"\"\n",
    "             <div style='text-align:center;'>\n",
    "               <img src=\"data:image/png;base64,{b64}\" width=200 /><br/>\n",
    "               <strong>Pred:</strong> {hyp}<br/>\n",
    "               <strong>Ref:</strong> {all_refs[0][0]}\n",
    "             </div>\"\"\"\n",
    "        html += \"</div>\"\n",
    "        display(HTML(html))\n",
    "\n",
    "# ──────────────── SAVE ──────────────── #\n",
    "#wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12110429,
     "datasetId": 7305746,
     "sourceId": 11642684,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "45e338651fdb4b5e8f37863ca3b8b28a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f7478c6db2246328a0cf3a028e99f86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fe0c3e51a5648659ad9b2a5b165d4f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a69f9647af146c7bfbd8da45ec177d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6a36a10d9524f599530e9df02879b23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f182e67c8e89403198aae302fec98e2f",
      "placeholder": "​",
      "style": "IPY_MODEL_4fe0c3e51a5648659ad9b2a5b165d4f1",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "bb9f07ff632b4a778988cabc89bf4033": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f7478c6db2246328a0cf3a028e99f86",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45e338651fdb4b5e8f37863ca3b8b28a",
      "value": 2
     }
    },
    "e6e980713e1c490ba92d6d91dd39f5fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9ec78ea19cd42e58bc2b80df25f7af2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f182e67c8e89403198aae302fec98e2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4159e4955cb4d1abfd96903b188f667": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b6a36a10d9524f599530e9df02879b23",
       "IPY_MODEL_bb9f07ff632b4a778988cabc89bf4033",
       "IPY_MODEL_f92ff29f69bd456583178a9d557fe531"
      ],
      "layout": "IPY_MODEL_e9ec78ea19cd42e58bc2b80df25f7af2"
     }
    },
    "f92ff29f69bd456583178a9d557fe531": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6e980713e1c490ba92d6d91dd39f5fa",
      "placeholder": "​",
      "style": "IPY_MODEL_7a69f9647af146c7bfbd8da45ec177d4",
      "value": " 2/2 [00:27&lt;00:00, 11.32s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
